{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762b50b7-5590-4f3c-8120-5bffbeca56fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:08<00:00,  2.93s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM , LlamaForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c826e9-090b-45a9-8a7f-a983a395f185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey, are you conscious? Can you talk to me?\\n\\nSure thing! I'm a large language model, so I can\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66771703-d874-4ece-935f-6dafc96b39bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/common/home/hg343/Research/LLM_ProblemConversion/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec74528-8638-4d83-96f0-4ab0a0ab373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menelaus's theorem, niven's theorem, ﻿pythagorean theorem, rédei's theorem, pitot theorem, prime number theorem, binomial theorem, exterior angle theorem, rational root theorem, miquel's theorem, dirichlet's approximation theorem, pythagorean theorem, descartes's theorem, pick's theorem, ﻿de moivre’s theorem, bayes' theorem, ﻿dirichlet’s theorem on arithmetic progressions, ﻿ceva’s theorem, de gua's theorem, ﻿vieta’s formulas, ﻿angle bisector theorem, ﻿binomial theorem, ﻿descartes’ theorem, viviani's theorem, ﻿bezout’s theorem, squeeze theorem, brianchon's theorem, fermat's little theorem, thales's theorem, intermediate value theorem, wilson's theorem, ﻿chinese remainder theorem, morley's trisector theorem\n",
      "ready to prompt\n",
      "input prepared\n",
      "generation done\n",
      "You are an expert at mathematics. Your job is to select the three best theorems most applicable for solving the given problem.\n",
      " You do not need to solve the problem. You must select your answer from this list of theorems: menelaus's theorem, niven's theorem, ﻿pythagorean theorem, rédei's theorem, pitot theorem, prime number theorem, binomial theorem, exterior angle theorem, rational root theorem, miquel's theorem, dirichlet's approximation theorem, pythagorean theorem, descartes's theorem, pick's theorem, ﻿de moivre’s theorem, bayes' theorem, ﻿dirichlet’s theorem on arithmetic progressions, ﻿ceva’s theorem, de gua's theorem, ﻿vieta’s formulas, ﻿angle bisector theorem, ﻿binomial theorem, ﻿descartes’ theorem, viviani's theorem, ﻿bezout’s theorem, squeeze theorem, brianchon's theorem, fermat's little theorem, thales's theorem, intermediate value theorem, wilson's theorem, ﻿chinese remainder theorem, morley's trisector theorem{question}What 3 theorems are best applicable for solving following question? Please provide response that are separated by , only. \n",
      "{\n",
      "In ABC, AB = 6, BC = 7, and CA = 8. Point D lies on BC, and AD bisects ∠ BAC. Point E lies on AC, and BE bisects ∠ ABC. The bisectors intersect at F. What is the ratio AF : FD?\n",
      "}\n",
      "\n",
      "Here are the three theorems that are most applicable for solving the given problem:\n",
      "\n",
      "1. Pitot's Theorem: This theorem states that if two lines are cut by a transversal, then the ratio of the lengths of the segments of the lines that are exterior to the transversal is equal to the ratio of the lengths of the segments of the lines that are interior to the transversal. In this case, the line AB is cut by the transversal BE, and the line BC is cut by the transversal AD. Therefore, Pitot's Theorem can be applied to find the ratio AF : FD.\n",
      "2. Rational Root Theorem: This theorem states that if a polynomial equation with rational coefficients has a rational root, then the root must be of the form p/q, where p and q are integers and q is non-zero. In this case, we can use the rational root theorem to find the roots of the equation x^2 + 7x + 8 = 0, which can be factored as (x + 3)(x + 2) = 0. Therefore, the rational roots of the equation are 3 and 2.\n",
      "3. Descartes' Theorem: This theorem states that if two lines are cut by a transversal, then the product of the lengths of the segments of one line that are exterior to the transversal is equal to the product of the lengths of the segments of the other line that are interior to the transversal. In this case, the line AB is cut by the transversal BE, and the line BC is cut by the transversal AD. Therefore, Descartes' Theorem can be applied to find the ratio AF : FD.\n",
      "\n",
      "Therefore, the three theorems that are most applicable for solving the given problem are Pitot's Theorem, Rational Root Theorem, and Descartes' Theorem.\n",
      "ready to prompt\n",
      "input prepared\n"
     ]
    }
   ],
   "source": [
    "problem_path = \"/common/home/hg343/Research/LLM_ProblemConversion/datasets/problems/Math_manual\"\n",
    "ground_truth_path = \"/common/home/hg343/Research/LLM_ProblemConversion/datasets/solution/ground_truth_Math_manual\"\n",
    "solution_path_noise = \"/common/home/hg343/Research/LLM_ProblemConversion/datasets/solution/Wikipedia\"\n",
    "def strip_and_normalize(content):\n",
    "    return re.sub(r'\\s+', ' ', content).strip().lower()\n",
    "\n",
    "unique_contents = set()\n",
    "for txt_file in os.listdir(ground_truth_path):\n",
    "    if txt_file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(ground_truth_path, txt_file)\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_content = strip_and_normalize(file.read())\n",
    "            unique_contents.add(file_content)\n",
    "hint_pool = \", \".join(unique_contents)\n",
    "# print(hint_pool)\n",
    "print(hint_pool)\n",
    "\n",
    "template = f\"\"\"You are an expert at mathematics. Your job is to select the three best theorems most applicable for solving the given problem.\n",
    " You do not need to solve the problem. You must select your answer from this list of theorems: {hint_pool}{{question}}\"\"\"\n",
    "\n",
    "# Process problems\n",
    "solution_path = \"./../datasets/inference_result\"\n",
    "hit_count, total_files = 0, 0\n",
    "for problem in os.listdir(problem_path):\n",
    "    if problem.endswith(\".txt\"):\n",
    "        total_files += 1\n",
    "        file_path = os.path.join(problem_path, problem)\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                problem_txt = file.read()\n",
    "        except IOError:\n",
    "            print(f\"Error reading '{file_path}'.\")\n",
    "            continue\n",
    "        print('ready to prompt')\n",
    "        question = f\"What 3 theorems are best applicable for solving following question? Please provide response that are separated by , only. \\n{{\\n{problem_txt}\\n}}\\n\"\n",
    "        prompt  =template + question\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        print('input prepared')\n",
    "        # Generate\n",
    "        generate_ids = model.generate(inputs.input_ids, max_length=1000)\n",
    "        print('generation done')\n",
    "        response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac7684-4907-41ae-8082-915ac4da8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c7d193-cb40-4752-abd5-4c2c98944421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_some_feedbacks(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # Generate\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "    return tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc94cc8-314d-4d62-9a86-808fc55bd071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, are you conscious? Can you talk to me?\n",
      "\n",
      "Sure, I can talk to you! I'm a large language\n"
     ]
    }
   ],
   "source": [
    "print(get_some_feedbacks('wat u doin?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c33fc9-e194-4610-bb8d-9f868be78cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The goal is to run the tests \n",
    "def hint_generation_llama2(problem_path,ground_truth_path,solution_space_path, model):\n",
    "    #########\n",
    "    model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "\n",
    "############\n",
    "    # Collect subdirectories\n",
    "    subdirectories = [item for item in os.listdir(solution_space_path)\n",
    "                      if os.path.isdir(os.path.join(solution_space_path, item))]\n",
    "    #  print(\", \".join(subdirectories))\n",
    "\n",
    "    # Function to strip and normalize content\n",
    "    def strip_and_normalize(content):\n",
    "        return re.sub(r'\\s+', ' ', content).strip().lower()\n",
    "\n",
    "    # Process ground truths\n",
    "    unique_contents = set()\n",
    "    for txt_file in os.listdir(ground_truth_path):\n",
    "        if txt_file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(ground_truth_path, txt_file)\n",
    "            with open(file_path, 'r') as file:\n",
    "                file_content = strip_and_normalize(file.read())\n",
    "                unique_contents.add(file_content)\n",
    "    hint_pool = \", \".join(unique_contents)\n",
    "    # print(hint_pool)\n",
    "\n",
    "\n",
    "    template = f\"\"\"You are an expert at mathematics. Your job is to select the three best theorems most applicable for solving the given problem.\n",
    "     You do not need to solve the problem. You must select your answer from this list of theorems: {hint_pool}{{question}}\"\"\"\n",
    "\n",
    "    # Process problems\n",
    "    solution_path = \"./../datasets/inference_result\"\n",
    "    hit_count, total_files = 0, 0\n",
    "    for problem in os.listdir(problem_path):\n",
    "        if problem.endswith(\".txt\"):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(problem_path, problem)\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    problem_txt = file.read()\n",
    "            except IOError:\n",
    "                print(f\"Error reading '{file_path}'.\")\n",
    "                continue\n",
    "            question = f\"What 3 theorems are best applicable for solving following question? Please provide response that are separated by , only. \\n{{\\n{problem_txt}\\n}}\\n\"\n",
    "            prompt  =template + question\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            # Generate\n",
    "            generate_ids = model.generate(inputs.input_ids, max_length=1000)\n",
    "            response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "            # Save response to file\n",
    "            output_file_path = os.path.join(solution_path, problem)\n",
    "            os.makedirs(solution_path, exist_ok=True)\n",
    "            try:\n",
    "                with open(output_file_path, 'w') as file:\n",
    "                    file.write(response)\n",
    "                    print(\"Wrote to: \" + output_file_path)\n",
    "            except IOError:\n",
    "                print(f\"Error writing to '{output_file_path}'.\")\n",
    "\n",
    "            # Compare with ground truth\n",
    "            ground_truth_file_path = os.path.join(ground_truth_path, problem)\n",
    "            if os.path.isfile(ground_truth_file_path):\n",
    "                with open(ground_truth_file_path, 'r',encoding='utf-8-sig') as ground_truth_file:\n",
    "                    ground_truth_items = [strip_and_normalize(item) for item in ground_truth_file.read().split(',')]\n",
    "                    response =[strip_and_normalize(item) for item in response.split(',')]\n",
    "                for item in ground_truth_items:\n",
    "                    item = item.strip('\\'')\n",
    "                    print('ground_truth_items: ', ground_truth_items)\n",
    "                    print('response: ', response)\n",
    "                    if item in response:\n",
    "                        hit_count += 1\n",
    "                        logging.info('response contains ground truth', 'hit_count:', hit_count)\n",
    "                        break\n",
    "                    else:\n",
    "                        logging.info('response does not include ground_truth')\n",
    "\n",
    "    logging.info('hit count: ',hit_count,'total problems: ',total_files)\n",
    "    return f\"{hit_count}/{total_files}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13b9b5-cc98-45d0-bd2d-4ab2a5f9e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_path = \"/common/home/hg343/Research/LLM_ProblemConversion/datasets/problems/Math_manual\"\n",
    "ground_truth_path = \"/common/home/hg343/Research/LLM_ProblemConversion/datasets/solution/ground_truth_Math_manual\"\n",
    "solution_path_noise = \"/common/home/hg343/Research/LLM_ProblemConversion/datasets/solution/Wikipedia\"\n",
    "hint_accuracy = hint_generation_llama2(problem_path,ground_truth_path,solution_path_noise, model=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
