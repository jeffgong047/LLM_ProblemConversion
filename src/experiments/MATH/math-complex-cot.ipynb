{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "283b7df3-a4f3-435c-a121-a95ed352b1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f339a0be3fe347fa991c42d02b55390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MATH with Complex CoT\n",
    "\n",
    "import guidance\n",
    "from guidance import models, gen, select\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import sympy\n",
    "import openai\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM , LlamaForCausalLM\n",
    "import time\n",
    "\n",
    "\n",
    "# openai.proxy = \"http://...\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'sk-...'\n",
    "\n",
    "TRY_CNT = 16\n",
    "\n",
    "\n",
    "# def get_parser():\n",
    "#     parser = argparse.ArgumentParser(description=\"Cumulative Reasoning\")\n",
    "#     parser.add_argument('--temperature', type=float, default=0.0, help='temperature')\n",
    "#     parser.add_argument('--majoritycnt', type=int, choices=range(1, 101), default=1,\n",
    "#                         help='numbers of majority voting times')\n",
    "#     parser.add_argument('--shots', type=int, choices=range(1, 101), default=8, help='numbers of few-shot examples')\n",
    "#     parser.add_argument('--hintcnt', type=int, choices=range(0, 101), default=2, help='numbers of hints to generate')\n",
    "#     parser.add_argument('--questioncnt', type=int, choices=range(0, 101), default=8,\n",
    "#                         help='numbers of questions to generate')\n",
    "#     parser.add_argument('--questiontrycnt', type=int, choices=range(0, 101), default=4,\n",
    "#                         help='numbers of tries to generate questions')\n",
    "#     parser.add_argument('--answertrycnt', type=int, choices=range(0, 101), default=4, help='numbers of tries to answer')\n",
    "#     parser.add_argument('--verbose', type=ast.literal_eval, default=True, help='verbose mode')\n",
    "#     parser.add_argument('--model', type=str, default='gpt-3.5-turbo-16k-0613', help='model to use')\n",
    "#     parser.add_argument('--withcode', type=ast.literal_eval, default='False', help='whether to use code to verify answers')\n",
    "#     parser.add_argument('--dataset', type=str, default='data/test.jsonl', help='dataset to use')\n",
    "#     parser.add_argument('--problem_level_lower_bound', type=int, default=1,\n",
    "#                         help='lower bound of problem level [lower_bound, upper_bound]')\n",
    "#     parser.add_argument('--problem_level_upper_bound', type=int, default=5,\n",
    "#                         help='upper bound of problem level [lower_bound, upper_bound]')\n",
    "#     # parser.add_argument('--problem_numbers', type=int, default=500, help='problem numbers to be evaluated')\n",
    "#     parser.add_argument('--problem_interval_begin', type=int, default=0, help='problem interval begin [begin, end]')\n",
    "#     parser.add_argument('--problem_interval_end', type=int, default=500, help='problem interval end [begin, end]')\n",
    "#     parser.add_argument('--inverse_problem_order', type=ast.literal_eval, default=True, help='whether to inverse problem order')\n",
    "#     return parser\n",
    "\n",
    "\n",
    "# parser = get_parser()\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# breakpoint()\n",
    "# # load a model (could be Transformers, LlamaCpp, VertexAI, OpenAI...)\n",
    "# llama2 = guidance.models.LlamaCpp('/common/home/zw393/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-chat-hf') \n",
    "# # append text or generations to the model\n",
    "# llama2 + f'Do you want a joke or a poem? ' + gen(stop='.')\n",
    "\n",
    "llama2 = models.Transformers(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267657c8-689f-4290-9dd1-9045e258a5a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m valid_correctness \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Define the guidance program judger, define the {{final_answer}} are correct,\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#   given the ground truth {{ground_truth_answer}}\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m llm_judger \u001b[38;5;241m=\u001b[39m llama2\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m#system}}YOU ARE one of the GREATEST mathematicians, logicians, programmers, and AI scientists. You are intelligent and rational. You are prudent and cautious. Your mastery over Arithmetic, Combinatorics, Number Theory, Probability Theory, Algebra, Analysis, and Geometry is unparalleled. You THINK NATURAL, BROAD AND DEEP. Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms think step by step. \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m/system}}\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m#system}}Your job is to judge whether the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is correct based on \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, do not be strict on the format, but check the content. Notice that unsolved half results are not Correct. \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m/system}}\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m#system}}Problem Subject: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mquestion_subject}}\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, Problem Content: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mquestion_content}}\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m/system}}\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m#system}}Is the final_answer correct, given the ground truth answer? Reply with Correct, Wrong or Unknown. \u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mfinal_answer}}\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mground_truth_answer}}\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m/system}}\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m#assistant}}\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mselect \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrectness\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m options=valid_correctness}}\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m/assistant}}\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llama2' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def try_wrapper(func):\n",
    "    def inner(*args, **kwargs):\n",
    "        try_cnt = 0\n",
    "        while try_cnt < TRY_CNT:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except Exception as e:\n",
    "                print(f\"func() failed, try again... (No. {try_cnt + 1}). Error: {e}\")\n",
    "                try_cnt += 1\n",
    "                time.sleep(min(1024, 2 ** (try_cnt / 2)))\n",
    "                continue\n",
    "\n",
    "    return inner\n",
    "\n",
    "\n",
    "def get_time_str(trycnt=0):\n",
    "    return \"2023-06-01-12-00-\" + str(trycnt).zfill(2)\n",
    "    return time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "\n",
    "examples = [\n",
    "]\n",
    "\n",
    "\"\"\n",
    "# we can pre-define valid option sets\n",
    "valid_correctness = [\"Correct\", \"Wrong\", \"Unknown\"]\n",
    "\n",
    "# Define the guidance program judger, define the {{final_answer}} are correct,\n",
    "#   given the ground truth {{ground_truth_answer}}\n",
    "llm_judger = llama2+ \"\"\"\n",
    "    {{#system}}YOU ARE one of the GREATEST mathematicians, logicians, programmers, and AI scientists. You are intelligent and rational. You are prudent and cautious. Your mastery over Arithmetic, Combinatorics, Number Theory, Probability Theory, Algebra, Analysis, and Geometry is unparalleled. You THINK NATURAL, BROAD AND DEEP. Let's think step by step. {{/system}}\n",
    "    {{#system}}Your job is to judge whether the \"final_answer\" is correct based on \"ground_truth_answer\", do not be strict on the format, but check the content. Notice that unsolved half results are not Correct. {{/system}}\n",
    "    {{#system}}Problem Subject: \"{{question_subject}}\", Problem Content: \"{{question_content}}\" {{/system}}\n",
    "    {{#system}}Is the final_answer correct, given the ground truth answer? Reply with Correct, Wrong or Unknown. \n",
    "    \"final_answer\": \"{{final_answer}}\", \"ground_truth_answer\": \"{{ground_truth_answer}}\"{{/system}}\n",
    "    {{#assistant}}{{select \"correctness\" options=valid_correctness}}{{/assistant}}\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5aced1-db98-42d3-b57d-4d81e9f65c42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 144\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(logfilename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    142\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(result) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# write each result as a new line\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Load the data from the JSONL file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(args\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m         cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    # Load the data from the JSONL file\n",
    "    data = []\n",
    "    with open(args.dataset, 'r', encoding='utf-8') as f:\n",
    "        cnt = 0\n",
    "        for line in f:\n",
    "            if (json.loads(line)['level'] < args.problem_level_lower_bound): continue\n",
    "            if (json.loads(line)['level'] > args.problem_level_upper_bound): continue\n",
    "            data.append(json.loads(line))\n",
    "            cnt += 1\n",
    "            # if (cnt == args.problem_numbers):\n",
    "            #     break\n",
    "    data = data[args.problem_interval_begin:args.problem_interval_end + 1]\n",
    "    print(len(data))\n",
    "    if args.inverse_problem_order:\n",
    "        data = data[::-1]\n",
    "\n",
    "    t = time.localtime()\n",
    "\n",
    "    complex_prompts = '''\n",
    "    {{#system}}\n",
    "    YOU ARE one of the GREATEST mathematicians, logicians, programmers, and AI scientists. You are intelligent and rational. You are prudent and cautious. Your mastery over Arithmetic, Combinatorics, Number Theory, Probability Theory, Algebra, Analysis, and Geometry is unparalleled. You THINK NATURAL, BROAD AND DEEP. Let's think step by step.\n",
    "    YOU will be given a mathematical question Q, and you need to generate intermediate thoughts to approach the answer of the given question Q.\n",
    "    Prioritize generating foundational hints that are useful for solving the problem. Prioritize generating foundational questions that are useful for solving the problem. We will solve these simpler components later, and then leverage these intermediate results to deduce the final solution.\n",
    "    {{/system}}\n",
    "    {{~#each examples}}\n",
    "    {{#user}}\n",
    "    Question:\n",
    "    {{this.question}}\n",
    "    A:\n",
    "    {{/user}}\n",
    "    {{#assistant}}{{this.solution}}{{/assistant}}\n",
    "    {{#user}}\n",
    "    Final Answer:\n",
    "    {{/user}}\n",
    "    {{#assistant}}{{this.final_answer}}{{/assistant}}\n",
    "    {{~/each}}\n",
    "    \n",
    "    {{#user}}Question: {{question}}{{/user}}\n",
    "    {{#assistant}}{{gen \"final_solution\" temperature=sol_temperature max_tokens=800}}{{/assistant}}\n",
    "    {{#user}}\n",
    "    Final Answer:\n",
    "    {{/user}}\n",
    "    {{#assistant}}{{gen \"final_answer\" temperature=ans_temperature max_tokens=50}}{{/assistant}}\n",
    "    '''\n",
    "    complex_examples = []\n",
    "    with open('complex-cot-math.txt', 'r', encoding='utf-8') as f:\n",
    "        t = f.read().split(\"\\n\\n\")\n",
    "        for i in t:\n",
    "            question = i.split(\"\\nA:\")[0].split('Question: ')[-1]\n",
    "            # print(question)\n",
    "            solution = \"\\nA:\".join(i.split(\"\\nA: \")[1:]).split(\"\\nThe answer is \")[0]\n",
    "            # print(answer)\n",
    "            final_answer = i.split(\"\\nThe answer is \")[-1]\n",
    "            print(final_answer)\n",
    "            complex_examples.append({'question': question, 'solution': solution, 'final_answer': final_answer})\n",
    "\n",
    "    complex_examples = complex_examples[:args.shots]\n",
    "\n",
    "    # Define the guidance program generate hints\n",
    "    program = llama2+ complex_prompts + complex_examples\n",
    "\n",
    "    t = time.localtime()\n",
    "\n",
    "    # extract 'test' from args.dataset in format 'data/test.jsonl'\n",
    "    dataset_name = args.dataset.split('/')[1].split('.')[0]\n",
    "    # change huggyllama/llama-13b to huggyllama-llama-13b\n",
    "    model_name = args.model.replace('/', '-')\n",
    "    logfilename = 'results/results-math-complex-cot-openai--' + model_name + '--' + dataset_name + '--k_' + str(\n",
    "        args.majoritycnt) + '--' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", t) + '.jsonl'\n",
    "    with open(logfilename, 'w') as f:\n",
    "        f.write(\"Model: \" + args.model + \"\\n\")\n",
    "        f.write(\"Temperature: \" + str(args.temperature) + \"\\n\")\n",
    "        f.write(\"Majority Cnt: \" + str(args.majoritycnt) + \"\\n\")\n",
    "        f.write(\"Hint Cnt: \" + str(args.hintcnt) + \"\\n\")\n",
    "        f.write(\"Question Cnt: \" + str(args.questioncnt) + \"\\n\")\n",
    "        f.write(\"Dataset: MATH - \" + args.dataset + \"\\n\")\n",
    "        f.write(\n",
    "            f\"Problem Level Interval: [{str(args.problem_level_lower_bound)}, {str(args.problem_level_upper_bound)}]\\n\")\n",
    "        # f.write(f\"Problem Numbers: First {str(args.problem_numbers)} Problems\\n\")\n",
    "        f.write(f\"Problem Interval: [{str(args.problem_interval_begin)}, {str(args.problem_interval_end)}]\\n\")\n",
    "        f.write(f\"Inverse Problem Order: {str(args.inverse_problem_order)}\\n\")\n",
    "        f.write(\"--------------------------------\\n\")\n",
    "    # Initialize counter for correct answers\n",
    "    correct_answers = 0\n",
    "    cnt = 0\n",
    "    total_cnt = len(data)\n",
    "\n",
    "    # Iterate over the data from the JSON file and call the solve function\n",
    "    for example in tqdm(data, desc=\"Evaluating\", unit=\"example\"):\n",
    "        cnt += 1\n",
    "\n",
    "        print(\"-------------------------\\n### Example ID: \", example[\"unique_id\"], \"\\t ( \", cnt, \"/\", total_cnt, \" )\")\n",
    "        print(\"Problem Level: \", example[\"level\"])\n",
    "        print(\"[Problem Subject]: \", example[\"subject\"])\n",
    "        print(\"[Problem Content]: \", example[\"problem\"])\n",
    "        # new Q for every example\n",
    "\n",
    "        try_cnt = 0\n",
    "        while True:\n",
    "            try_cnt += 1\n",
    "            try:\n",
    "                breakpoint()\n",
    "                out = try_wrapper(program)(question=example['problem'], sol_temperature=args.temperature, ans_temperature=args.temperature)\n",
    "\n",
    "                judgement = try_wrapper(judger)(question_content=example['problem'],\n",
    "                                                question_subject=example['subject'], final_answer=out['final_answer'],\n",
    "                                                ground_truth_answer=example['answer'])\n",
    "\n",
    "                print(\"[Final Solution]: \", out['final_solution'])\n",
    "                print(\"[Final Answer]: \", out['final_answer'])\n",
    "                # print(\"[Ground Truth Solution]: \", example[\"solution\"])\n",
    "                print(\"[Ground Truth Answer]: \", example[\"answer\"])\n",
    "                print(\"[Correctness]: \", judgement[\"correctness\"])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(min(1024, 2 ** (try_cnt / 2)))\n",
    "                continue\n",
    "\n",
    "        correct_answers += (judgement['correctness'] == 'Correct')\n",
    "        # Calculate and print the running accuracy\n",
    "        accuracy = correct_answers / cnt\n",
    "\n",
    "        print(\"[Running Average Accuracy]: \", accuracy)\n",
    "\n",
    "        result = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"example_id\": example[\"unique_id\"],\n",
    "            \"level\": example[\"level\"],\n",
    "            \"problem_subject\": example[\"subject\"],\n",
    "            \"problem_content\": example[\"problem\"],\n",
    "            \"correctness\": judgement[\"correctness\"],\n",
    "            \"final_solution\": out['final_solution'],\n",
    "            \"final_answer\": out['final_answer'],\n",
    "            \"ground_truth_solution\": example[\"solution\"],\n",
    "            \"ground_truth_answer\": example[\"answer\"],\n",
    "        }\n",
    "\n",
    "        # Write the result to a JSON file, note that we open the file in append mode ('a')\n",
    "        with open(logfilename, 'a') as f:\n",
    "            f.write(json.dumps(result) + '\\n')  # write each result as a new line\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e974bc06-3be2-41fe-aa77-17f83460de38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python LLM_PC",
   "language": "python",
   "name": "llm_pc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
