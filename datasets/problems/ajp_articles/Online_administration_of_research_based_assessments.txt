
View
Online
Export
CitationCrossMarkGUEST EDITORIAL| JANUARY 01 2021
Online administration of research-based assessments 
Ben V an Dusen ; Mollee Shultz ; Jayson M. Nissen ; Bethany R. Wilcox ; N. G. Holmes ; Manher Jariwala ;
Eleanor W . Close ; H. J. Lewandowski ; Steven Pollock
Am. J. Phys.  89, 7–8 (2021)
https://doi.org/10.1 119/10.0002888
Articles Y ou May Be Interested In
Providing Context for Identifying Ef fective Introductory Mechanics Courses
Phys. T each.  (March 2022)
The Ebers–Moll model for magnetic bipolar transistors
Appl. Phys. Lett.  (March 2005)
Software architecture of administrative information system oriented to complex administrative services
(one stop shop)
AIP Conference Proceedings  (September 2022) 04 October 2023 23:23:41
GUEST EDITORIAL
Online administration of research-based assessments
(Received 9 August 2020; accepted 13 November 2020)
https://doi.org/10.1119/10.0002888
I. INTRODUCTION
Research-based assessments (RBAs; e.g., the Force
Concept Inventory) that measure student content knowledge,attitudes, or identities have played a major role in transform-ing physics teaching practices. RBAs offer instructors a stan-dardized method for empirically investigating the efﬁcacy oftheir instructional practices and documenting the impacts ofcourse transformations. Unlike course exams, the commonusage of standardized RBAs across institutions uniquely sup-ports instructors to compare their student outcomes overtime or against multi-institutional data sets. While the num-ber of RBAs and RBA-using instructors has increased overthe last three decades, barriers to administering RBAs keepmany physics instructors from using them.
1,2To mitigate
these barriers, we have created full-service online RBA plat-forms (i.e., the Learning About STEM Student Outcomes[LASSO],
3Colorado Learning Attitudes About Science
Survey for Experimental Physics [E-CLASS],4and Physics
Lab Inventory of Critical thinking [PLIC]5platforms) that
host, administer, score, and analyze RBAs. These web-basedplatforms can make it easier for instructors to use RBAs,
especially as many courses have been forced to transition to
online instruction.
We hope that this editorial can serve as a guide for instruc-
tors considering administering RBAs online. In what fol-
lows, we examine common barriers to using RBAs, how
online administration can remove those barriers, and the
research into online administration of RBAs. In the supple-
mentary material,6we also include a practical how-to for
administering RBAs online and sample student email
wording.
II. ONLINE SOLUTIONS TO COMMON BARRIERS
TO USING RBAS
Below we have listed common reasons instructors give for
choosing notto use RBAs during class and discuss how
online administration addresses these concerns.
I can’t spare 30 1minutes of class time twice in a
semester to give an RBA. Administering RBAs online
allows students to complete RBAs either at home or in class.
Studies have found that with sufﬁcient incentives, students’
participation and scores are the same whether completed in
class or at home (see the discussion in Sec. III).
I don’t have the time or TA power to score an RBA.
Administering the RBA online removes the step of scoring
scantrons or paper surveys and automatically generates
spreadsheets of student responses that can be quickly and
easily analyzed. Online RBA platforms (e.g., LASSO, E-
CLASS, PLIC, and PhysPort DataExplorer7) can automate
the scoring process altogether, providing instructors full stu-
dent responses and scored responses.
I need an online version of the assessment and can’t
spare the time to set this up myself. Online RBA platformsalready host and administer a wide array of physics RBAs
for free.
I don’t know what my results mean. Online RBA plat-
forms can automatically generate reports that include visualiza-
tions and summary statistics to contextualize student outcomedata. This can help instructors make sense of their students’ per-
formance and inform concrete changes to their instruction.
I don’t have access to any comparison data. Online plat-
forms can automate comparisons with their larger datasets.They can also standardize data formats, making it easy to
compare or combine course data. These platforms collect
course meta-data that can identify appropriate comparisonpoints for a wide range of courses and institutions. They can
also automatically aggregate and anonymize datasets to sup-
port large-scale, multi-institution investigations.
III. CONCERNS WITH USING RBAS ONLINE AND
RESEARCH-BASED RESPONSES
Moving an RBA online brings with it several potential
concerns, including student engagement, test security, and
use of unauthorized resources. Below, we articulate some of
these concerns and summarize research ﬁndings that begin toaddress them.
11
Does giving the test online impact how many and which
of my students participate? Low-stakes RBAs administered
online have yielded similar participation rates as the equiva-lent paper tests administered in class.
2In an experiment
where researchers randomly assigned students at one institu-
tion to take the same RBA online outside of class versus onpaper inside class, participation rates were comparable if
instructors administered the RBAs using the recommended
practices described in the supplemental materials (alsoaccessible at Ref. 8). Moreover, the participation rates did
not differ between online and in-person based on gender or
ﬁnal course grade.
2Incentive structures strongly inﬂuence
participation rates; for example, another study9found an
increase in online participation rates compared to historical
norms, which the authors attributed to changes in incentives
(explicit credit for participation when administered online).
Does taking the test online impact the scores for my
course and can I compare my scores to previous terms?
In the ﬁrst study described above, researchers found that stu-
dent performance on the online, computer-based tests wereequivalent to performance on the same tests administered on
paper during class.
2This result held for both concept inven-
tory tests and attitudinal surveys, suggesting that instructorscan compare results from online and in-person administra-
tions. The second study described above
9found slightly
lower online scores relative to historical data sets. Theyattributed this effect to the increased participation rate from
lower-performing students in online assessments compared
with in-person assessments. Increasing the participation of
7 Am. J. Phys. 89(1), January 2021 http://aapt.org/ajp VC2021 American Association of Physics Teachers 7 04 October 2023 23:23:41
lower-performing students has the added beneﬁt of reducing
bias in the scores and making them more representative.
What if my students use the internet to look up the
answers to the questions? In a study examining students’
behaviors when taking research-based assessments online,9
researchers found that only /C2410% of students showed direct
evidence of copying question text, potentially intending tosearch the text to ﬁnd the correct answer online. For testswith solutions readily available online, this behavior corre-lated with increased performance, while for tests withoutavailable solutions, it correlated with lower performance.However, because the proportion of students engaging inthese behaviors was small, the impact on the overall averagefor the course was small. These ﬁndings align with other
2,10
ﬁndings about the lack of impact on performance associated
with administering an assessment online.
What if my students get distracted and don’t take the
test seriously? Researchers have used browser focus data (i.e.,
how often and for how long the assessment tab becomes hiddenon the student’s screen) to determine how common distractionmight be during online RBAs. This study
9found that browser
focus data indicated that between half and two-thirds of stu-dents lost focus on the assessment at least once, though themajority of these events (two-thirds) were less than 1 min induration. Additionally, neither the number nor the duration offocus loss events correlated with students’ scores. Thus, in thatstudy, there was no apparent negative impact on students’scores due to distraction in the online environment.
What if my students save the test and post it online?
Security of RBAs becomes particularly important whenadministering the assessments online, and, in practice, thenature of these concerns depends on the assessment in ques-tion. For example, well-used introductory assessments suchas the FMCE or BEMA are already available online on paidsites such as Chegg or CourseHero.
9Less well used or newer
assessments do not appear to have worked solutions avail-able online to date. In one study, very few students (less than1%–2%) attempted to save the test using print commandsduring online assessments.
9However, it is likely inevitable
that questions (and solutions) will become increasinglyavailable to students over time. This makes it all the moreimportant that faculty keep these assessments low-stakes, notgraded, and provide appropriate instructions to motivate stu-dents to take the assessment in the intended spirit, as a learn-ing tool (see the supplementary material for more details).
IV. CONCLUSIONS
RBAs are useful measures of the impact of a course on
students’ content knowledge, attitudes, and identities andhave been a major driver of change in physics education.Many physics instructors, however, do not use RBAs for avariety of reasons. The transition to online courses has onlyexacerbated the challenges of administering RBAs. Webelieve that online administration of RBAs, particularlythrough full-service RBA platforms, can remove many of thebarriers to using RBAs. Researchers have found that instruc-tors can get similar amounts, and quality, of RBA datawhether they administer them in-class or online. Further,research has found minimal impact in student scores fromusing unauthorized resources or evidence of students com-promising assessment security when administered online. Inaddition to being free for instructors, full-service onlineRBA platforms (e.g., LASSO,
3E-CLASS,4and PLIC5) alsocontribute to large-scale investigations. We hope that these
resources will support physics instructors and researchers.
Ben Van Dusen
School of Education, Iowa State University,
Ames, Iowa 50011
Mollee Shultz
Department of Physics, Texas State University,
San Marcos, Texas 78666
Jayson M. Nissen
Nissen Education Research and Design,
Corvallis, Oregon 97333
Bethany R. Wilcox
Department of Physics, University of Colorado Boulder,
Boulder, Colorado 80309
N. G. Holmes
Laboratory of Atomic and Solid State Physics and
the Department of Physics, Cornell University,
Ithaca, New York 14850
Manher Jariwala
Department of Physics, Boston University,
Boston, Massachusetts 02215
Eleanor W. Close
Department of Physics, Texas State University,
San Marcos, Texas 78666
H. J. Lewandowski
JILA and the Department of Physics,
University of Colorado, Boulder,
Colorado 80309
Steven Pollock
Department of Physics, University of Colorado Boulder,
Boulder, Colorado 80309
1Bethany R. Wilcox, Benjamin M. Zwickl, Robert D. Hobbs, John M.
Aiken, Nathan M. Welch, and H. J. Lewandowski, “Alternative model for
administration and analysis of research-based assessments,” Phys. Rev. -
Phys. Educ. Res. 12, 010139 (2016).
2Jayson M. Nissen, Manher Jariwala, Eleanor W. Close, and Ben Van
Dusen, “Participation and performance on paper-and computer-based low-stakes assessments,” Int. J. STEM Educ. 5, 21 (2018).
3Learning Assistant Alliance, <https://learningassistantalliance.org/mod-
ules/public/lasso.php >.
4Lewandowski Group, <https://jila.colorado.edu/lewandowski/research/e-class-
colorado-learning-attitudes-about-science-survey-experimental-physics#
administering >.
5Cornell Physics Education Research Lab, <http://cperl.lassp.cornell.edu/PLIC >.
6See supplementary material at https://www.scitation.org/doi/suppl/
10.1119/10.0002888 for a practical how-to for administering RBAs online
and sample student email wording.
7PhysPort, <https://www.physport.org/DataExplorer/ >.
8PhysPort, <https://www.physport.org/recommendations/Entry.cfm?ID ¼93329>.
9Bethany R. Wilcox and Steven J. Pollock, “Investigating students’ behav-
ior and performance in online conceptual assessment,” Phys. Rev. - Phys.
Educ. Res. 15, 020145 (2019).
10Scott Bonham, “Reliability, compliance, and security in web-based course
assessments,” Phys. Rev. Spec. Top. - Phys. Educ. Res. 4, 010106 (2008).
11The ﬁndings discussed here represent a snapshot of our current understanding of
student engagement with online RBAs; as the use of online tests becomes more
common and norms change, these ﬁndings may become less generalizable.
8 Am. J. Phys., Vol. 89, No. 1, January 2021 Van Dusen et al. 8 04 October 2023 23:23:41
