matrices: Let A=(aij)=(a_ij) be an n×n positive matrix: aij>0_ij>0 for 1≤i,j≤n1,j. Then the following statements hold. There is a positive real number r, called the Perron root or the Perron–Frobenius eigenvalue (also called the leading eigenvalue or dominant eigenvalue), such that r is an eigenvalue of A and any other eigenvalue λ (possibly complex) in absolute value is strictly smaller than r , |λ| < r. Thus, the spectral radius ρ(A)ρ(A) is equal to r. If the matrix coefficients are algebraic, this implies that the eigenvalue is a Perron number. The Perron–Frobenius eigenvalue is simple: r is a simple root of the characteristic polynomial of A. Consequently, the eigenspace associated to r is one-dimensional. (The same is true for the left eigenspace, i.e., the eigenspace for AT, the transpose of A.) There exists an eigenvector v = (v1,...,vn)T of A with eigenvalue r such that all components of v are positive: A v = r v, vi > 0 for 1 ≤ i ≤ n. (Respectively, there exists a positive left eigenvector w : wT A = r wT, wi > 0.) It is known in the literature under many variations as the Perron vector, Perron eigenvector, Perron-Frobenius eigenvector, leading eigenvector, or dominant eigenvector. There are no other positive (moreover non-negative) eigenvectors except positive multiples of v (respectively, left eigenvectors except 'ww'w), i.e., all other eigenvectors must have at least one negative or non-real component. limk→∞Ak/rk=vwTlim_k→∞A^k/r^k=vw^T, where the left and right eigenvectors for A are normalized so that wTv = 1. Moreover, the matrix v wT is the projection onto the eigenspace corresponding to r. This projection is called the Perron projection. Collatz–Wielandt formula: for all non-negative non-zero vectors x, let f(x) be the minimum value of [Ax]i / xi taken over all those i such that xi ≠ 0. Then f is a real valued function whose maximum over all non-negative non-zero vectors x is the Perron–Frobenius eigenvalue. A "Min-max" Collatz–Wielandt formula takes a form similar to the one above: for all strictly positive vectors x, let g(x) be the maximum value of [Ax]i / xi taken over i. Then g is a real valued function whose minimum over all strictly positive vectors x is the Perron–Frobenius eigenvalue. Birkhoff–Varga formula: Let x and y be strictly positive vectors. Then r=supx>0infy>0y⊤Axy⊤x=infx>0supy>0y⊤Axy⊤x=infx>0supy>0∑i,j=1nyiaijxj/∑i=1nyixi.=sup_x>0inf_y>0y^⊤Ax/y^⊤x=inf_x>0sup_y>0y^⊤Ax/y^⊤x=inf_x>0sup_y>0∑_i,j=1^ny_ia_ijx_j/∑_i=1^ny_ix_i. [8] Donsker–Varadhan–Friedland formula: Let p be a probability vector and x a strictly positive vector. Then r=suppinfx>0∑i=1npi[Ax]i/xi.=sup_pinf_x>0∑_i=1^np_i[Ax]_i/x_i. [9][10] Fiedler formula: r=supz>0infx>0,y>0,x∘y=zy⊤Axy⊤x=supz>0infx>0,y>0,x∘y=z∑i,j=1nyiaijxj/∑i=1nyixi.=sup_z>0 inf_x>0,>0,=zy^⊤Ax/y^⊤x=sup_z>0 inf_x>0,>0,=z∑_i,j=1^ny_ia_ijx_j/∑_i=1^ny_ix_i.[11] The Perron–Frobenius eigenvalue satisfies the inequalities mini∑jaij≤r≤maxi∑jaij.min_i∑_ja_ij≤max_i∑_ja_ij. All of these properties extend beyond strictly positive matrices to primitive matrices (see below). Facts 1–7 can be found in Meyer[12] chapter 8 claims 8.2.11–15 page 667 and exercises 8.2.5,7,9 pages 668–669. The left and right eigenvectors w and v are sometimes normalized so that the sum of their components is equal to 1; in this case, they are sometimes called stochastic eigenvectors. Often they are normalized so that the right eigenvector v sums to one, while wTv=1^Tv=1. Non-negative