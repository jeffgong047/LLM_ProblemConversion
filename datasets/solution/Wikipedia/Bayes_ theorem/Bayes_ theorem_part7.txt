case: Let PYx_Y^x be the conditional distribution of Y given X=x=x and let PX_X be the distribution of X. The joint distribution is then PX,Y(dx,dy)=PYx(dy)PX(dx)_X,Y(dx,dy)=P_Y^x(dy)P_X(dx). The conditional distribution PXy_X^y of X given Y=y=y is then determined by PXy(A)=E(1A(X)|Y=y)_X^y(A)=E(1_A(X)|Y=y) Existence and uniqueness of the needed conditional expectation is a consequence of the Radonâ€“Nikodym theorem. This was formulated by Kolmogorov in his famous book from 1933. Kolmogorov underlines the importance of conditional probability by writing "I wish to call attention to ... and especially the theory of conditional probabilities and conditional expectations ..." in the Preface.[18] The Bayes theorem determines the posterior distribution from the prior distribution. Bayes' theorem can be generalized to include improper prior distributions such as the uniform distribution on the real line.[19] Modern Markov chain Monte Carlo methods have boosted the importance of Bayes' theorem including cases with improper priors.[20]