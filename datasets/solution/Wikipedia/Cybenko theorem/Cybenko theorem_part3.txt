case: A spate of papers in the 1980s—1990s, from George Cybenko and Kurt Hornik [de] etc, established several universal approximation theorems for arbitrary width and bounded depth.[31][11][32][12] See[33][34][14] for reviews. The following is the most often quoted: Universal approximation theorem — Let C(X,Rm)(X,ℝ^m) denote the set of continuous functions from a subset X of a Euclidean Rnℝ^n space to a Euclidean space Rmℝ^m. Let σ∈C(R,R)σ(ℝ,ℝ). Note that (σ∘x)i=σ(xi)(σ)_i=σ(x_i), so σ∘xσ denotes σσ applied to each component of x. Then σσ is not polynomial if and only if for every n∈N∈ℕ, m∈N∈ℕ, compact K⊆Rn⊆ℝ^n, f∈C(K,Rm),ε>0(K,ℝ^m),ε>0 there exist k∈N∈ℕ, A∈Rk×n∈ℝ^k, b∈Rk∈ℝ^k, C∈Rm×k∈ℝ^m such that supx∈K‖f(x)−g(x)‖<εsup_xf(x)-g(x)<ε where g(x)=C⋅(σ∘(A⋅x+b))(x)=C·(σ∘(A+b)) Such an f can also be approximated by a network of greater depth by using the same construction for the first layer and approximating the identity function with later layers. Proof sketch It suffices to prove the case where m=1=1, since uniform convergence in Rmℝ^m is just uniform convergence in each coordinate. Let Fσ_σ be the set of all one-hidden-layer neural networks constructed with σσ. Let C0(Rd,R)_0(ℝ^d,ℝ) be the set of all C(Rd,R)(ℝ^d,ℝ) with compact support. If the function is a polynomial of degree d, then Fσ_σ is contained in the closed subspace of all polynomials of degree d, so its closure is also contained in it, which is not all of C0(Rd,R)_0(ℝ^d,ℝ). Otherwise, we show that Fσ_σ's closure is all of C0(Rd,R)_0(ℝ^d,ℝ). Suppose we can construct arbitrarily good approximations of the ramp function r(x)=−1ifx<−1+xif|x|≤1+1ifx>1(x)=-1 ifx<-1 +x if|x|≤1 +1 ifx>1 then it can be combined to construct arbitrary compactly-supported continuous function to arbitrary precision. It remains to approximate the ramp function. Any of the commonly used activation functions used in machine learning can obviously be used to approximate the ramp function, or first approximate the ReLU, then the ramp function. if σσ is "squashing", that is, it has limits σ(−∞)<σ(+∞)σ(-∞)<σ(+∞), then one can first affinely scale down its x-axis so that its graph looks like a step-function with two sharp "overshoots", then make a linear sum of enough of them to make a "staircase" approximation of the ramp function. With more steps of the staircase, the overshoots smooth out and we get arbitrarily good approximation of the ramp function. The case where σσ is a generic non-polynomial function is harder, and the reader is directed to.[14] The problem with polynomials may be removed by allowing the outputs of the hidden layers to be multiplied together (the "pi-sigma networks"), yielding the generalization:[32] Universal approximation theorem for pi-sigma networks — With any nonconstant activation function, a one-hidden-layer pi-sigma network is a universal approximator. Arbitrary-depth