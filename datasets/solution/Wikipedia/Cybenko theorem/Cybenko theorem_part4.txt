case: The "dual" versions of the theorem consider networks of bounded width and arbitrary depth. A variant of the universal approximation theorem was proved for the arbitrary depth case by Zhou Lu et al. in 2017.[18] They showed that networks of width n + 4 with ReLU activation functions can approximate any Lebesgue-integrable function on n-dimensional input space with respect to L1^1 distance if network depth is allowed to grow. It was also shown that if the width was less than or equal to n, this general expressive power to approximate any Lebesgue integrable function was lost. In the same paper[18] it was shown that ReLU networks with width n + 1 were sufficient to approximate any continuous function of n-dimensional input variables.[35] The following refinement, specifies the optimal minimum width for which such an approximation is possible and is due to.[36] Universal approximation theorem (L1 distance, ReLU activation, arbitrary depth, minimal width). For any Bochnerâ€“Lebesgue p-integrable function f:Rnâ†’Rm:â„^nâ†’â„^m and any Îµ>0Îµ>0, there exists a fully connected ReLU network F of width exactly dm=maxn+1,m_m=max{n+1,m}, satisfying âˆ«Rnâ€–f(x)âˆ’F(x)â€–pdx<Îµ.âˆ«_â„^nf(x)-F(x)^p dx<Îµ. Moreover, there exists a function fâˆˆLp(Rn,Rm)^p(â„^n,â„^m) and some Îµ>0Îµ>0, for which there is no fully connected ReLU network of width less than dm=maxn+1,m_m=max{n+1,m} satisfying the above approximation bound. Remark: If the activation is replaced by leaky-ReLU, and the input is restricted in a compact domain, then the exact minimum width is[24] dm=maxn,m,2_m=max{n,m,2}. Quantitative refinement: In the case where, when X=[0,1]dğ’³=[0,1]^d and D=1=1 and where ÏƒÏƒ is the ReLU activation function, the exact depth and width for a ReLU network to achieve ÎµÎµ error is also known.[37] If, moreover, the target function f is smooth, then the required number of layer and their width can be exponentially smaller.[38] Even if f is not smooth, the curse of dimensionality can be broken if f admits additional "compositional structure".[39][40] Together, the central result of[20] yields the following universal approximation theorem for networks with bounded width (see also[16] for the first result of this kind). Universal approximation theorem (Uniform non-affine activation, arbitrary depth, constrained width). Let Xğ’³ be a compact subset of Rdâ„^d. Let Ïƒ:Râ†’RÏƒ:â„â†’â„ be any non-affine continuous function which is continuously differentiable at at least one point, with nonzero derivative at that point. Let Nd,D:d+D+2Ïƒğ’©_d,D:d+D+2^Ïƒ denote the space of feed-forward neural networks with d input neurons, D output neurons, and an arbitrary number of hidden layers each with d+D+2+D+2 neurons, such that every hidden neuron has activation function ÏƒÏƒ and every output neuron has the identity as its activation function, with input layer Ï•Ï• and output layer ÏÏ. Then given any Îµ>0Îµ>0 and any fâˆˆC(X,RD)(ğ’³,â„^D), there exists f^âˆˆNd,D:d+D+2ÏƒfÌ‚âˆˆğ’©_d,D:d+D+2^Ïƒ such that supxâˆˆXâ€–f^(x)âˆ’f(x)â€–<Îµ.sup_xâˆˆğ’³fÌ‚(x)-f(x)<Îµ. In other words, Nğ’© is dense in C(X;RD)(ğ’³;â„^D) with respect to the topology of uniform convergence. Quantitative refinement: The number of layers and the width of each layer required to approximate f to ÎµÎµ precision known;[21] moreover, the result hold true when Xğ’³ and RDâ„^D are replaced with any non-positively curved Riemannian manifold. Certain necessary conditions for the bounded width, arbitrary depth case have been established, but there is still a gap between the known sufficient and necessary conditions.[18][19][41] Bounded depth and bounded width