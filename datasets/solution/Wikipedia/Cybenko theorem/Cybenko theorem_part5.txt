case: The first result on approximation capabilities of neural networks with bounded number of layers, each containing a limited number of artificial neurons was obtained by Maiorov and Pinkus.[25] Their remarkable result revealed that such networks can be universal approximators and for achieving this property two hidden layers are enough. Universal approximation theorem:[25] There exists an activation function σσ which is analytic, strictly increasing and sigmoidal and has the following property: For any f∈C[0,1]d[0,1]^d and ε>0ε>0 there exist constants di,cij,θij,γi_i,c_ij,θ_ij,γ_i, and vectors wij∈Rd𝐰^ij∈ℝ^d for which |f(x)−∑i=16d+3diσ(∑j=13dcijσ(wij⋅x−θij)−γi)|<ε(𝐱)-∑_i=1^6d+3d_iσ(∑_j=1^3dc_ijσ(𝐰^ij·𝐱-θ_ij)-γ_i)|<ε for all x=(x1,...,xd)∈[0,1]d𝐱=(x_1,...,x_d)∈[0,1]^d. This is an existence result. It says that activation functions providing universal approximation property for bounded depth bounded width networks exist. Using certain algorithmic and computer programming techniques, Guliyev and Ismailov efficiently constructed such activation functions depending on a numerical parameter. The developed algorithm allows one to compute the activation functions at any point of the real axis instantly. For the algorithm and the corresponding computer code see.[26] The theoretical result can be formulated as follows. Universal approximation theorem:[26][27] Let [a,b][a,b] be a finite segment of the real line, s=b−a=b-a and λλ be any positive number. Then one can algorithmically construct a computable sigmoidal activation function σ:R→Rσℝ→ℝ, which is infinitely differentiable, strictly increasing on (−∞,s)(-∞,s), λλ -strictly increasing on [s,+∞)[s,+∞), and satisfies the following properties: 1) For any f∈C[a,b][a,b] and ε>0ε>0 there exist numbers c1,c2,θ1_1,c_2,θ_1 and θ2θ_2 such that for all x∈[a,b]∈[a,b] |f(x)−c1σ(x−θ1)−c2σ(x−θ2)|<ε|f(x)-c_1σ(x-θ_1)-c_2σ(x-θ_2)|<ε 2) For any continuous function F on the d-dimensional box [a,b]d[a,b]^d and ε>0ε>0, there exist constants ep_p, cpq_pq, θpqθ_pq and ζpζ_p such that the inequality |F(x)−∑p=12d+2epσ(∑q=1dcpqσ(wq⋅x−θpq)−ζp)|<ε|F(𝐱)-∑_p=1^2d+2e_pσ(∑_q=1^dc_pqσ(𝐰^q·𝐱-θ_pq)-ζ_p)|<ε holds for all x=(x1,…,xd)∈[a,b]d𝐱=(x_1,…,x_d)∈[a,b]^d. Here the weights wq𝐰^q, q=1,…,d=1,…,d, are fixed as follows: w1=(1,0,…,0),w2=(0,1,…,0),…,wd=(0,0,…,1).𝐰^1=(1,0,…,0), 𝐰^2=(0,1,…,0), …, 𝐰^d=(0,0,…,1). In addition, all the coefficients ep_p, except one, are equal. Here “σ:R→Rσℝ→ℝ is λλ-strictly increasing on some set X” means that there exists a strictly increasing function u:X→R→ℝ such that |σ(x)−u(x)|≤λ|σ(x)-u(x)|≤λ for all x∈X. Clearly, a λλ-increasing function behaves like a usual increasing function as λλ gets small. In the "depth-width" terminology, the above theorem says that for certain activation functions depth-22 width-22 networks are universal approximators for univariate functions and depth-33 width-(2d+2)(2d+2) networks are universal approximators for d-variable functions (d>1>1). Graph