background: The sampling theorem was implied by the work of Harry Nyquist in 1928,[11] in which he showed that up to 2B2B independent pulse samples could be sent through a system of bandwidth B; but he did not explicitly consider the problem of sampling and reconstruction of continuous signals. About the same time, Karl Küpfmüller showed a similar result[12] and discussed the sinc-function impulse response of a band-limiting filter, via its integral, the step-response sine integral; this bandlimiting and reconstruction filter that is so central to the sampling theorem is sometimes referred to as a Küpfmüller filter (but seldom so in English). The sampling theorem, essentially a dual of Nyquist's result, was proved by Claude E. Shannon.[2] The mathematician E. T. Whittaker published similar results in 1915,[13] J. M. Whittaker in 1935,[14] and Gabor in 1946 ("Theory of communication"). In 1948 and 1949, Claude E. Shannon published the two revolutionary articles in which he founded the information theory.[15][16][2] In Shannon 1948 the sampling theorem is formulated as "Theorem 13": Let f(t)(t) contain no frequencies over W. Then f(t)=∑n=−∞∞Xnsin⁡π(2Wt−n)π(2Wt−n),(t)=∑_n=-∞^∞X_nsinπ(2Wt-n)/π(2Wt-n), where Xn=f(n2W)_n=f(n/2W). It was not until these articles were published that the theorem known as "Shannon's sampling theorem" became common property among communication engineers, although Shannon himself writes that this is a fact which is common knowledge in the communication art.[B] A few lines further on, however, he adds: "but in spite of its evident importance, [it] seems not to have appeared explicitly in the literature of communication theory". Other