Origin: Wolpert and Macready give two NFL theorems that are closely related to the folkloric theorem. In their paper, they state: We have dubbed the associated results NFL theorems because they demonstrate that if an algorithm performs well on a certain class of problems then it necessarily pays for that with degraded performance on the set of all remaining problems.[5] The first theorem hypothesizes objective functions that do not change while optimization is in progress, and the second hypothesizes objective functions that may change.[5] Theorem 1: For any algorithms a1 and a2, at iteration step m ∑fP(dmy∣f,m,a1)=∑fP(dmy∣f,m,a2),∑_fP(d_m^y,m,a_1)=∑_fP(d_m^y,m,a_2), where dmy_m^y denotes the ordered set of size m of the cost values y associated to input values x∈X, f:X→Y:X is the function being optimized and P(dmy∣f,m,a)(d_m^y,m,a) is the conditional probability of obtaining a given sequence of cost values from algorithm a run m times on function f. The theorem can be equivalently formulated as follows: Theorem 1: Given a finite set V and a finite set S of real numbers, assume that f:V→S:V is chosen at random according to uniform distribution on the set SV^V of all possible functions from V to S. For the problem of optimizing f over the set V, then no algorithm performs better than blind search. Here, blind search means that at each step of the algorithm, the element v∈V is chosen at random with uniform probability distribution from the elements of V that have not been chosen previously. In essence, this says that when all functions f are equally likely, the probability of observing an arbitrary sequence of m values in the course of optimization does not depend upon the algorithm. In the analytic framework of Wolpert and Macready, performance is a function of the sequence of observed values (and not e.g. of wall-clock time), so it follows easily that all algorithms have identically distributed performance when objective functions are drawn uniformly at random, and also that all algorithms have identical mean performance. But identical mean performance of all algorithms does not imply Theorem 1, and thus the folkloric theorem is not equivalent to the original theorem. Theorem 2 establishes a similar, but "more subtle", NFL result for time-varying objective functions.[5]