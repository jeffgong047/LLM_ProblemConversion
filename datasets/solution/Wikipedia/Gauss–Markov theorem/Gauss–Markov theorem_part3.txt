Remark: Proof that the OLS indeed minimizes the sum of squares of residuals may proceed as follows with a calculation of the Hessian matrix and showing that it is positive definite. The MSE function we want to minimize is f(β0,β1,…,βp)=∑i=1n(yi−β0−β1xi1−⋯−βpxip)2(β_0,β_1,…,β_p)=∑_i=1^n(y_i-β_0-β_1x_i1-…-β_px_ip)^2 for a multiple regression model with p variables. The first derivative is ddβf=−2XT(y−Xβ)=−2[∑i=1n(yi−⋯−βpxip)∑i=1nxi1(yi−⋯−βpxip)⋮∑i=1nxip(yi−⋯−βpxip)]=0p+1,d/dβf =-2X^T(𝐲-Xβ) =-2[ ∑_i=1^n(y_i-…-β_px_ip); ∑_i=1^nx_i1(y_i-…-β_px_ip); ⋮; ∑_i=1^nx_ip(y_i-…-β_px_ip) ] =0_p+1, where XT^T is the design matrix X=[1x11⋯x1p1x21⋯x2p⋮1xn1⋯xnp]∈Rn×(p+1);n≥p+1=[ 1 x_11 ⋯ x_1p; 1 x_21 ⋯ x_2p; ⋮; 1 x_n1 ⋯ x_np ]∈ℝ^n×(p+1);+1 The Hessian matrix of second derivatives is H=2[n∑i=1nxi1⋯∑i=1nxip∑i=1nxi1∑i=1nxi12⋯∑i=1nxi1xip⋮⋮⋱⋮∑i=1nxip∑i=1nxipxi1⋯∑i=1nxip2]=2XTXℋ=2[ n ∑_i=1^nx_i1 ⋯ ∑_i=1^nx_ip; ∑_i=1^nx_i1 ∑_i=1^nx_i1^2 ⋯ ∑_i=1^nx_i1x_ip; ⋮ ⋮ ⋱ ⋮; ∑_i=1^nx_ip ∑_i=1^nx_ipx_i1 ⋯ ∑_i=1^nx_ip^2 ]=2X^TX Assuming the columns of X are linearly independent so that XTX^TX is invertible, let X=[v1v2⋯vp+1]=[ 𝐯_1 𝐯_2 ⋯ 𝐯_p+1 ], then k1v1+⋯+kp+1vp+1=0⟺k1=⋯=kp+1=0_1𝐯_1+…+k_p+1𝐯_p+1=0_1=…=k_p+1=0 Now let k=(k1,…,kp+1)T∈R(p+1)×1𝐤=(k_1,…,k_p+1)^T∈ℝ^(p+1)×1 be an eigenvector of Hℋ. k≠0⟹(k1v1+⋯+kp+1vp+1)2>0𝐤≠0(k_1𝐯_1+…+k_p+1𝐯_p+1)^2>0 In terms of vector multiplication, this means [k1⋯kp+1][v1⋮vp+1][v1⋯vp+1][k1⋮kp+1]=kTHk=λkTk>0[ k_1 ⋯ k_p+1 ][ 𝐯_1; ⋮; 𝐯_p+1 ][ 𝐯_1 ⋯ 𝐯_p+1 ][ k_1; ⋮; k_p+1 ]=𝐤^Tℋ𝐤=λ𝐤^T𝐤>0 where λλ is the eigenvalue corresponding to k𝐤. Moreover, kTk=∑i=1p+1ki2>0⟹λ>0𝐤^T𝐤=∑_i=1^p+1k_i^2>0λ>0 Finally, as eigenvector k𝐤 was arbitrary, it means all eigenvalues of Hℋ are positive, therefore Hℋ is positive definite. Thus, β=(XTX)−1XTYβ=(X^TX)^-1X^TY is indeed a global minimum. Or, just see that for all vectors v,vTXTXv=‖Xv‖2≥0𝐯,𝐯^TX^TX𝐯=𝐗𝐯^2≥0. So the Hessian is positive definite if full rank.