Proof: Let β =Cyβ̃=Cy be another linear estimator of ββ with C=(XTX)−1XT+D=(X^TX)^-1X^T+D where D is a K×n non-zero matrix. As we're restricting to unbiased estimators, minimum mean squared error implies minimum variance. The goal is therefore to show that such an estimator has a variance no smaller than that of β^,β, the OLS estimator. We calculate: E⁡[β ]=E⁡[Cy]=E⁡[((XTX)−1XT+D)(Xβ+ε)]=((XTX)−1XT+D)Xβ+((XTX)−1XT+D)E⁡[ε]=((XTX)−1XT+D)XβE⁡[ε]=0=(XTX)−1XTXβ+DXβ=(IK+DX)β.E[β̃] =E[Cy] =E[((X^TX)^-1X^T+D)(Xβ+ε)] =((X^TX)^-1X^T+D)Xβ+((X^TX)^-1X^T+D)E[ε] =((X^TX)^-1X^T+D)Xβ E[ε]=0 =(X^TX)^-1X^TXβ+DXβ =(I_K+DX)β. Therefore, since ββ is unobservable, β β̃ is unbiased if and only if DX=0=0. Then: Var⁡(β )=Var⁡(Cy)=CVar(y)CT=σ2CCT=σ2((XTX)−1XT+D)(X(XTX)−1+DT)=σ2((XTX)−1XTX(XTX)−1+(XTX)−1XTDT+DX(XTX)−1+DDT)=σ2(XTX)−1+σ2(XTX)−1(DX)T+σ2DX(XTX)−1+σ2DDT=σ2(XTX)−1+σ2DDTDX=0=Var⁡(β^)+σ2DDTσ2(XTX)−1=Var⁡(β^)Var(β̃) =Var(Cy) =CVar(y)C^T =σ^2CC^T =σ^2((X^TX)^-1X^T+D)(X(X^TX)^-1+D^T) =σ^2((X^TX)^-1X^TX(X^TX)^-1+(X^TX)^-1X^TD^T+DX(X^TX)^-1+DD^T) =σ^2(X^TX)^-1+σ^2(X^TX)^-1(DX)^T+σ^2DX(X^TX)^-1+σ^2DD^T =σ^2(X^TX)^-1+σ^2DD^T DX=0 =Var(β)+σ^2DD^T σ^2(X^TX)^-1=Var(β) Since DDT^T is a positive semidefinite matrix, Var⁡(β )Var(β̃) exceeds Var⁡(β^)Var(β) by a positive semidefinite matrix. Remarks on the