theorem: In information theory, the source coding theorem (Shannon 1948)[2] informally states that (MacKay 2003, pg. 81,[3] Cover 2006, Chapter 5[4]): N i.i.d. random variables each with entropy H(X) can be compressed into more than N H(X) bits with negligible risk of information loss, as N → ∞; but conversely, if they are compressed into fewer than N H(X) bits it is virtually certain that information will be lost.The NH(X)(X) coded sequence represents the compressed message in a biunivocal way, under the assumption that the decoder knows the source. From a practical point of view, this hypothesis is not always true. Consequently, when the entropy encoding is applied the transmitted message is NH(X)+(inf.source)(X)+(inf.source). Usually, the information that characterizes the source is inserted at the beginning of the transmitted message. Source coding theorem for symbol