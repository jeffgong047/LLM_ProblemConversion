The Hammersley–Clifford theorem is a result in probability theory, mathematical statistics and statistical mechanics that gives necessary and sufficient conditions under which a strictly positive probability distribution (of events in a probability space)[clarification needed] can be represented as events generated by a Markov network (also known as a Markov random field). It is the fundamental theorem of random fields.[1] It states that a probability distribution that has a strictly positive mass or density satisfies one of the Markov properties with respect to an undirected graph G if and only if it is a Gibbs random field, that is, its density can be factorized over the cliques (or complete subgraphs) of the graph.
The relationship between Markov and Gibbs random fields was initiated by Roland Dobrushin[2] and Frank Spitzer[3] in the context of statistical mechanics. The theorem is named after John Hammersley and Peter Clifford, who proved the equivalence in an unpublished paper in 1971.[4][5] Simpler proofs using the inclusion–exclusion principle were given independently by Geoffrey Grimmett,[6] Preston[7] and Sherman[8] in 1973, with a further proof by Julian Besag in 1974.[9]


Proof outline[edit]
A simple Markov network for demonstrating that any Gibbs random field satisfies every Markov property.
It is a trivial matter to show that a Gibbs random field satisfies every Markov property. As an example of this fact, see the following:
In the image to the right, a Gibbs random field over the provided graph has the form Pr(A,B,C,D,E,F)∝f1(A,B,D)f2(A,C,D)f3(C,D,F)f4(C,E,F)(A,B,C,D,E,F)_1(A,B,D)f_2(A,C,D)f_3(C,D,F)f_4(C,E,F). If variables C and D are fixed, then the global Markov property requires that: A,B⊥E,F|C,D,B,F|C,D (see conditional independence), since C,D,D forms a barrier between A,B,B and E,F,F.
With C and D constant, Pr(A,B,E,F|C=c,D=d)∝[f1(A,B,d)f2(A,c,d)]⋅[f3(c,d,F)f4(c,E,F)]=g1(A,B)g2(E,F)(A,B,E,F|C=c,D=d)∝[f_1(A,B,d)f_2(A,c,d)]·[f_3(c,d,F)f_4(c,E,F)]=g_1(A,B)g_2(E,F) where g1(A,B)=f1(A,B,d)f2(A,c,d)_1(A,B)=f_1(A,B,d)f_2(A,c,d) and g2(E,F)=f3(c,d,F)f4(c,E,F)_2(E,F)=f_3(c,d,F)f_4(c,E,F). This implies that A,B⊥E,F|C,D,B,F|C,D.
To establish that every positive probability distribution that satisfies the local Markov property is also a Gibbs random field, the following lemma, which provides a means for combining different factorizations, needs to be proved:

Lemma 1 provides a means for combining factorizations as shown in this diagram. Note that in this image, the overlap between sets is ignored.
Lemma 1
Let U denote the set of all random variables under consideration, and let Θ,Φ1,Φ2,…,Φn⊆UΘ,Φ_1,Φ_2,…,Φ_n and Ψ1,Ψ2,…,Ψm⊆UΨ_1,Ψ_2,…,Ψ_m denote arbitrary sets of variables. (Here, given an arbitrary set of variables X, X will also denote an arbitrary assignment to the variables from X.)
If
Pr(U)=f(Θ)∏i=1ngi(Φi)=∏j=1mhj(Ψj)(U)=f(Θ)∏_i=1^ng_i(Φ_i)=∏_j=1^mh_j(Ψ_j)
for functions f,g1,g2,…gn,g_1,g_2,_n and h1,h2,…,hm_1,h_2,…,h_m, then there exist functions h1′,h2′,…,hm′'_1,h'_2,…,h'_m and g1′,g2′,…,gn′'_1,g'_2,…,g'_n such that
Pr(U)=(∏j=1mhj′(Θ∩Ψj))(∏i=1ngi′(Φi))(U)=(∏_j=1^mh'_j(Θ∩Ψ_j))(∏_i=1^ng'_i(Φ_i))
In other words, ∏j=1mhj(Ψj)∏_j=1^mh_j(Ψ_j) provides a template for further factorization of f(Θ)(Θ).




Proof of Lemma 1



In order to use ∏j=1mhj(Ψj)∏_j=1^mh_j(Ψ_j) as a template to further factorize f(Θ)(Θ), all variables outside of ΘΘ need to be fixed. To this end, let θ¯θ̅ be an arbitrary fixed assignment to the variables from U∖Θ∖Θ (the variables not in ΘΘ). For an arbitrary set of variables X, let θ¯[X]θ̅[X] denote the assignment θ¯θ̅ restricted to the variables from X∖Θ∖Θ (the variables from X, excluding the variables from ΘΘ). 
Moreover, to factorize only f(Θ)(Θ), the other factors g1(Φ1),g2(Φ2),...,gn(Φn)_1(Φ_1),g_2(Φ_2),...,g_n(Φ_n) need to be rendered moot for the variables from ΘΘ. To do this, the factorization 
Pr(U)=f(Θ)∏i=1ngi(Φi)(U)=f(Θ)∏_i=1^ng_i(Φ_i)
will be re-expressed as
Pr(U)=(f(Θ)∏i=1ngi(Φi∩Θ,θ¯[Φi]))(∏i=1ngi(Φi)gi(Φi∩Θ,θ¯[Φi]))(U)=(f(Θ)∏_i=1^ng_i(Φ_i∩Θ,θ̅[Φ_i]))(∏_i=1^ng_i(Φ_i)/g_i(Φ_i∩Θ,θ̅[Φ_i]))
For each i=1,2,...,n=1,2,...,n: gi(Φi∩Θ,θ¯[Φi])_i(Φ_i∩Θ,θ̅[Φ_i]) is gi(Φi)_i(Φ_i) where all variables outside of ΘΘ have been fixed to the values prescribed by θ¯θ̅. 
Let 
f′(Θ)=f(Θ)∏i=1ngi(Φi∩Θ,θ¯[Φi])'(Θ)=f(Θ)∏_i=1^ng_i(Φ_i∩Θ,θ̅[Φ_i])
and 
gi′(Φi)=gi(Φi)gi(Φi∩Θ,θ¯[Φi])'_i(Φ_i)=g_i(Φ_i)/g_i(Φ_i∩Θ,θ̅[Φ_i])
for each i=1,2,…,n=1,2,…,n so
Pr(U)=f′(Θ)∏i=1ngi′(Φi)=∏j=1mhj(Ψj)(U)=f'(Θ)∏_i=1^ng'_i(Φ_i)=∏_j=1^mh_j(Ψ_j)
What is most important is that gi′(Φi)=gi(Φi)gi(Φi∩Θ,θ¯[Φi])=1'_i(Φ_i)=g_i(Φ_i)/g_i(Φ_i∩Θ,θ̅[Φ_i])=1 when the values assigned to ΦiΦ_i do not conflict with the values prescribed by θ¯θ̅, making gi′(Φi)'_i(Φ_i) "disappear" when all variables not in ΘΘ are fixed to the values from θ¯θ̅.
Fixing all variables not in ΘΘ to the values from θ¯θ̅ gives
Pr(Θ,θ¯)=f′(Θ)∏i=1ngi′(Φi∩Θ,θ¯[Φi])=∏j=1mhj(Ψj∩Θ,θ¯[Ψj])(Θ,θ̅)=f'(Θ)∏_i=1^ng'_i(Φ_i∩Θ,θ̅[Φ_i])=∏_j=1^mh_j(Ψ_j∩Θ,θ̅[Ψ_j])
Since gi′(Φi∩Θ,θ¯[Φi])=1'_i(Φ_i∩Θ,θ̅[Φ_i])=1,
f′(Θ)=∏j=1mhj(Ψj∩Θ,θ¯[Ψj])'(Θ)=∏_j=1^mh_j(Ψ_j∩Θ,θ̅[Ψ_j])
Letting 
hj′(Θ∩Ψj)=hj(Ψj∩Θ,θ¯[Ψj])'_j(Θ∩Ψ_j)=h_j(Ψ_j∩Θ,θ̅[Ψ_j])
gives:
f′(Θ)=∏j=1mhj′(Θ∩Ψj)'(Θ)=∏_j=1^mh'_j(Θ∩Ψ_j)
which finally gives:
Pr(U)=(∏j=1mhj′(Θ∩Ψj))(∏i=1ngi′(Φi))(U)=(∏_j=1^mh'_j(Θ∩Ψ_j))(∏_i=1^ng'_i(Φ_i))


The clique formed by vertices x1_1, x2_2, and x3_3, is the intersection of x1∪∂x1{x_1}∪_1, x2∪∂x2{x_2}∪_2, and x3∪∂x3{x_3}∪_3.
Lemma 1 provides a means of combining two different factorizations of Pr(U)(U). The local Markov property implies that for any random variable x∈U, that there exists factors fx_x and f−x_-x such that:
Pr(U)=fx(x,∂x)f−x(U∖x)(U)=f_x(x,)f_-x(U∖{x})
where ∂x are the neighbors of node x. Applying Lemma 1 repeatedly eventually factors Pr(U)(U) into a product of clique potentials (see the image on the right).
End of Proof

See also[edit]
Markov random field
Conditional random field
Notes[edit]


^ Lafferty, John D.; Mccallum, Andrew (2001). "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data". Proc. of the 18th Intl. Conf. on Machine Learning (ICML-2001). Morgan Kaufmann. ISBN 9781558607781. Retrieved 14 December 2014. by the fundamental theorem of random fields (Hammersley & Clifford 1971)

^ Dobrushin, P. L. (1968), "The Description of a Random Field by Means of Conditional Probabilities and Conditions of Its Regularity", Theory of Probability and Its Applications, 13 (2): 197–224, doi:10.1137/1113026

^ Spitzer, Frank (1971), "Markov Random Fields and Gibbs Ensembles", The American Mathematical Monthly, 78 (2): 142–154, doi:10.2307/2317621, JSTOR 2317621

^ Hammersley, J. M.; Clifford, P. (1971), Markov fields on finite graphs and lattices (PDF)

^ Clifford, P. (1990), "Markov random fields in statistics", in Grimmett, G. R.; Welsh, D. J. A. (eds.), Disorder in Physical Systems: A Volume in Honour of John M. Hammersley, Oxford University Press, pp. 19–32, ISBN 978-0-19-853215-6, MR 1064553, retrieved 2009-05-04

^ Grimmett, G. R. (1973), "A theorem about random fields", Bulletin of the London Mathematical Society, 5 (1): 81–84, CiteSeerX 10.1.1.318.3375, doi:10.1112/blms/5.1.81, MR 0329039

^ Preston, C. J. (1973), "Generalized Gibbs states and Markov random fields", Advances in Applied Probability, 5 (2): 242–261, doi:10.2307/1426035, JSTOR 1426035, MR 0405645

^ Sherman, S. (1973), "Markov random fields and Gibbs random fields", Israel Journal of Mathematics, 14 (1): 92–103, doi:10.1007/BF02761538, MR 0321185

^ Besag, J. (1974), "Spatial interaction and the statistical analysis of lattice systems", Journal of the Royal Statistical Society, Series B, 36 (2): 192–236, JSTOR 2984812, MR 0373208


Further reading[edit]
Bilmes, Jeff (Spring 2006), Handout 2: Hammersley–Clifford (PDF), course notes from University of Washington course.
Grimmett, Geoffrey (2018), "7.", Probability on Graphs (2nd ed.), Cambridge University Press, ISBN 9781108438179
Langseth, Helge, The Hammersley–Clifford Theorem and its Impact on Modern Statistics (PDF), Department of Mathematical Sciences, Norwegian University of Science and Technology



