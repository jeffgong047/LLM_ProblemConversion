law: Doob's martingale convergence theorems imply that conditional expectations also have a convergence property. Let (Î©,F,P)(Î©,F,ğ) be a probability space and let X be a random variable in L1^1. Let Fâˆ—=(Fk)kâˆˆN_*=(F_k)_kâˆˆğ be any filtration of F, and define Fâˆ_âˆ to be the minimal Ïƒ-algebra generated by (Fk)kâˆˆN(F_k)_kâˆˆğ. Then Eâ¡[Xâˆ£Fk]â†’Eâ¡[Xâˆ£Fâˆ]askâ†’âˆE[X_k]â†’E[X_âˆ]askâ†’âˆ both Pğ-almost surely and in L1^1. This result is usually called LÃ©vy's zeroâ€“one law or Levy's upwards theorem. The reason for the name is that if A is an event in Fâˆ_âˆ, then the theorem says that P[Aâˆ£Fk]â†’1Ağ[A_k]â†’1_A almost surely, i.e., the limit of the probabilities is 0 or 1. In plain language, if we are learning gradually all the information that determines the outcome of an event, then we will become gradually certain what the outcome will be. This sounds almost like a tautology, but the result is still non-trivial. For instance, it easily implies Kolmogorov's zeroâ€“one law, since it says that for any tail event A, we must have P[A]=1Ağ[A]=1_A almost surely, hence P[A]âˆˆ0,1ğ[A]âˆˆ{0,1}. Similarly we have the Levy's downwards theorem : Let (Î©,F,P)(Î©,F,ğ) be a probability space and let X be a random variable in L1^1. Let (Fk)kâˆˆN(F_k)_kâˆˆğ be any decreasing sequence of sub-sigma algebras of F, and define Fâˆ_âˆ to be the intersection. Then Eâ¡[Xâˆ£Fk]â†’Eâ¡[Xâˆ£Fâˆ]askâ†’âˆE[X_k]â†’E[X_âˆ]askâ†’âˆ both Pğ-almost surely and in L1^1. See