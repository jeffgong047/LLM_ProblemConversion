law: Doob's martingale convergence theorems imply that conditional expectations also have a convergence property. Let (Ω,F,P)(Ω,F,𝐏) be a probability space and let X be a random variable in L1^1. Let F∗=(Fk)k∈N_*=(F_k)_k∈𝐍 be any filtration of F, and define F∞_∞ to be the minimal σ-algebra generated by (Fk)k∈N(F_k)_k∈𝐍. Then E⁡[X∣Fk]→E⁡[X∣F∞]ask→∞E[X_k]→E[X_∞]ask→∞ both P𝐏-almost surely and in L1^1. This result is usually called Lévy's zero–one law or Levy's upwards theorem. The reason for the name is that if A is an event in F∞_∞, then the theorem says that P[A∣Fk]→1A𝐏[A_k]→1_A almost surely, i.e., the limit of the probabilities is 0 or 1. In plain language, if we are learning gradually all the information that determines the outcome of an event, then we will become gradually certain what the outcome will be. This sounds almost like a tautology, but the result is still non-trivial. For instance, it easily implies Kolmogorov's zero–one law, since it says that for any tail event A, we must have P[A]=1A𝐏[A]=1_A almost surely, hence P[A]∈0,1𝐏[A]∈{0,1}. Similarly we have the Levy's downwards theorem : Let (Ω,F,P)(Ω,F,𝐏) be a probability space and let X be a random variable in L1^1. Let (Fk)k∈N(F_k)_k∈𝐍 be any decreasing sequence of sub-sigma algebras of F, and define F∞_∞ to be the intersection. Then E⁡[X∣Fk]→E⁡[X∣F∞]ask→∞E[X_k]→E[X_∞]ask→∞ both P𝐏-almost surely and in L1^1. See