version: One case of Rao–Blackwell theorem states: The mean squared error of the Rao–Blackwell estimator does not exceed that of the original estimator. In other words, E⁡((δ1(X)−θ)2)≤E⁡((δ(X)−θ)2).E((δ_1(X)-θ)^2)≤E((δ(X)-θ)^2). The essential tools of the proof besides the definition above are the law of total expectation and the fact that for any random variable Y, E(Y2) cannot be less than [E(Y)]2. That inequality is a case of Jensen's inequality, although it may also be shown to follow instantly from the frequently mentioned fact that 0≤Var⁡(Y)=E⁡((Y−E⁡(Y))2)=E⁡(Y2)−(E⁡(Y))2.0≤Var(Y)=E((Y-E(Y))^2)=E(Y^2)-(E(Y))^2. More precisely, the mean square error of the Rao-Blackwell estimator has the following decomposition[4] E⁡[(δ1(X)−θ)2]=E⁡[(δ(X)−θ)2]−E⁡[Var⁡(δ(X)∣T(X))]E[(δ_1(X)-θ)^2]=E[(δ(X)-θ)^2]-E[Var(δ(X)(X))] Since E⁡[Var⁡(δ(X)∣T(X))]≥0E[Var(δ(X)(X))]≥0, the Rao-Blackwell theorem immediately follows. Convex loss