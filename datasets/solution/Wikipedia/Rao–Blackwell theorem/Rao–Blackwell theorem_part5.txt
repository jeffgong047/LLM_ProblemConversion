generalization: The more general version of the Rao–Blackwell theorem speaks of the "expected loss" or risk function: E⁡(L(δ1(X)))≤E⁡(L(δ(X)))E(L(δ_1(X)))≤E(L(δ(X))) where the "loss function" L may be any convex function. If the loss function is twice-differentiable, as in the case for mean-squared-error, then we have the sharper inequality[4] E⁡(L(δ(X)))−E⁡(L(δ1(X)))≥12ET⁡[infxL″(x)Var⁡(δ(X)∣T)].E(L(δ(X)))-E(L(δ_1(X)))≥1/2E_T[inf_xL”(x)Var(δ(X))].