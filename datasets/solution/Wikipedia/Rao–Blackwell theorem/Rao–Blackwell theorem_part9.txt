variance: If the conditioning statistic is both complete and sufficient, and the starting estimator is unbiased, then the Rao–Blackwell estimator is the unique "best unbiased estimator": see Lehmann–Scheffé theorem. An example of an improvable Rao–Blackwell improvement, when using a minimal sufficient statistic that is not complete, was provided by Galili and Meilijson in 2016.[6] Let X1,…,Xn_1,…,X_n be a random sample from a scale-uniform distribution X∼U((1−k)θ,(1+k)θ),((1-k)θ,(1+k)θ), with unknown mean E[X]=θ[X]=θ and known design parameter k∈(0,1)∈(0,1). In the search for "best" possible unbiased estimators for θ,θ, it is natural to consider X1_1 as an initial (crude) unbiased estimator for θθ and then try to improve it. Since X1_1 is not a function of T=(X(1),X(n))=(X_(1),X_(n)), the minimal sufficient statistic for θθ (where X(1)=min(Xi)_(1)=min(X_i) and X(n)=max(Xi)_(n)=max(X_i)), it may be improved using the Rao–Blackwell theorem as follows: θ^RB=Eθ[X1|X(1),X(n)]=X(1)+X(n)2.θ̂_RB=E_θ[X_1|X_(1),X_(n)]=X_(1)+X_(n)/2. However, the following unbiased estimator can be shown to have lower variance: θ^LV=12(k2n−1n+1+1)[(1−k)X(1)+(1+k)X(n)].θ̂_LV=1/2(k^2n-1/n+1+1)[(1-k)X_(1)+(1+k)X_(n)]. And in fact, it could be even further improved when using the following estimator: θ^BAYES=n+1n[1−(X(1)1−k)(X(n)1+k)−1[(X(1)1−k)(X(n)1+k)]n+1−1]X(n)1+kθ̂_BAYES=n+1/n[1-(X_(1)/1-k)/(X_(n)/1+k)-1/[(X_(1)/1-k)/(X_(n)/1+k)]^n+1-1]X_(n)/1+k The model is a scale model. Optimal equivariant estimators can then be derived for loss functions that are invariant.[7] See