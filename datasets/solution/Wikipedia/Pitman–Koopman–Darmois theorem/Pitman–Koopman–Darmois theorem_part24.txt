entropy: The relative entropy (Kullback–Leibler divergence, KL divergence) of two distributions in an exponential family has a simple expression as the Bregman divergence between the natural parameters with respect to the log-normalizer.[14] The relative entropy is defined in terms of an integral, while the Bregman divergence is defined in terms of a derivative and inner product, and thus is easier to calculate and has a closed-form expression (assuming the derivative has a closed-form expression). Further, the Bregman divergence in terms of the natural parameters and the log-normalizer equals the Bregman divergence of the dual parameters (expectation parameters), in the opposite order, for the convex conjugate function.[15] Fixing an exponential family with log-normalizer A (with convex conjugate A∗^*), writing PA,θ_A,θ for the distribution in this family corresponding a fixed value of the natural parameter θθ (writing θ′θ' for another value, and with η,η′η,η' for the corresponding dual expectation/moment parameters), writing KL for the KL divergence, and BA_A for the Bregman divergence, the divergences are related as: KL(PA,θ∥PA,θ′)=BA(θ′∥θ)=BA∗(η∥η′).KL(P_A,θ_A,θ')=B_A(θ'∥θ)=B_A^*(η∥η'). The KL divergence is conventionally written with respect to the first parameter, while the Bregman divergence is conventionally written with respect to the second parameter, and thus this can be read as "the relative entropy is equal to the Bregman divergence defined by the log-normalizer on the swapped natural parameters", or equivalently as "equal to the Bregman divergence defined by the dual to the log-normalizer on the expectation parameters". Maximum-entropy