derivation: Exponential families arise naturally as the answer to the following question: what is the maximum-entropy distribution consistent with given constraints on expected values? The information entropy of a probability distribution dF(x) can only be computed with respect to some other probability distribution (or, more generally, a positive measure), and both measures must be mutually absolutely continuous. Accordingly, we need to pick a reference measure dH(x) with the same support as dF(x). The entropy of dF(x) relative to dH(x) is S[dF∣dH]=−∫dFdHlog⁡dFdHdH[dF]=-∫dF/dHlogdF/dH dH or S[dF∣dH]=∫log⁡dHdFdF[dF]=∫logdH/dF dF where dF/dH and dH/dF are Radon–Nikodym derivatives. The ordinary definition of entropy for a discrete distribution supported on a set I, namely S=−∑i∈Ipilog⁡pi=-∑_ip_i_i assumes, though this is seldom pointed out, that dH is chosen to be the counting measure on I. Consider now a collection of observable quantities (random variables) Ti. The probability distribution dF whose entropy with respect to dH is greatest, subject to the conditions that the expected value of Ti be equal to ti, is an exponential family with dH as reference measure and (T1, ..., Tn) as sufficient statistic. The derivation is a simple variational calculation using Lagrange multipliers. Normalization is imposed by letting T0 = 1 be one of the constraints. The natural parameters of the distribution are the Lagrange multipliers, and the normalization factor is the Lagrange multiplier associated to T0. For examples of such derivations, see Maximum entropy probability distribution. Role in