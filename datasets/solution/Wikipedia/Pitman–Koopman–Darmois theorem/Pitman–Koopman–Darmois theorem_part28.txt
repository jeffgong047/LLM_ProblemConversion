distributions: Exponential families are also important in Bayesian statistics. In Bayesian statistics a prior distribution is multiplied by a likelihood function and then normalised to produce a posterior distribution. In the case of a likelihood which belongs to an exponential family there exists a conjugate prior, which is often also in an exponential family. A conjugate prior π for the parameter ηη of an exponential family f(x∣η)=h(x)exp⁡(ηTT(x)−A(η))(x|η)=h(x)exp(η^T𝐓(x)-A(η)) is given by pπ(η∣χ,ν)=f(χ,ν)exp⁡(ηTχ−νA(η)),_π(η|χ,ν)=f(χ,ν)exp(η^Tχ-(η)), or equivalently pπ(η∣χ,ν)=f(χ,ν)g(η)νexp⁡(ηTχ),χ∈Rs_π(η|χ,ν)=f(χ,ν)g(η)^νexp(η^Tχ), χ∈ℝ^s where s is the dimension of ηη and ν>0ν>0 and χχ are hyperparameters (parameters controlling parameters). νν corresponds to the effective number of observations that the prior distribution contributes, and χχ corresponds to the total amount that these pseudo-observations contribute to the sufficient statistic over all observations and pseudo-observations. f(χ,ν)(χ,ν) is a normalization constant that is automatically determined by the remaining functions and serves to ensure that the given function is a probability density function (i.e. it is normalized). A(η)(η) and equivalently g(η)(η) are the same functions as in the definition of the distribution over which π is the conjugate prior. A conjugate prior is one which, when combined with the likelihood and normalised, produces a posterior distribution which is of the same type as the prior. For example, if one is estimating the success probability of a binomial distribution, then if one chooses to use a beta distribution as one's prior, the posterior is another beta distribution. This makes the computation of the posterior particularly simple. Similarly, if one is estimating the parameter of a Poisson distribution the use of a gamma prior will lead to another gamma posterior. Conjugate priors are often very flexible and can be very convenient. However, if one's belief about the likely value of the theta parameter of a binomial is represented by (say) a bimodal (two-humped) prior distribution, then this cannot be represented by a beta distribution. It can however be represented by using a mixture density as the prior, here a combination of two beta distributions; this is a form of hyperprior. An arbitrary likelihood will not belong to an exponential family, and thus in general no conjugate prior exists. The posterior will then have to be computed by numerical methods. To show that the above prior distribution is a conjugate prior, we can derive the posterior. First, assume that the probability of a single observation follows an exponential family, parameterized using its natural parameter: pF(x∣η)=h(x)g(η)exp⁡(ηTT(x))_F(x|η)=h(x)g(η)exp(η^T𝐓(x)) Then, for data X=(x1,…,xn)𝐗=(x_1,…,x_n), the likelihood is computed as follows: p(X∣η)=(∏i=1nh(xi))g(η)nexp⁡(ηT∑i=1nT(xi))(𝐗|η)=(∏_i=1^nh(x_i))g(η)^nexp(η^T∑_i=1^n𝐓(x_i)) Then, for the above conjugate prior: pπ(η∣χ,ν)=f(χ,ν)g(η)νexp⁡(ηTχ)∝g(η)νexp⁡(ηTχ)p_π(η|χ,ν) =f(χ,ν)g(η)^νexp(η^Tχ)(η)^νexp(η^Tχ) We can then compute the posterior as follows: p(η∣X,χ,ν)∝p(X∣η)pπ(η∣χ,ν)=(∏i=1nh(xi))g(η)nexp⁡(ηT∑i=1nT(xi))f(χ,ν)g(η)νexp⁡(ηTχ)∝g(η)nexp⁡(ηT∑i=1nT(xi))g(η)νexp⁡(ηTχ)∝g(η)ν+nexp⁡(ηT(χ+∑i=1nT(xi)))p(η|𝐗,χ,ν) (𝐗|η)p_π(η|χ,ν) =(∏_i=1^nh(x_i))g(η)^nexp(η^T∑_i=1^n𝐓(x_i))f(χ,ν)g(η)^νexp(η^Tχ) (η)^nexp(η^T∑_i=1^n𝐓(x_i))g(η)^νexp(η^Tχ) (η)^ν+nexp(η^T(χ+∑_i=1^n𝐓(x_i))) The last line is the kernel of the posterior distribution, i.e. p(η∣X,χ,ν)=pπ(η|χ+∑i=1nT(xi),ν+n)(η|𝐗,χ,ν)=p_π(η| χ+∑_i=1^n𝐓(x_i),ν+n.) This shows that the posterior has the same form as the prior. The data X enters into this equation only in the expression T(X)=∑i=1nT(xi),𝐓(𝐗)=∑_i=1^n𝐓(x_i), which is termed the sufficient statistic of the data. That is, the value of the sufficient statistic is sufficient to completely determine the posterior distribution. The actual data points themselves are not needed, and all sets of data points with the same sufficient statistic will have the same distribution. This is important because the dimension of the sufficient statistic does not grow with the data size — it has only as many components as the components of ηη (equivalently, the number of parameters of the distribution of a single data point). The update equations are as follows: χ′=χ+T(X)=χ+∑i=1nT(xi)ν′=ν+nχ' =χ+𝐓(𝐗) =χ+∑_i=1^n𝐓(x_i) ν' =ν+n This shows that the update equations can be written simply in terms of the number of data points and the sufficient statistic of the data. This can be seen clearly in the various examples of update equations shown in the conjugate prior page. Because of the way that the sufficient statistic is computed, it necessarily involves sums of components of the data (in some cases disguised as products or other forms — a product can be written in terms of a sum of logarithms). The cases where the update equations for particular distributions don't exactly match the above forms are cases where the conjugate prior has been expressed using a different parameterization than the one that produces a conjugate prior of the above form — often specifically because the above form is defined over the natural parameter ηη while conjugate priors are usually defined over the actual parameter θ.θ. Unbiased