proofs: In the first proof, one was able to determine the coefficients Bi of B based on the right-hand fundamental relation for the adjugate only. In fact the first n equations derived can be interpreted as determining the quotient B of the Euclidean division of the polynomial p(t)In on the left by the monic polynomial Int − A, while the final equation expresses the fact that the remainder is zero. This division is performed in the ring of polynomials with matrix coefficients. Indeed, even over a non-commutative ring, Euclidean division by a monic polynomial P is defined, and always produces a unique quotient and remainder with the same degree condition as in the commutative case, provided it is specified at which side one wishes P to be a factor (here that is to the left). To see that quotient and remainder are unique (which is the important part of the statement here), it suffices to write PQ+r=PQ′+r′+r=PQ'+r' as P(Q−Q′)=r′−r(Q-Q')=r'-r and observe that since P is monic, P(Q−Q′) cannot have a degree less than that of P, unless Q = Q′. But the dividend p(t)In and divisor Int − A used here both lie in the subring (R[A])[t], where R[A] is the subring of the matrix ring M(n, R) generated by A: the R-linear span of all powers of A. Therefore, the Euclidean division can in fact be performed within that commutative polynomial ring, and of course it then gives the same quotient B and remainder 0 as in the larger ring; in particular this shows that B in fact lies in (R[A])[t]. But, in this commutative setting, it is valid to set t to A in the equation p(t)In=(tIn−A)B;(t)I_n=(tI_n-A)B; in other words, to apply the evaluation map evA:(R[A])[t]→R[A]ev_A:(R[A])[t][A] which is a ring homomorphism, giving p(A)=0⋅evA⁡(B)=0(A)=0·ev_A(B)=0 just like in the second proof, as desired. In addition to proving the theorem, the above argument tells us that the coefficients Bi of B are polynomials in A, while from the second proof we only knew that they lie in the centralizer Z of A; in general Z is a larger subring than R[A], and not necessarily commutative. In particular the constant term B0 = adj(−A) lies in R[A]. Since A is an arbitrary square matrix, this proves that adj(A) can always be expressed as a polynomial in A (with coefficients that depend on A). In fact, the equations found in the first proof allow successively expressing Bn−1,…,B1,B0_n-1,…,B_1,B_0 as polynomials in A, which leads to the identity adj⁡(−A)=∑i=1nciAi−1,adj(-A)=∑_i=1^nc_iA^i-1, valid for all n × n matrices, where p(t)=tn+cn−1tn−1+⋯+c1t+c0(t)=t^n+c_n-1t^n-1+⋯+c_1t+c_0 is the characteristic polynomial of A. Note that this identity also implies the statement of the Cayley–Hamilton theorem: one may move adj(−A) to the right hand side, multiply the resulting equation (on the left or on the right) by A, and use the fact that −A⋅adj⁡(−A)=adj⁡(−A)⋅(−A)=det(−A)In=c0In.-A·adj(-A)=adj(-A)·(-A)=(-A)I_n=c_0I_n. See also: Faddeev–LeVerrier algorithm A proof using matrices of