coefficients: This proof is similar to the first one, but tries to give meaning to the notion of polynomial with matrix coefficients that was suggested by the expressions occurring in that proof. This requires considerable care, since it is somewhat unusual to consider polynomials with coefficients in a non-commutative ring, and not all reasoning that is valid for commutative polynomials can be applied in this setting. Notably, while arithmetic of polynomials over a commutative ring models the arithmetic of polynomial functions, this is not the case over a non-commutative ring (in fact there is no obvious notion of polynomial function in this case that is closed under multiplication). So when considering polynomials in t with matrix coefficients, the variable t must not be thought of as an "unknown", but as a formal symbol that is to be manipulated according to given rules; in particular one cannot just set t to a specific value. (f+g)(x)=∑i(fi+gi)xi=∑ifixi+∑igixi=f(x)+g(x).(f+g)(x)=∑_i(f_i+g_i)x^i=∑_if_ix^i+∑_ig_ix^i=f(x)+g(x). Let M(n,R)(n,R) be the ring of n × n matrices with entries in some ring R (such as the real or complex numbers) that has A as an element. Matrices with as coefficients polynomials in t, such as tIn−A_n-A or its adjugate B in the first proof, are elements of M(n,R[t])(n,R[t]). By collecting like powers of t, such matrices can be written as "polynomials" in t with constant matrices as coefficients; write M(n,R)[t](n,R)[t] for the set of such polynomials. Since this set is in bijection with M(n,R[t])(n,R[t]), one defines arithmetic operations on it correspondingly, in particular multiplication is given by (∑iMiti)(∑jNjtj)=∑i,j(MiNj)ti+j,(∑_iM_it^i)(∑_jN_jt^j)=∑_i,j(M_iN_j)t^i+j, respecting the order of the coefficient matrices from the two operands; obviously this gives a non-commutative multiplication. Thus, the identity (tIn−A)B=p(t)In.(tI_n-A)B=p(t)I_n. from the first proof can be viewed as one involving a multiplication of elements in M(n,R)[t](n,R)[t]. At this point, it is tempting to simply set t equal to the matrix A, which makes the first factor on the left equal to the zero matrix, and the right hand side equal to p(A); however, this is not an allowed operation when coefficients do not commute. It is possible to define a "right-evaluation map" evA : M[t ] → M, which replaces each t i by the matrix power Ai of A, where one stipulates that the power is always to be multiplied on the right to the corresponding coefficient. But this map is not a ring homomorphism: the right-evaluation of a product differs in general from the product of the right-evaluations. This is so because multiplication of polynomials with matrix coefficients does not model multiplication of expressions containing unknowns: a product MtiNtj=(M⋅N)ti+j^iNt^j=(M)t^i+j is defined assuming that t commutes with N, but this may fail if t is replaced by the matrix A. One can work around this difficulty in the particular situation at hand, since the above right-evaluation map does become a ring homomorphism if the matrix A is in the center of the ring of coefficients, so that it commutes with all the coefficients of the polynomials (the argument proving this is straightforward, exactly because commuting t with coefficients is now justified after evaluation). Now, A is not always in the center of M, but we may replace M with a smaller ring provided it contains all the coefficients of the polynomials in question: In_n, A, and the coefficients Bi_i of the polynomial B. The obvious choice for such a subring is the centralizer Z of A, the subring of all matrices that commute with A; by definition A is in the center of Z. This centralizer obviously contains In_n, and A, but one has to show that it contains the matrices Bi_i. To do this, one combines the two fundamental relations for adjugates, writing out the adjugate B as a polynomial: (∑i=0mBiti)(tIn−A)=(tIn−A)∑i=0mBiti∑i=0mBiti+1−∑i=0mBiAti=∑i=0mBiti+1−∑i=0mABiti∑i=0mBiAti=∑i=0mABiti.(∑_i=0^mB_it^i)(tI_n-A) =(tI_n-A)∑_i=0^mB_it^i ∑_i=0^mB_it^i+1-∑_i=0^mB_iAt^i =∑_i=0^mB_it^i+1-∑_i=0^mAB_it^i ∑_i=0^mB_iAt^i =∑_i=0^mAB_it^i. Equating the coefficients shows that for each i, we have ABi = Bi A as desired. Having found the proper setting in which evA is indeed a homomorphism of rings, one can complete the proof as suggested above: evA⁡(p(t)In)=evA⁡((tIn−A)B)p(A)=evA⁡(tIn−A)⋅evA⁡(B)p(A)=(AIn−A)⋅evA⁡(B)=O⋅evA⁡(B)=O.ev_A(p(t)I_n) =ev_A((tI_n-A)B) p(A) =ev_A(tI_n-A)·ev_A(B) p(A) =(AI_n-A)·ev_A(B)=O·ev_A(B)=O. This completes the proof. A synthesis of the first two