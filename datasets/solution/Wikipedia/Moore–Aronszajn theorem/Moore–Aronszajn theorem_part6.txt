maps: A feature map is a map φ:X→Fφ, where F is a Hilbert space which we will call the feature space. The first sections presented the connection between bounded/continuous evaluation functions, positive definite functions, and integral operators and in this section we provide another representation of the RKHS in terms of feature maps. Every feature map defines a kernel via K(x,y)=⟨φ(x),φ(y)⟩F.(x,y)=⟨φ(x),φ(y)⟩_F. (3) Clearly K is symmetric and positive definiteness follows from the properties of inner product in F. Conversely, every positive definite function and corresponding reproducing kernel Hilbert space has infinitely many associated feature maps such that (3) holds. For example, we can trivially take F=H=H and φ(x)=Kxφ(x)=K_x for all x∈X. Then (3) is satisfied by the reproducing property. Another classical example of a feature map relates to the previous section regarding integral operators by taking F=ℓ2=ℓ^2 and φ(x)=(σiφi(x))iφ(x)=(√(σ_i)φ_i(x))_i. This connection between kernels and feature maps provides us with a new way to understand positive definite functions and hence reproducing kernels as inner products in H. Moreover, every feature map can naturally define a RKHS by means of the definition of a positive definite function. Lastly, feature maps allow us to construct function spaces that reveal another perspective on the RKHS. Consider the linear space Hφ=f:X→R∣∃w∈F,f(x)=⟨w,φ(x)⟩F,∀x∈X._φ={f:X→ℝ|,f(x)=,φ(x)⟩_F,∀x}. We can define a norm on Hφ_φ by ‖f‖φ=inf‖w‖F:w∈F,f(x)=⟨w,φ(x)⟩F,∀x∈X.f_φ=inf{w_F:w,f(x)=,φ(x)⟩_F,∀x}. It can be shown that Hφ_φ is a RKHS with kernel defined by K(x,y)=⟨φ(x),φ(y)⟩F(x,y)=⟨φ(x),φ(y)⟩_F. This representation implies that the elements of the RKHS are inner products of elements in the feature space and can accordingly be seen as hyperplanes. This view of the RKHS is related to the kernel trick in machine learning.[7]