function: The ReLU function is commonly defined as f(x)=max0,x(x)=max{0,x} and is a mainstay in the architecture of neural networks where it is used as an activation function. One can construct a ReLU-like nonlinear function using the theory of reproducing kernel Hilbert spaces. Below, we derive this construction and show how it implies the representation power of neural networks with ReLU activations. We will work with the Hilbert space H=L21(0)[0,∞)ℋ=L_2^1(0)[0,∞) of absolutely continuous functions with f(0)=0(0)=0 and square integrable (i.e. L2_2) derivative. It has the inner product ⟨f,g⟩H=∫0∞f′(x)g′(x)dx.,g⟩_ℋ=∫_0^∞f'(x)g'(x) dx. To construct the reproducing kernel it suffices to consider a dense subspace, so let f∈C1[0,∞)^1[0,∞) and f(0)=0(0)=0. The Fundamental Theorem of Calculus then gives f(y)=∫0yf′(x)dx=∫0∞G(x,y)f′(x)dx=⟨Ky,f⟩(y)=∫_0^yf'(x) dx=∫_0^∞G(x,y)f'(x) dx=_y,f⟩ where G(x,y)=1,x<y0,otherwise(x,y)=1, x<y 0, otherwise and Ky′(x)=G(x,y),Ky(0)=0_y'(x)=G(x,y),_y(0)=0 i.e. K(x,y)=Ky(x)=∫0xG(z,y)dz=x,0≤x<yy,otherwise.=min(x,y)(x,y)=K_y(x)=∫_0^xG(z,y) dz=x, 0<y y, otherwise.=min(x,y) This implies Ky=K(⋅,y)_y=K(·,y) reproduces f. Moreover the minimum function on X×X=[0,∞)×[0,∞)=[0,∞)×[0,∞) has the following representations with the ReLu function: min(x,y)=x−ReLU⁡(x−y)=y−ReLU⁡(y−x).min(x,y)=x-ReLU(x-y)=y-ReLU(y-x). Using this formulation, we can apply the representer theorem to the RKHS, letting one prove the optimality of using ReLU activations in neural network settings.[citation needed] See