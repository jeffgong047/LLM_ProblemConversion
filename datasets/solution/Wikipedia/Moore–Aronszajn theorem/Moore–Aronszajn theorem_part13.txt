functions: In this section we extend the definition of the RKHS to spaces of vector-valued functions as this extension is particularly important in multi-task learning and manifold regularization. The main difference is that the reproducing kernel ΓΓ is a symmetric function that is now a positive semi-definite matrix for every x,y,y in X. More formally, we define a vector-valued RKHS (vvRKHS) as a Hilbert space of functions f:X→RT:X→ℝ^T such that for all c∈RT∈ℝ^T and x∈X Γxc(y)=Γ(x,y)c∈Hfory∈XΓ_xc(y)=Γ(x,y)cfory and ⟨f,Γxc⟩H=f(x)⊺c.,Γ_xc⟩_H=f(x)^⊺c. This second property parallels the reproducing property for the scalar-valued case. This definition can also be connected to integral operators, bounded evaluation functions, and feature maps as we saw for the scalar-valued RKHS. We can equivalently define the vvRKHS as a vector-valued Hilbert space with a bounded evaluation functional and show that this implies the existence of a unique reproducing kernel by the Riesz Representation theorem. Mercer's theorem can also be extended to address the vector-valued setting and we can therefore obtain a feature map view of the vvRKHS. Lastly, it can also be shown that the closure of the span of Γxc:x∈X,c∈RT{Γ_xc:x,c∈ℝ^T} coincides with H, another property similar to the scalar-valued case. We can gain intuition for the vvRKHS by taking a component-wise perspective on these spaces. In particular, we find that every vvRKHS is isometrically isomorphic to a scalar-valued RKHS on a particular input space. Let Λ=1,…,TΛ={1,…,T}. Consider the space X×Λ×Λ and the corresponding reproducing kernel γ:X×Λ×X×Λ→R.γ:X×Λ×Λ→ℝ. (4) As noted above, the RKHS associated to this reproducing kernel is given by the closure of the span of γ(x,t):x∈X,t∈Λ{γ_(x,t):x,t∈Λ} where γ(x,t)(y,s)=γ((x,t),(y,s))γ_(x,t)(y,s)=γ((x,t),(y,s)) for every set of pairs (x,t),(y,s)∈X×Λ(x,t),(y,s)×Λ. The connection to the scalar-valued RKHS can then be made by the fact that every matrix-valued kernel can be identified with a kernel of the form of (4) via Γ(x,y)(t,s)=γ((x,t),(y,s)).Γ(x,y)_(t,s)=γ((x,t),(y,s)). Moreover, every kernel with the form of (4) defines a matrix-valued kernel with the above expression. Now letting the map D:HΓ→Hγ:H_Γ_γ be defined as (Df)(x,t)=⟨f(x),et⟩RT(Df)(x,t)=(x),e_t⟩_ℝ^T where et_t is the tth^th component of the canonical basis for RTℝ^T, one can show that D is bijective and an isometry between HΓ_Γ and Hγ_γ. While this view of the vvRKHS can be useful in multi-task learning, this isometry does not reduce the study of the vector-valued case to that of the scalar-valued case. In fact, this isometry procedure can make both the scalar-valued kernel and the input space too difficult to work with in practice as properties of the original kernels are often lost.[10][11][12] An important class of matrix-valued reproducing kernels are separable kernels which can factorized as the product of a scalar valued kernel and a T-dimensional symmetric positive semi-definite matrix. In light of our previous discussion these kernels are of the form γ((x,t),(y,s))=K(x,y)KT(t,s)γ((x,t),(y,s))=K(x,y)K_T(t,s) for all x,y,y in X and t,s,s in T. As the scalar-valued kernel encodes dependencies between the inputs, we can observe that the matrix-valued kernel encodes dependencies among both the inputs and the outputs. We lastly remark that the above theory can be further extended to spaces of functions with values in function spaces but obtaining kernels for these spaces is a more difficult task.[13] Connection between RKHSs and the ReLU