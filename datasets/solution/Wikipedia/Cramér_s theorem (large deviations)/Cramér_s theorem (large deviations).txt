Cramér's theorem is a fundamental result in the theory of large deviations, a subdiscipline of probability theory. It determines the rate function of a series of iid random variables.
A weak version of this result was first shown by Harald Cramér in 1938.

Statement[edit]
The logarithmic moment generating function (which is the cumulant-generating function) of a random variable is defined as:

Λ(t)=log⁡E⁡[exp⁡(tX1)].Λ(t)=logE[exp(tX_1)].
Let X1,X2,…_1,X_2,… be a sequence of iid real random variables with finite logarithmic moment generating function, i.e. Λ(t)<∞Λ(t)<∞ for all t∈R∈ℝ.
Then the Legendre transform of ΛΛ:

Λ∗(x):=supt∈R(tx−Λ(t))Λ^*(x):=sup_t∈ℝ(tx-Λ(t))
satisfies,

limn→∞1nlog⁡(P(∑i=1nXi≥nx))=−Λ∗(x)lim_n→∞1/nlog(P(∑_i=1^nX_i))=-Λ^*(x)
for all x>E⁡[X1].>E[X_1].
In the terminology of the theory of large deviations the result can be reformulated as follows:
If X1,X2,…_1,X_2,… is a series of iid random variables, then the distributions (L(1n∑i=1nXi))n∈N(ℒ(1n∑_i=1^nX_i))_n∈ℕ satisfy a large deviation principle with rate function Λ∗Λ^*.

References[edit]
Klenke, Achim (2008). Probability Theory. Berlin: Springer. pp. 508. doi:10.1007/978-1-84800-048-3. ISBN 978-1-84800-047-6.
"Cramér theorem", Encyclopedia of Mathematics, EMS Press, 2001 [1994]



