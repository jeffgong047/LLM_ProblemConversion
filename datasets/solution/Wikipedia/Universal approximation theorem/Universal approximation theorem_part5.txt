case: The first result on approximation capabilities of neural networks with bounded number of layers, each containing a limited number of artificial neurons was obtained by Maiorov and Pinkus.[25] Their remarkable result revealed that such networks can be universal approximators and for achieving this property two hidden layers are enough. Universal approximation theorem:[25] There exists an activation function ÏƒÏƒ which is analytic, strictly increasing and sigmoidal and has the following property: For any fâˆˆC[0,1]d[0,1]^d and Îµ>0Îµ>0 there exist constants di,cij,Î¸ij,Î³i_i,c_ij,Î¸_ij,Î³_i, and vectors wijâˆˆRdğ°^ijâˆˆâ„^d for which |f(x)âˆ’âˆ‘i=16d+3diÏƒ(âˆ‘j=13dcijÏƒ(wijâ‹…xâˆ’Î¸ij)âˆ’Î³i)|<Îµ(ğ±)-âˆ‘_i=1^6d+3d_iÏƒ(âˆ‘_j=1^3dc_ijÏƒ(ğ°^ijÂ·ğ±-Î¸_ij)-Î³_i)|<Îµ for all x=(x1,...,xd)âˆˆ[0,1]dğ±=(x_1,...,x_d)âˆˆ[0,1]^d. This is an existence result. It says that activation functions providing universal approximation property for bounded depth bounded width networks exist. Using certain algorithmic and computer programming techniques, Guliyev and Ismailov efficiently constructed such activation functions depending on a numerical parameter. The developed algorithm allows one to compute the activation functions at any point of the real axis instantly. For the algorithm and the corresponding computer code see.[26] The theoretical result can be formulated as follows. Universal approximation theorem:[26][27] Let [a,b][a,b] be a finite segment of the real line, s=bâˆ’a=b-a and Î»Î» be any positive number. Then one can algorithmically construct a computable sigmoidal activation function Ïƒ:Râ†’RÏƒâ„â†’â„, which is infinitely differentiable, strictly increasing on (âˆ’âˆ,s)(-âˆ,s), Î»Î» -strictly increasing on [s,+âˆ)[s,+âˆ), and satisfies the following properties: 1) For any fâˆˆC[a,b][a,b] and Îµ>0Îµ>0 there exist numbers c1,c2,Î¸1_1,c_2,Î¸_1 and Î¸2Î¸_2 such that for all xâˆˆ[a,b]âˆˆ[a,b] |f(x)âˆ’c1Ïƒ(xâˆ’Î¸1)âˆ’c2Ïƒ(xâˆ’Î¸2)|<Îµ|f(x)-c_1Ïƒ(x-Î¸_1)-c_2Ïƒ(x-Î¸_2)|<Îµ 2) For any continuous function F on the d-dimensional box [a,b]d[a,b]^d and Îµ>0Îµ>0, there exist constants ep_p, cpq_pq, Î¸pqÎ¸_pq and Î¶pÎ¶_p such that the inequality |F(x)âˆ’âˆ‘p=12d+2epÏƒ(âˆ‘q=1dcpqÏƒ(wqâ‹…xâˆ’Î¸pq)âˆ’Î¶p)|<Îµ|F(ğ±)-âˆ‘_p=1^2d+2e_pÏƒ(âˆ‘_q=1^dc_pqÏƒ(ğ°^qÂ·ğ±-Î¸_pq)-Î¶_p)|<Îµ holds for all x=(x1,â€¦,xd)âˆˆ[a,b]dğ±=(x_1,â€¦,x_d)âˆˆ[a,b]^d. Here the weights wqğ°^q, q=1,â€¦,d=1,â€¦,d, are fixed as follows: w1=(1,0,â€¦,0),w2=(0,1,â€¦,0),â€¦,wd=(0,0,â€¦,1).ğ°^1=(1,0,â€¦,0), ğ°^2=(0,1,â€¦,0), â€¦, ğ°^d=(0,0,â€¦,1). In addition, all the coefficients ep_p, except one, are equal. Here â€œÏƒ:Râ†’RÏƒâ„â†’â„ is Î»Î»-strictly increasing on some set Xâ€ means that there exists a strictly increasing function u:Xâ†’Râ†’â„ such that |Ïƒ(x)âˆ’u(x)|â‰¤Î»|Ïƒ(x)-u(x)|â‰¤Î» for all xâˆˆX. Clearly, a Î»Î»-increasing function behaves like a usual increasing function as Î»Î» gets small. In the "depth-width" terminology, the above theorem says that for certain activation functions depth-22 width-22 networks are universal approximators for univariate functions and depth-33 width-(2d+2)(2d+2) networks are universal approximators for d-variable functions (d>1>1). Graph