case: The "dual" versions of the theorem consider networks of bounded width and arbitrary depth. A variant of the universal approximation theorem was proved for the arbitrary depth case by Zhou Lu et al. in 2017.[18] They showed that networks of width n + 4 with ReLU activation functions can approximate any Lebesgue-integrable function on n-dimensional input space with respect to L1^1 distance if network depth is allowed to grow. It was also shown that if the width was less than or equal to n, this general expressive power to approximate any Lebesgue integrable function was lost. In the same paper[18] it was shown that ReLU networks with width n + 1 were sufficient to approximate any continuous function of n-dimensional input variables.[35] The following refinement, specifies the optimal minimum width for which such an approximation is possible and is due to.[36] Universal approximation theorem (L1 distance, ReLU activation, arbitrary depth, minimal width). For any Bochner–Lebesgue p-integrable function f:Rn→Rm:ℝ^n→ℝ^m and any ε>0ε>0, there exists a fully connected ReLU network F of width exactly dm=maxn+1,m_m=max{n+1,m}, satisfying ∫Rn‖f(x)−F(x)‖pdx<ε.∫_ℝ^nf(x)-F(x)^p dx<ε. Moreover, there exists a function f∈Lp(Rn,Rm)^p(ℝ^n,ℝ^m) and some ε>0ε>0, for which there is no fully connected ReLU network of width less than dm=maxn+1,m_m=max{n+1,m} satisfying the above approximation bound. Remark: If the activation is replaced by leaky-ReLU, and the input is restricted in a compact domain, then the exact minimum width is[24] dm=maxn,m,2_m=max{n,m,2}. Quantitative refinement: In the case where, when X=[0,1]d𝒳=[0,1]^d and D=1=1 and where σσ is the ReLU activation function, the exact depth and width for a ReLU network to achieve εε error is also known.[37] If, moreover, the target function f is smooth, then the required number of layer and their width can be exponentially smaller.[38] Even if f is not smooth, the curse of dimensionality can be broken if f admits additional "compositional structure".[39][40] Together, the central result of[20] yields the following universal approximation theorem for networks with bounded width (see also[16] for the first result of this kind). Universal approximation theorem (Uniform non-affine activation, arbitrary depth, constrained width). Let X𝒳 be a compact subset of Rdℝ^d. Let σ:R→Rσ:ℝ→ℝ be any non-affine continuous function which is continuously differentiable at at least one point, with nonzero derivative at that point. Let Nd,D:d+D+2σ𝒩_d,D:d+D+2^σ denote the space of feed-forward neural networks with d input neurons, D output neurons, and an arbitrary number of hidden layers each with d+D+2+D+2 neurons, such that every hidden neuron has activation function σσ and every output neuron has the identity as its activation function, with input layer ϕϕ and output layer ρρ. Then given any ε>0ε>0 and any f∈C(X,RD)(𝒳,ℝ^D), there exists f^∈Nd,D:d+D+2σf̂∈𝒩_d,D:d+D+2^σ such that supx∈X‖f^(x)−f(x)‖<ε.sup_x∈𝒳f̂(x)-f(x)<ε. In other words, N𝒩 is dense in C(X;RD)(𝒳;ℝ^D) with respect to the topology of uniform convergence. Quantitative refinement: The number of layers and the width of each layer required to approximate f to εε precision known;[21] moreover, the result hold true when X𝒳 and RDℝ^D are replaced with any non-positively curved Riemannian manifold. Certain necessary conditions for the bounded width, arbitrary depth case have been established, but there is still a gap between the known sufficient and necessary conditions.[18][19][41] Bounded depth and bounded width