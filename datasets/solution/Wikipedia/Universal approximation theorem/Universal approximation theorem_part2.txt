History: One of the first versions of the arbitrary width case was proven by George Cybenko in 1989 for sigmoid activation functions.[11] Kurt Hornik [de], Maxwell Stinchcombe, and Halbert White showed in 1989 that multilayer feed-forward networks with as few as one hidden layer are universal approximators.[1] Hornik also showed in 1991[12] that it is not the specific choice of the activation function but rather the multilayer feed-forward architecture itself that gives neural networks the potential of being universal approximators. Moshe Leshno et al in 1993[13] and later Allan Pinkus in 1999[14] showed that the universal approximation property is equivalent to having a nonpolynomial activation function. In 2022, Shen Zuowei, Haizhao Yang, and Shijun Zhang[15] obtained precise quantitative information on the depth and width required to approximate a target function by deep and wide ReLU neural networks. The arbitrary depth case was also studied by a number of authors such as Gustaf Gripenberg in 2003,[16] Dmitry Yarotsky,[17] Zhou Lu et al in 2017,[18] Boris Hanin and Mark Sellke in 2018[19] who focused on neural networks with ReLU activation function. In 2020, Patrick Kidger and Terry Lyons[20] extended those results to neural networks with general activation functions such, e.g. tanh, GeLU, or Swish, and in 2022, their result was made quantitative by Leonie Papon and Anastasis Kratsios[21] who derived explicit depth estimates depending on the regularity of the target function and of the activation function. The question of minimal possible width for universality was first studied in 2021, Park et al obtained the minimum width required for the universal approximation of Lp functions using feed-forward neural networks with ReLU as activation functions.[10] Similar results that can be directly applied to residual neural networks were also obtained in the same year by Paulo Tabuada and Bahman Gharesifard using control-theoretic arguments.[22][23] In 2023, Cai[24] obtained the optimal minimum width bound for the universal approximation. The bounded depth and bounded width case was first studied by Maiorov and Pinkus in 1999.[25] They showed that there exists an analytic sigmoidal activation function such that two hidden layer neural networks with bounded number of units in hidden layers are universal approximators. Using algorithmic and computer programming techniques, Guliyev and Ismailov constructed a smooth sigmoidal activation function providing universal approximation property for two hidden layer feedforward neural networks with less units in hidden layers.[26] It was constructively proved in 2018 paper[27] that single hidden layer networks with bounded width are still universal approximators for univariate functions, but this property is no longer true for multivariable functions. Several extensions of the theorem exist, such as to discontinuous activation functions,[13] noncompact domains,[20] certifiable networks,[28] random neural networks,[29] and alternative network architectures and topologies.[20][30] Arbitrary-width