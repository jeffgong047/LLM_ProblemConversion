law: During 1928, Hartley formulated a way to quantify information and its line rate (also known as data signalling rate R bits per second).[2] This method, later known as Hartley's law, became an important precursor for Shannon's more sophisticated notion of channel capacity. Hartley argued that the maximum number of distinguishable pulse levels that can be transmitted and received reliably over a communications channel is limited by the dynamic range of the signal amplitude and the precision with which the receiver can distinguish amplitude levels. Specifically, if the amplitude of the transmitted signal is restricted to the range of [−A ... +A] volts, and the precision of the receiver is ±ΔV volts, then the maximum number of distinct pulses M is given by M=1+AΔV=1+A. By taking information per pulse in bit/pulse to be the base-2-logarithm of the number of distinct messages M that could be sent, Hartley[3] constructed a measure of the line rate R as: R=fplog2⁡(M),=f_plog_2(M), where fp_p is the pulse rate, also known as the symbol rate, in symbols/second or baud. Hartley then combined the above quantification with Nyquist's observation that the number of independent pulses that could be put through a channel of bandwidth B hertz was 2B2B pulses per second, to arrive at his quantitative measure for achievable line rate. Hartley's law is sometimes quoted as just a proportionality between the analog bandwidth, B, in Hertz and what today is called the digital bandwidth, R, in bit/s.[4] Other times it is quoted in this more quantitative form, as an achievable line rate of R bits per second:[5] R≤2Blog2⁡(M).≤2Blog_2(M). Hartley did not work out exactly how the number M should depend on the noise statistics of the channel, or how the communication could be made reliable even when individual symbol pulses could not be reliably distinguished to M levels; with Gaussian noise statistics, system designers had to choose a very conservative value of M to achieve a low error rate. The concept of an error-free capacity awaited Claude Shannon, who built on Hartley's observations about a logarithmic measure of information and Nyquist's observations about the effect of bandwidth limitations. Hartley's rate result can be viewed as the capacity of an errorless M-ary channel of 2B2B symbols per second. Some authors refer to it as a capacity. But such an errorless channel is an idealization, and if M is chosen small enough to make the noisy channel nearly errorless, the result is necessarily less than the Shannon capacity of the noisy channel of bandwidth B, which is the Hartley–Shannon result that followed later. Noisy channel coding theorem and