In probability theory, Kolmogorov's Three-Series Theorem, named after Andrey Kolmogorov, gives a criterion for the almost sure convergence of an infinite series of random variables in terms of the convergence of three different series involving properties of their probability distributions. Kolmogorov's three-series theorem, combined with Kronecker's lemma, can be used to give a relatively easy proof of the Strong Law of Large Numbers.[1]


Statement of the theorem[edit]
Let (Xn)nâˆˆN(X_n)_nâˆˆâ„• be independent random variables. The random series âˆ‘n=1âˆXnâˆ‘_n=1^âˆX_n converges almost surely in Râ„ if the following conditions hold for some A>0>0, and only if the following conditions hold for any A>0>0:

âˆ‘n=1âˆP(|Xn|â‰¥A)âˆ‘_n=1^âˆâ„™(|X_n|) converges.Let Yn=Xn1|Xn|â‰¤A_n=X_n1_{|X_n|}. Then âˆ‘n=1âˆE[Yn]âˆ‘_n=1^âˆğ”¼[Y_n], the series of expected values of Yn_n, converges.âˆ‘n=1âˆvar(Yn)âˆ‘_n=1^âˆvar(Y_n) converges, where Yn_n is defined as in the second condition.
Proof[edit]
Sufficiency of conditions ("if")[edit]
Condition (i) and Borelâ€“Cantelli give that Xn=Yn_n=Y_n for n large, almost surely. Hence âˆ‘n=1âˆXnâˆ‘_n=1^âˆX_n converges if and only if âˆ‘n=1âˆYnâˆ‘_n=1^âˆY_n converges. Conditions (ii)-(iii) and Kolmogorov's Two-Series Theorem give the almost sure convergence of âˆ‘n=1âˆYnâˆ‘_n=1^âˆY_n.

Necessity of conditions ("only if")[edit]
Suppose that âˆ‘n=1âˆXnâˆ‘_n=1^âˆX_n converges almost surely.
Without condition (i), by Borelâ€“Cantelli there would exist some A>0>0 such that |Xn|â‰¥A{|X_n|} for infinitely many n, almost surely. But then the series would diverge. Therefore, we must have condition (i).
We see that condition (iii) implies condition (ii): Kolmogorov's two-series theorem along with condition (i) applied to the case A=1=1 gives the convergence of âˆ‘n=1âˆ(Ynâˆ’E[Yn])âˆ‘_n=1^âˆ(Y_n-ğ”¼[Y_n]). So given the convergence of âˆ‘n=1âˆYnâˆ‘_n=1^âˆY_n, we have âˆ‘n=1âˆE[Yn]âˆ‘_n=1^âˆğ”¼[Y_n]  converges, so condition (ii) is implied.
Thus, it only remains to demonstrate the necessity of condition (iii), and we will have obtained the full result. It is equivalent to check condition (iii) for the series
âˆ‘n=1âˆZn=âˆ‘n=1âˆ(Ynâˆ’Ynâ€²)âˆ‘_n=1^âˆZ_n=âˆ‘_n=1^âˆ(Y_n-Y'_n) where for each n, Yn_n and Ynâ€²'_n are IIDâ€”that is, to employ the assumption that E[Yn]=0ğ”¼[Y_n]=0, since Zn_n is a sequence of random variables bounded by 2, converging almost surely, and with var(Zn)=2var(Yn)var(Z_n)=2var(Y_n). So we wish to check that if âˆ‘n=1âˆZnâˆ‘_n=1^âˆZ_n converges, then âˆ‘n=1âˆvar(Zn)âˆ‘_n=1^âˆvar(Z_n) converges as well. This is a special case of a more general result from martingale theory with summands equal to the increments of a martingale sequence and the same conditions (E[Zn]=0ğ”¼[Z_n]=0; the series of the variances is converging; and the summands are bounded).[2][3][4]

Example[edit]
As an illustration of the theorem, consider the example of the harmonic series with random signs:

âˆ‘n=1âˆÂ±1n.âˆ‘_n=1^âˆÂ±1/n.
Here, "Â±Â±" means that each term 1/n1/n is taken with a random sign that is either 11 or âˆ’1-1 with respective probabilities 1/2,1/21/2,/2, and all random signs are chosen independently. Let Xn_n in the theorem denote a random variable that takes the values 1/n1/n and âˆ’1/n-1/n with equal probabilities. With A=2=2 the summands of the first two series are identically zero and var(Yn)=nâˆ’2^-2. The conditions of the theorem are then satisfied, so it follows that the harmonic series with random signs converges almost surely. On the other hand, the analogous series of (for example) square root reciprocals with random signs, namely

âˆ‘n=1âˆÂ±1n,âˆ‘_n=1^âˆÂ±1/âˆš(n),
diverges almost surely, since condition (3) in the theorem is not satisfied for any A. Note that this is different from the behavior of the analogous series with alternating signs, âˆ‘n=1âˆ(âˆ’1)n/nâˆ‘_n=1^âˆ(-1)^n/âˆš(n), which does converge.

Notes[edit]

^ Durrett, Rick. "Probability: Theory and Examples." Duxbury advanced series, Third Edition, Thomson Brooks/Cole, 2005, Section 1.8, pp. 60â€“69.

^ Sun, Rongfeng. Lecture notes. http://www.math.nus.edu.sg/~matsr/ProbI/Lecture4.pdf Archived 2018-04-17 at the Wayback Machine

^ M. LoÃ¨ve, "Probability theory", Princeton Univ. Press (1963) pp. Sect. 16.3

^ W. Feller, "An introduction to probability theory and its applications", 2, Wiley (1971) pp. Sect. IX.9





