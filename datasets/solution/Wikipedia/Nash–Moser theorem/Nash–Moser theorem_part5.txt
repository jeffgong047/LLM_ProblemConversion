solution: This section only aims to describe an idea, and as such it is intentionally imprecise. For concreteness, suppose that P is an order-one differential operator on some function spaces, so that it defines a map P: Ck+1 → Ck for each k. Suppose that, at some Ck+1 function f, the linearization DPf: Ck+1 → Ck has a right inverse S: Ck → Ck; in the above language this reflects a "loss of one derivative". One can concretely see the failure of trying to use Newton's method to prove the Banach space implicit function theorem in this context: if g∞ is close to P(f) in Ck and one defines the iteration fn+1=fn+S(g∞−P(fn)),_n+1=f_n+S(g_∞-P(f_n)), then f1∈Ck+1 implies that g∞−P(fn) is in Ck, and then f2 is in Ck. By the same reasoning, f3 is in Ck-1, and f4 is in Ck-2, and so on. In finitely many steps the iteration must end, since it will lose all regularity and the next step will not even be defined. Nash's solution is quite striking in its simplicity. Suppose that for each n>0 one has a smoothing operator θn which takes a Ck function, returns a smooth function, and approximates the identity when n is large. Then the "smoothed" Newton iteration fn+1=fn+S(θn(g∞−P(fn)))_n+1=f_n+S(θ_n(g_∞-P(f_n))) transparently does not encounter the same difficulty as the previous "unsmoothed" version, since it is an iteration in the space of smooth functions which never loses regularity. So one has a well-defined sequence of functions; the major surprise of Nash's approach is that this sequence actually converges to a function f∞ with P(f∞) = g∞. For many mathematicians, this is rather surprising, since the "fix" of throwing in a smoothing operator seems too superficial to overcome the deep problem in the standard Newton method. For instance, on this point Mikhael Gromov says You must be a novice in analysis or a genius like Nash to believe anything like that can be ever true. [...] [This] may strike you as realistic as a successful performance of perpetuum mobile with a mechanical implementation of Maxwell’s demon... unless you start following Nash’s computation and realize to your immense surprise that the smoothing does work. Remark. The true "smoothed Newton iteration" is a little more complicated than the above form, although there are a few inequivalent forms, depending on where one chooses to insert the smoothing operators. The primary difference is that one requires invertibility of DPf for an entire open neighborhood of choices of f, and then one uses the "true" Newton iteration, corresponding to (using single-variable notation) xn+1=xn−f(xn)f′(xn)_n+1=x_n-f(x_n)/f'(x_n) as opposed to xn+1=xn−f(xn)f′(x0),_n+1=x_n-f(x_n)/f'(x_0), the latter of which reflects the forms given above. This is rather important, since the improved quadratic convergence of the "true" Newton iteration is significantly used to combat the error of "smoothing," in order to obtain convergence. Certain approaches, in particular Nash's and Hamilton's, follow the solution of an ordinary differential equation in function space rather than an iteration in function space; the relation of the latter to the former is essentially that of the solution of Euler's method to that of a differential equation. Hamilton's formulation of the