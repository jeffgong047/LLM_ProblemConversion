matrices: We begin by considering a Hermitian matrix on Cnℂ^n (but the following discussion will be adaptable to the more restrictive case of symmetric matrices on Rnℝ^n). We consider a Hermitian map A on a finite-dimensional complex inner product space V endowed with a positive definite sesquilinear inner product ⟨⋅,⋅⟩⟨·,·⟩. The Hermitian condition on A means that for all x, y ∈ V, ⟨Ax,y⟩=⟨x,Ay⟩.,y⟩=,Ay⟩. An equivalent condition is that A* = A, where A* is the Hermitian conjugate of A. In the case that A is identified with a Hermitian matrix, the matrix of A* is equal to its conjugate transpose. (If A is a real matrix, then this is equivalent to AT = A, that is, A is a symmetric matrix.) This condition implies that all eigenvalues of a Hermitian map are real: To see this, it is enough to apply it to the case when x = y is an eigenvector. (Recall that an eigenvector of a linear map A is a non-zero vector x such that Ax = λx for some scalar λ. The value λ is the corresponding eigenvalue. Moreover, the eigenvalues are roots of the characteristic polynomial.) Theorem — If A is Hermitian on V, then there exists an orthonormal basis of V consisting of eigenvectors of A. Each eigenvalue of A is real. We provide a sketch of a proof for the case where the underlying field of scalars is the complex numbers. By the fundamental theorem of algebra, applied to the characteristic polynomial of A, there is at least one eigenvalue λ1 and eigenvector e1, which must by definition be non-zero. Then since λ1⟨e1,e1⟩=⟨A(e1),e1⟩=⟨e1,A(e1)⟩=λ¯1⟨e1,e1⟩,λ_1_1,e_1⟩=(e_1),e_1⟩=_1,A(e_1)⟩=λ̅_1_1,e_1⟩, we find that λ1 is real. Now consider the space K = span{e1}⊥, the orthogonal complement of e1. By Hermiticity, K is an invariant subspace of A. Applying the same argument to K shows that A has an eigenvector e2 ∈ K. Finite induction then finishes the proof. The spectral theorem holds also for symmetric maps on finite-dimensional real inner product spaces, but the existence of an eigenvector does not follow immediately from the fundamental theorem of algebra. To prove this, consider A as a Hermitian matrix and use the fact that all eigenvalues of a Hermitian matrix are real. The matrix representation of A in a basis of eigenvectors is diagonal, and by the construction the proof gives a basis of mutually orthogonal eigenvectors; by choosing them to be unit vectors one obtains an orthonormal basis of eigenvectors. A can be written as a linear combination of pairwise orthogonal projections, called its spectral decomposition. Let Vλ=v∈V:Av=λv_λ={v:Av=} be the eigenspace corresponding to an eigenvalue λ. Note that the definition does not depend on any choice of specific eigenvectors. V is the orthogonal direct sum of the spaces Vλ where the index ranges over eigenvalues. In other words, if Pλ denotes the orthogonal projection onto Vλ, and λ1, ..., λm are the eigenvalues of A, then the spectral decomposition may be written as A=λ1Pλ1+⋯+λmPλm.=λ_1P_λ_1+⋯+λ_mP_λ_m. If the spectral decomposition of A is A=λ1P1+⋯+λmPm=λ_1P_1+⋯+λ_mP_m, then A2=(λ1)2P1+⋯+(λm)2Pm^2=(λ_1)^2P_1+⋯+(λ_m)^2P_m and μA=μλ1P1+⋯+μλmPm=μλ_1P_1+⋯+μλ_mP_m for any scalar μ.μ. It follows that for any polynomial f one has f(A)=f(λ1)P1+⋯+f(λm)Pm.(A)=f(λ_1)P_1+⋯+f(λ_m)P_m. The spectral decomposition is a special case of both the Schur decomposition and the singular value decomposition. Normal