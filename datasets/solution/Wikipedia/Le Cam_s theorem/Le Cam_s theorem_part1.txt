In probability theory, Le Cam's theorem, named after Lucien Le Cam (1924 – 2000), states the following.[1][2][3] Suppose: X1,X2,X3,…_1,X_2,X_3,… are independent random variables, each with a Bernoulli distribution (i.e., equal to either 0 or 1), not necessarily identically distributed. Pr(Xi=1)=pi,fori=1,2,3,….(X_i=1)=p_i,fori=1,2,3,…. λn=p1+⋯+pn.λ_n=p_1+⋯+p_n. Sn=X1+⋯+Xn._n=X_1+⋯+X_n. (i.e. Sn_n follows a Poisson binomial distribution) Then ∑k=0∞|Pr(Sn=k)−λnke−λnk!|<2(∑i=1npi2).∑_k=0^∞|(S_n=k)-λ_n^ke^-λ_n!|<2(∑_i=1^np_i^2). In other words, the sum has approximately a Poisson distribution and the above inequality bounds the approximation error in terms of the total variation distance. By setting pi = λn/n, we see that this generalizes the usual Poisson limit theorem. When λnλ_n is large a better bound is possible: ∑k=0∞|Pr(Sn=k)−λnke−λnk!|<2(1∧1λn)(∑i=1npi2).∑_k=0^∞|(S_n=k)-λ_n^ke^-λ_n!|<2(1∧1/λ_n)(∑_i=1^np_i^2).,[4] where ∧∧ represents the minmin operator. It is also possible to weaken the independence requirement.[4]