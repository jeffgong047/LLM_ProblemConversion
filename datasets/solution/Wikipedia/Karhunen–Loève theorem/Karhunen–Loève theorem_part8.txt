error: In the introduction, we mentioned that the truncated Karhunen–Loeve expansion was the best approximation of the original process in the sense that it reduces the total mean-square error resulting of its truncation. Because of this property, it is often said that the KL transform optimally compacts the energy. More specifically, given any orthonormal basis {fk} of L2([a, b]), we may decompose the process Xt as: Xt(ω)=∑k=1∞Ak(ω)fk(t)_t(ω)=∑_k=1^∞A_k(ω)f_k(t) where Ak(ω)=∫abXt(ω)fk(t)dt_k(ω)=∫_a^bX_t(ω)f_k(t) dt and we may approximate Xt by the finite sum X^t(ω)=∑k=1NAk(ω)fk(t)X̂_t(ω)=∑_k=1^NA_k(ω)f_k(t) for some integer N. Claim. Of all such approximations, the KL approximation is the one that minimizes the total mean square error (provided we have arranged the eigenvalues in decreasing order). Proof Consider the error resulting from the truncation at the N-th term in the following orthonormal expansion: εN(t)=∑k=N+1∞Ak(ω)fk(t)ε_N(t)=∑_k=N+1^∞A_k(ω)f_k(t) The mean-square error εN2(t) can be written as: εN2(t)=E[∑i=N+1∞∑j=N+1∞Ai(ω)Aj(ω)fi(t)fj(t)]=∑i=N+1∞∑j=N+1∞E[∫ab∫abXtXsfi(t)fj(s)dsdt]fi(t)fj(t)=∑i=N+1∞∑j=N+1∞fi(t)fj(t)∫ab∫abKX(s,t)fi(t)fj(s)dsdtε_N^2(t) =𝐄[∑_i=N+1^∞∑_j=N+1^∞A_i(ω)A_j(ω)f_i(t)f_j(t)] =∑_i=N+1^∞∑_j=N+1^∞𝐄[∫_a^b∫_a^bX_tX_sf_i(t)f_j(s) ds dt]f_i(t)f_j(t) =∑_i=N+1^∞∑_j=N+1^∞f_i(t)f_j(t)∫_a^b∫_a^bK_X(s,t)f_i(t)f_j(s) ds dt We then integrate this last equality over [a, b]. The orthonormality of the fk yields: ∫abεN2(t)dt=∑k=N+1∞∫ab∫abKX(s,t)fk(t)fk(s)dsdt∫_a^bε_N^2(t) dt=∑_k=N+1^∞∫_a^b∫_a^bK_X(s,t)f_k(t)f_k(s) ds dt The problem of minimizing the total mean-square error thus comes down to minimizing the right hand side of this equality subject to the constraint that the fk be normalized. We hence introduce βk, the Lagrangian multipliers associated with these constraints, and aim at minimizing the following function: Er[fk(t),k∈N+1,…]=∑k=N+1∞∫ab∫abKX(s,t)fk(t)fk(s)dsdt−βk(∫abfk(t)fk(t)dt−1)[f_k(t),k∈{N+1,…}]=∑_k=N+1^∞∫_a^b∫_a^bK_X(s,t)f_k(t)f_k(s) ds dt-β_k(∫_a^bf_k(t)f_k(t) dt-1) Differentiating with respect to fi(t) (this is a functional derivative) and setting the derivative to 0 yields: ∂Er∂fi(t)=∫ab(∫abKX(s,t)fi(s)ds−βifi(t))dt=0/_i(t)=∫_a^b(∫_a^bK_X(s,t)f_i(s) ds-β_if_i(t)) dt=0 which is satisfied in particular when ∫abKX(s,t)fi(s)ds=βifi(t).∫_a^bK_X(s,t)f_i(s) ds=β_if_i(t). In other words, when the fk are chosen to be the eigenfunctions of TKX, hence resulting in the KL expansion. Explained