transform: It remains to perform the actual KL transformation, called the principal component transform in this case. Recall that the transform was found by expanding the process with respect to the basis spanned by the eigenvectors of the covariance function. In this case, we hence have: X=∑i=1N⟨φi,X⟩φi=∑i=1NφiTXφi=∑_i=1^N⟨φ_i,X⟩φ_i=∑_i=1^Nφ_i^TXφ_i In a more compact form, the principal component transform of X is defined by: Y=ΦTXX=ΦYY=Φ^TX X= The i-th component of Y is Yi=φiTX_i=φ_i^TX, the projection of X on φiφ_i and the inverse transform X = ΦY yields the expansion of X on the space spanned by the φiφ_i: X=∑i=1NYiφi=∑i=1N⟨φi,X⟩φi=∑_i=1^NY_iφ_i=∑_i=1^N⟨φ_i,X⟩φ_i As in the continuous case, we may reduce the dimensionality of the problem by truncating the sum at some K∈1,…,N∈{1,…,N} such that ∑i=1Kλi∑i=1Nλi≥α∑_i=1^Kλ_i/∑_i=1^Nλ_i≥α where α is the explained variance threshold we wish to set. We can also reduce the dimensionality through the use of multilevel dominant eigenvector estimation (MDEE).[8]