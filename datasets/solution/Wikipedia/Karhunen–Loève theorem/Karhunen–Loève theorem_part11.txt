approximations: Consider a whole class of signals we want to approximate over the first M vectors of a basis. These signals are modeled as realizations of a random vector Y[n] of size N. To optimize the approximation we design a basis that minimizes the average approximation error. This section proves that optimal bases are Karhunenâ€“Loeve bases that diagonalize the covariance matrix of Y. The random vector Y can be decomposed in an orthogonal basis gm0â‰¤mâ‰¤N{g_m}_0 as follows: Y=âˆ‘m=0Nâˆ’1âŸ¨Y,gmâŸ©gm,=âˆ‘_m=0^N-1,g_m_m, where each âŸ¨Y,gmâŸ©=âˆ‘n=0Nâˆ’1Y[n]gmâˆ—[n],g_mâŸ©=âˆ‘_n=0^N-1Y[n]g_m^*[n] is a random variable. The approximation from the first M â‰¤ N vectors of the basis is YM=âˆ‘m=0Mâˆ’1âŸ¨Y,gmâŸ©gm_M=âˆ‘_m=0^M-1,g_m_m The energy conservation in an orthogonal basis implies Îµ[M]=Eâ€–Yâˆ’YMâ€–2=âˆ‘m=MNâˆ’1E|âŸ¨Y,gmâŸ©|2Îµ[M]=ğ„{Y-Y_M^2}=âˆ‘_m=M^N-1ğ„{|,g_mâŸ©|^2} This error is related to the covariance of Y defined by R[n,m]=EY[n]Yâˆ—[m][n,m]=ğ„{Y[n]Y^*[m]} For any vector x[n] we denote by K the covariance operator represented by this matrix, E|âŸ¨Y,xâŸ©|2=âŸ¨Kx,xâŸ©=âˆ‘n=0Nâˆ’1âˆ‘m=0Nâˆ’1R[n,m]x[n]xâˆ—[m]ğ„{|,xâŸ©|^2}=,xâŸ©=âˆ‘_n=0^N-1âˆ‘_m=0^N-1R[n,m]x[n]x^*[m] The error Îµ[M] is therefore a sum of the last N âˆ’ M coefficients of the covariance operator Îµ[M]=âˆ‘m=MNâˆ’1âŸ¨Kgm,gmâŸ©Îµ[M]=âˆ‘_m=M^N-1_m,g_mâŸ© The covariance operator K is Hermitian and Positive and is thus diagonalized in an orthogonal basis called a Karhunenâ€“LoÃ¨ve basis. The following theorem states that a Karhunenâ€“LoÃ¨ve basis is optimal for linear approximations. Theorem (Optimality of Karhunenâ€“LoÃ¨ve basis). Let K be a covariance operator. For all M â‰¥ 1, the approximation error Îµ[M]=âˆ‘m=MNâˆ’1âŸ¨Kgm,gmâŸ©Îµ[M]=âˆ‘_m=M^N-1_m,g_mâŸ© is minimum if and only if gm0â‰¤m<N{g_m}_0<N is a Karhunenâ€“Loeve basis ordered by decreasing eigenvalues. âŸ¨Kgm,gmâŸ©â‰¥âŸ¨Kgm+1,gm+1âŸ©,0â‰¤m<Nâˆ’1._m,g_mâŸ©â‰¥_m+1,g_m+1âŸ©, 0<N-1. Non-Linear approximation in