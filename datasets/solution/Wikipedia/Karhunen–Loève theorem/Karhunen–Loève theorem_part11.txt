approximations: Consider a whole class of signals we want to approximate over the first M vectors of a basis. These signals are modeled as realizations of a random vector Y[n] of size N. To optimize the approximation we design a basis that minimizes the average approximation error. This section proves that optimal bases are Karhunen–Loeve bases that diagonalize the covariance matrix of Y. The random vector Y can be decomposed in an orthogonal basis gm0≤m≤N{g_m}_0 as follows: Y=∑m=0N−1⟨Y,gm⟩gm,=∑_m=0^N-1,g_m_m, where each ⟨Y,gm⟩=∑n=0N−1Y[n]gm∗[n],g_m⟩=∑_n=0^N-1Y[n]g_m^*[n] is a random variable. The approximation from the first M ≤ N vectors of the basis is YM=∑m=0M−1⟨Y,gm⟩gm_M=∑_m=0^M-1,g_m_m The energy conservation in an orthogonal basis implies ε[M]=E‖Y−YM‖2=∑m=MN−1E|⟨Y,gm⟩|2ε[M]=𝐄{Y-Y_M^2}=∑_m=M^N-1𝐄{|,g_m⟩|^2} This error is related to the covariance of Y defined by R[n,m]=EY[n]Y∗[m][n,m]=𝐄{Y[n]Y^*[m]} For any vector x[n] we denote by K the covariance operator represented by this matrix, E|⟨Y,x⟩|2=⟨Kx,x⟩=∑n=0N−1∑m=0N−1R[n,m]x[n]x∗[m]𝐄{|,x⟩|^2}=,x⟩=∑_n=0^N-1∑_m=0^N-1R[n,m]x[n]x^*[m] The error ε[M] is therefore a sum of the last N − M coefficients of the covariance operator ε[M]=∑m=MN−1⟨Kgm,gm⟩ε[M]=∑_m=M^N-1_m,g_m⟩ The covariance operator K is Hermitian and Positive and is thus diagonalized in an orthogonal basis called a Karhunen–Loève basis. The following theorem states that a Karhunen–Loève basis is optimal for linear approximations. Theorem (Optimality of Karhunen–Loève basis). Let K be a covariance operator. For all M ≥ 1, the approximation error ε[M]=∑m=MN−1⟨Kgm,gm⟩ε[M]=∑_m=M^N-1_m,g_m⟩ is minimum if and only if gm0≤m<N{g_m}_0<N is a Karhunen–Loeve basis ordered by decreasing eigenvalues. ⟨Kgm,gm⟩≥⟨Kgm+1,gm+1⟩,0≤m<N−1._m,g_m⟩≥_m+1,g_m+1⟩, 0<N-1. Non-Linear approximation in