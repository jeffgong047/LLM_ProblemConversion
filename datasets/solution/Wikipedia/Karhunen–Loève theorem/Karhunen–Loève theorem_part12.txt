bases: Linear approximations project the signal on M vectors a priori. The approximation can be made more precise by choosing the M orthogonal vectors depending on the signal properties. This section analyzes the general performance of these non-linear approximations. A signal f∈H∈H is approximated with M vectors selected adaptively in an orthonormal basis for HH[definition needed] B=gmm∈NB={g_m}_m∈ℕ Let fM_M be the projection of f over M vectors whose indices are in IM: fM=∑m∈IM⟨f,gm⟩gm_M=∑_m_M,g_m_m The approximation error is the sum of the remaining coefficients ε[M]=‖f−fM‖2=∑m∉IMN−1|⟨f,gm⟩|2ε[M]={f-f_M^2}=∑_m_M^N-1{|,g_m⟩|^2} To minimize this error, the indices in IM must correspond to the M vectors having the largest inner product amplitude |⟨f,gm⟩|.|,g_m⟩|. These are the vectors that best correlate f. They can thus be interpreted as the main features of f. The resulting error is necessarily smaller than the error of a linear approximation which selects the M approximation vectors independently of f. Let us sort |⟨f,gm⟩|m∈N{|,g_m⟩|}_m∈ℕ in decreasing order |⟨f,gmk⟩|≥|⟨f,gmk+1⟩|.|,g_m_k⟩|≥|,g_m_k+1⟩|. The best non-linear approximation is fM=∑k=1M⟨f,gmk⟩gmk_M=∑_k=1^M,g_m_k_m_k It can also be written as inner product thresholding: fM=∑m=0∞θT(⟨f,gm⟩)gm_M=∑_m=0^∞θ_T(,g_m⟩)g_m with T=|⟨f,gmM⟩|,θT(x)=x|x|≥T0|x|<T=|,g_m_M⟩|, θ_T(x)=x |x| 0 |x|<T The non-linear error is ε[M]=‖f−fM‖2=∑k=M+1∞|⟨f,gmk⟩|2ε[M]={f-f_M^2}=∑_k=M+1^∞{|,g_m_k⟩|^2} this error goes quickly to zero as M increases, if the sorted values of |⟨f,gmk⟩||,g_m_k⟩| have a fast decay as k increases. This decay is quantified by computing the IPI^P norm of the signal inner products in B: ‖f‖B,p=(∑m=0∞|⟨f,gm⟩|p)1pf_B,p=(∑_m=0^∞|,g_m⟩|^p)^1/p The following theorem relates the decay of ε[M] to ‖f‖B,pf_B,p Theorem (decay of error). If ‖f‖B,p<∞f_B,p<∞ with p < 2 then ε[M]≤‖f‖B,p22p−1M1−2pε[M]≤f_B,p^2/2/p-1M^1-2/p and ε[M]=o(M1−2p).ε[M]=o(M^1-2/p). Conversely, if ε[M]=o(M1−2p)ε[M]=o(M^1-2/p) then ‖f‖B,q<∞f_B,q<∞ for any q > p. Non-optimality of Karhunen–Loève