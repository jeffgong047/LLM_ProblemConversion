Statement: Let Xn,Yn_n,Y_n be sequences of scalar/vector/matrix random elements. If Xn_n converges in distribution to a random element X and Yn_n converges in probability to a constant c, then Xn+Yn→dX+c;_n+Y_n{+c; XnYn→dXc;_nY_n xrightarrowd; Xn/Yn→dX/c,_n/Y_n{/c, provided that c is invertible, where →d denotes convergence in distribution. Notes: The requirement that Yn converges to a constant is important — if it were to converge to a non-degenerate random variable, the theorem would be no longer valid. For example, let Xn∼Uniform(0,1)_n∼Uniform(0,1) and Yn=−Xn_n=-X_n. The sum Xn+Yn=0_n+Y_n=0 for all values of n. Moreover, Yn→dUniform(−1,0)_n Uniform(-1,0), but Xn+Yn_n+Y_n does not converge in distribution to X+Y+Y, where X∼Uniform(0,1)∼Uniform(0,1), Y∼Uniform(−1,0)∼Uniform(-1,0), and X and Y are independent.[4] The theorem remains valid if we replace all convergences in distribution with convergences in probability.