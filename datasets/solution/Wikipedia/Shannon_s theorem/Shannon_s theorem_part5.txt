channels: This particular proof of achievability follows the style of proofs that make use of the asymptotic equipartition property (AEP). Another style can be found in information theory texts using error exponents. Both types of proofs make use of a random coding argument where the codebook used across a channel is randomly constructed - this serves to make the analysis simpler while still proving the existence of a code satisfying a desired low probability of error at any data rate below the channel capacity. By an AEP-related argument, given a channel, length n strings of source symbols X1n_1^n, and length n strings of channel outputs Y1n_1^n, we can define a jointly typical set by the following: AŒµ(n)=(xn,yn)‚ààXn√óYn_Œµ^(n)={(x^n,y^n)‚ààùí≥^n√óùí¥^n 2‚àín(H(X)+Œµ)‚â§p(X1n)‚â§2‚àín(H(X)‚àíŒµ)2^-n(H(X)+Œµ)(X_1^n)‚â§2^-n(H(X)-Œµ) 2‚àín(H(Y)+Œµ)‚â§p(Y1n)‚â§2‚àín(H(Y)‚àíŒµ)2^-n(H(Y)+Œµ)(Y_1^n)‚â§2^-n(H(Y)-Œµ) 2‚àín(H(X,Y)+Œµ)‚â§p(X1n,Y1n)‚â§2‚àín(H(X,Y)‚àíŒµ)2^-n(H(X,Y)+Œµ)(X_1^n,Y_1^n)‚â§2^-n(H(X,Y)-Œµ)} We say that two sequences X1nX_1^n and Y1n_1^n are jointly typical if they lie in the jointly typical set defined above. Steps In the style of the random coding argument, we randomly generate 2nR2^nR codewords of length n from a probability distribution Q. This code is revealed to the sender and receiver. It is also assumed that one knows the transition matrix p(y|x)(y|x) for the channel being used. A message W is chosen according to the uniform distribution on the set of codewords. That is, Pr(W=w)=2‚àínR,w=1,2,‚Ä¶,2nR(W=w)=2^-nR,w=1,2,‚Ä¶,2^nR. The message W is sent across the channel. The receiver receives a sequence according to P(yn|xn(w))=‚àèi=1np(yi|xi(w))(y^n|x^n(w))=‚àè_i=1^np(y_i|x_i(w)) Sending these codewords across the channel, we receive Y1n_1^n, and decode to some source sequence if there exists exactly 1 codeword that is jointly typical with Y. If there are no jointly typical codewords, or if there are more than one, an error is declared. An error also occurs if a decoded codeword doesn't match the original codeword. This is called typical set decoding. The probability of error of this scheme is divided into two parts: First, error can occur if no jointly typical X sequences are found for a received Y sequence Second, error can occur if an incorrect X sequence is jointly typical with a received Y sequence. By the randomness of the code construction, we can assume that the average probability of error averaged over all codes does not depend on the index sent. Thus, without loss of generality, we can assume W = 1. From the joint AEP, we know that the probability that no jointly typical X exists goes to 0 as n grows large. We can bound this error probability by ŒµŒµ. Also from the joint AEP, we know the probability that a particular X1n(i)_1^n(i) and the Y1n_1^n resulting from W = 1 are jointly typical is ‚â§2‚àín(I(X;Y)‚àí3Œµ)‚â§2^-n(I(X;Y)-3Œµ). Define: Ei=(X1n(i),Y1n)‚ààAŒµ(n),i=1,2,‚Ä¶,2nR_i={(X_1^n(i),Y_1^n)_Œµ^(n)},i=1,2,‚Ä¶,2^nR as the event that message i is jointly typical with the sequence received when message 1 is sent. P(error)=P(error|W=1)‚â§P(E1c)+‚àëi=22nRP(Ei)‚â§P(E1c)+(2nR‚àí1)2‚àín(I(X;Y)‚àí3Œµ)‚â§Œµ+2‚àín(I(X;Y)‚àíR‚àí3Œµ).P(error) =P(error|W=1)(E_1^c)+‚àë_i=2^2^nRP(E_i) (E_1^c)+(2^nR-1)2^-n(I(X;Y)-3Œµ) ‚â§Œµ+2^-n(I(X;Y)-R-3Œµ). We can observe that as n goes to infinity, if R<I(X;Y)<I(X;Y) for the channel, the probability of error will go to 0. Finally, given that the average codebook is shown to be "good," we know that there exists a codebook whose performance is better than the average, and so satisfies our need for arbitrarily low error probability communicating across the noisy channel. Weak converse for discrete memoryless