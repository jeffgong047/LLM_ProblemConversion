Theorem in statistics
In statistics, Basu's theorem states that any boundedly complete minimal sufficient statistic is independent of any ancillary statistic. This is a 1955 result of Debabrata Basu.[1]
It is often used in statistics as a tool to prove independence of two statistics, by first demonstrating one is complete sufficient and the other is ancillary, then appealing to the theorem.[2] An example of this is to show that the sample mean and sample variance of a normal distribution are independent statistics, which is done in the Example section below. This property (independence of sample mean and sample variance) characterizes normal distributions.


Statement[edit]
Let (PÎ¸;Î¸âˆˆÎ˜)(P_Î¸;Î¸âˆˆÎ˜) be a family of distributions on a measurable space (X,A)(X,ğ’œ) and a statistic T  maps  from (X,A)(X,ğ’œ) to some measurable space (Y,B)(Y,â„¬). If T is a boundedly complete sufficient statistic for Î¸Î¸, and A is ancillary to Î¸Î¸, then conditional on Î¸Î¸, T is independent of A. That is, TâŠ¥âŠ¥A|Î¸âŠ¥|Î¸.

Proof[edit]
Let PÎ¸T_Î¸^T and PÎ¸A_Î¸^A be the marginal distributions of T and A respectively.
Denote by Aâˆ’1(B)^-1(B) the preimage of a set B under the map A. For any measurable set BâˆˆBâˆˆâ„¬ we have

PÎ¸A(B)=PÎ¸(Aâˆ’1(B))=âˆ«YPÎ¸(Aâˆ’1(B)âˆ£T=t)PÎ¸T(dt)._Î¸^A(B)=P_Î¸(A^-1(B))=âˆ«_YP_Î¸(A^-1(B)=t)_Î¸^T(dt).
The distribution PÎ¸A_Î¸^A does not depend on Î¸Î¸  because A  is ancillary. Likewise, PÎ¸(â‹…âˆ£T=t)_Î¸(Â·=t) does not depend on Î¸Î¸ because T is sufficient. Therefore

âˆ«Y[P(Aâˆ’1(B)âˆ£T=t)âˆ’PA(B)]PÎ¸T(dt)=0.âˆ«_Y[P(A^-1(B)=t)-P^A(B)]_Î¸^T(dt)=0.
Note the integrand (the function inside the integral) is a function of t and not Î¸Î¸. Therefore, since T is boundedly complete the function

g(t)=P(Aâˆ’1(B)âˆ£T=t)âˆ’PA(B)(t)=P(A^-1(B)=t)-P^A(B)
is zero for PÎ¸T_Î¸^T almost all values of t and thus

P(Aâˆ’1(B)âˆ£T=t)=PA(B)(A^-1(B)=t)=P^A(B)
for almost all t. Therefore, A is independent of T.

Example[edit]
Independence of sample mean and sample variance of a normal distribution[edit]
Let X1, X2, ..., Xn be independent, identically distributed normal random variables with mean Î¼ and variance Ïƒ2.
Then with respect to the parameter Î¼, one can show that

Î¼^=âˆ‘Xin,Î¼=_i/n,
the sample mean, is a complete and sufficient statistic â€“ it is all the information one can derive to estimate Î¼, and no more â€“ and

Ïƒ^2=âˆ‘(Xiâˆ’XÂ¯)2nâˆ’1,Ïƒ^2=âˆ‘(X_i-XÌ…)^2/n-1,
the sample variance, is an ancillary statistic â€“ its distribution does not depend on Î¼.
Therefore, from Basu's theorem it follows that these statistics are independent conditional on Î¼Î¼, conditional on Ïƒ2Ïƒ^2.
This independence result can also be proven by Cochran's theorem.
Further, this property (that the sample mean and sample variance of the normal distribution are independent) characterizes the normal distribution â€“ no other distribution has this property.[3]

Notes[edit]


^ Basu (1955)

^ Ghosh, Malay; Mukhopadhyay, Nitis; Sen, Pranab Kumar (2011), Sequential Estimation, Wiley Series in Probability and Statistics, vol.Â 904, John Wiley & Sons, p.Â 80, ISBNÂ 9781118165911, The following theorem, due to Basu ... helps us in proving independence between certain types of statistics, without actually deriving the joint and marginal distributions of the statistics involved. This is a very powerful tool and it is often used ...

^ Geary, R.C. (1936). "The Distribution of "Student's" Ratio for Non-Normal Samples". Supplement to the Journal of the Royal Statistical Society. 3 (2): 178â€“184. doi:10.2307/2983669. JFMÂ 63.1090.03. JSTORÂ 2983669.


This article includes a list of general references, but it lacks sufficient corresponding inline citations. Please help to improve this article by introducing more precise citations. (December 2009) (Learn how and when to remove this template message)
References[edit]
Basu, D. (1955). "On Statistics Independent of a Complete Sufficient Statistic". SankhyÄ. 15 (4): 377â€“380. JSTORÂ 25048259. MRÂ 0074745. ZblÂ 0068.13401.
Mukhopadhyay, Nitis (2000). Probability and Statistical Inference. Statistics: A Series of Textbooks and Monographs. 162. Florida: CRC Press USA. ISBNÂ 0-8247-0379-0.
Boos, Dennis D.; Oliver, Jacqueline M. Hughes (Aug 1998). "Applications of Basu's Theorem". The American Statistician. 52 (3): 218â€“221. doi:10.2307/2685927. JSTORÂ 2685927. MRÂ 1650407.
Ghosh, Malay (October 2002). "Basu's Theorem with Applications: A Personalistic Review". SankhyÄ: The Indian Journal of Statistics, Series A. 64 (3): 509â€“531. JSTORÂ 25051412. MRÂ 1985397.
vteStatistics
Outline
Index
Descriptive statisticsContinuous dataCenter
Mean
Arithmetic
Arithmetic-Geometric
Cubic
Generalized/power
Geometric
Harmonic
Heronian
Heinz
Lehmer
Median
Mode
Dispersion
Average absolute deviation
Coefficient of variation
Interquartile range
Percentile
Range
Standard deviation
Variance
Shape
Central limit theorem
Moments
Kurtosis
L-moments
Skewness
Count data
Index of dispersion
Summary tables
Contingency table
Frequency distribution
Grouped data
Dependence
Partial correlation
Pearson product-moment correlation
Rank correlation
Kendall's Ï„
Spearman's Ï
Scatter plot
Graphics
Bar chart
Biplot
Box plot
Control chart
Correlogram
Fan chart
Forest plot
Histogram
Pie chart
Qâ€“Q plot
Radar chart
Run chart
Scatter plot
Stem-and-leaf display
Violin plot
Data collectionStudy design
Effect size
Missing data
Optimal design
Population
Replication
Sample size determination
Statistic
Statistical power
Survey methodology
Sampling
Cluster
Stratified
Opinion poll
Questionnaire
Standard error
Controlled experiments
Blocking
Factorial experiment
Interaction
Random assignment
Randomized controlled trial
Randomized experiment
Scientific control
Adaptive designs
Adaptive clinical trial
Stochastic approximation
Up-and-down designs
Observational studies
Cohort study
Cross-sectional study
Natural experiment
Quasi-experiment
Statistical inferenceStatistical theory
Population
Statistic
Probability distribution
Sampling distribution
Order statistic
Empirical distribution
Density estimation
Statistical model
Model specification
Lp space
Parameter
location
scale
shape
Parametric family
LikelihoodÂ (monotone)
Locationâ€“scale family
Exponential family
Completeness
Sufficiency
Statistical functional
Bootstrap
U
V
Optimal decision
loss function
Efficiency
Statistical distance
divergence
Asymptotics
Robustness
Frequentist inferencePoint estimation
Estimating equations
Maximum likelihood
Method of moments
M-estimator
Minimum distance
Unbiased estimators
Mean-unbiased minimum-variance
Raoâ€“Blackwellization
Lehmannâ€“ScheffÃ© theorem
Median unbiased
Plug-in
Interval estimation
Confidence interval
Pivot
Likelihood interval
Prediction interval
Tolerance interval
Resampling
Bootstrap
Jackknife
Testing hypotheses
1- & 2-tails
Power
Uniformly most powerful test
Permutation test
Randomization test
Multiple comparisons
Parametric tests
Likelihood-ratio
Score/Lagrange multiplier
Wald
Specific tests
Z-test (normal)
Student's t-test
F-test
Goodness of fit
Chi-squared
G-test
Kolmogorovâ€“Smirnov
Andersonâ€“Darling
Lilliefors
Jarqueâ€“Bera
Normality (Shapiroâ€“Wilk)
Likelihood-ratio test
Model selection
Cross validation
AIC
BIC
Rank statistics
Sign
Sample median
Signed rank (Wilcoxon)
Hodgesâ€“Lehmann estimator
Rank sum (Mannâ€“Whitney)
Nonparametric anova
1-way (Kruskalâ€“Wallis)
2-way (Friedman)
Ordered alternative (Jonckheereâ€“Terpstra)
Van der Waerden test
Bayesian inference
Bayesian probability
prior
posterior
Credible interval
Bayes factor
Bayesian estimator
Maximum posterior estimator
CorrelationRegression analysisCorrelation
Pearson product-moment
Partial correlation
Confounding variable
Coefficient of determination
Regression analysis
Errors and residuals
Regression validation
Mixed effects models
Simultaneous equations models
Multivariate adaptive regression splines (MARS)
Linear regression
Simple linear regression
Ordinary least squares
General linear model
Bayesian regression
Non-standard predictors
Nonlinear regression
Nonparametric
Semiparametric
Isotonic
Robust
Heteroscedasticity
Homoscedasticity
Generalized linear model
Exponential families
Logistic (Bernoulli)Â / BinomialÂ / Poisson regressions
Partition of variance
Analysis of variance (ANOVA, anova)
Analysis of covariance
Multivariate ANOVA
Degrees of freedom
CategoricalÂ / MultivariateÂ / Time-seriesÂ / Survival analysisCategorical
Cohen's kappa
Contingency table
Graphical model
Log-linear model
McNemar's test
Cochranâ€“Mantelâ€“Haenszel statistics
Multivariate
Regression
Manova
Principal components
Canonical correlation
Discriminant analysis
Cluster analysis
Classification
Structural equation model
Factor analysis
Multivariate distributions
Elliptical distributions
Normal
Time-seriesGeneral
Decomposition
Trend
Stationarity
Seasonal adjustment
Exponential smoothing
Cointegration
Structural break
Granger causality
Specific tests
Dickeyâ€“Fuller
Johansen
Q-statistic (Ljungâ€“Box)
Durbinâ€“Watson
Breuschâ€“Godfrey
Time domain
Autocorrelation (ACF)
partial (PACF)
Cross-correlation (XCF)
ARMA model
ARIMA model (Boxâ€“Jenkins)
Autoregressive conditional heteroskedasticity (ARCH)
Vector autoregression (VAR)
Frequency domain
Spectral density estimation
Fourier analysis
Least-squares spectral analysis
Wavelet
Whittle likelihood
SurvivalSurvival function
Kaplanâ€“Meier estimator (product limit)
Proportional hazards models
Accelerated failure time (AFT) model
First hitting time
Hazard function
Nelsonâ€“Aalen estimator
Test
Log-rank test
ApplicationsBiostatistics
Bioinformatics
Clinical trialsÂ / studies
Epidemiology
Medical statistics
Engineering statistics
Chemometrics
Methods engineering
Probabilistic design
ProcessÂ / quality control
Reliability
System identification
Social statistics
Actuarial science
Census
Crime statistics
Demography
Econometrics
Jurimetrics
National accounts
Official statistics
Population statistics
Psychometrics
Spatial statistics
Cartography
Environmental statistics
Geographic information system
Geostatistics
Kriging

Category
Â Mathematics portal
Commons
 WikiProject




