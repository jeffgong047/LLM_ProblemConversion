Theorem in statistics and econometrics
In econometrics, the Frisch‚ÄìWaugh‚ÄìLovell (FWL) theorem is named after the econometricians Ragnar Frisch, Frederick V. Waugh, and Michael C. Lovell.[1][2][3]
The Frisch‚ÄìWaugh‚ÄìLovell theorem states that if the regression we are concerned with is expressed in terms of two separate sets of predictor variables:

Y=X1Œ≤1+X2Œ≤2+u=X_1Œ≤_1+X_2Œ≤_2+u
where X1_1 and X2_2 are matrices, Œ≤1Œ≤_1 and Œ≤2Œ≤_2 are vectors (and u is the error term), then the estimate of Œ≤2Œ≤_2 will be the same as the estimate of it from a modified regression of the form:

MX1Y=MX1X2Œ≤2+MX1u,_X_1Y=M_X_1X_2Œ≤_2+M_X_1u,
where MX1_X_1 projects onto the orthogonal complement of the image of the projection matrix X1(X1TX1)‚àí1X1T_1(X_1^ùñ≥X_1)^-1X_1^ùñ≥.  Equivalently, MX1 projects onto the orthogonal complement of the column space of¬†X1.  Specifically,

MX1=I‚àíX1(X1TX1)‚àí1X1T,_X_1=I-X_1(X_1^ùñ≥X_1)^-1X_1^ùñ≥,
and this particular orthogonal projection matrix is known as the residual maker matrix or annihilator matrix.[4][5]
The vector MX1Y_X_1Y is the vector of residuals from regression of Y on the columns of X1_1.
The most relevant consequence of the theorem is that the parameters in Œ≤2Œ≤_2 do not apply to X2_2 but to MX1X2_X_1X_2, that is: the part of X2_2 uncorrelated with X1_1. This is the basis for understanding the contribution of each single variable to a multivariate regression (see, for instance, Ch. 13 in [6]).
The theorem also implies that the secondary regression used for obtaining MX1_X_1 is unnecessary when the predictor variables are uncorrelated: using projection matrices to make the explanatory variables orthogonal to each other will lead to the same results as running the regression with all non-orthogonal explanators included.
Moreover, the standard errors from the partial regression equal those from the full regression.[7]

History[edit]
The origin of the theorem is uncertain, but it was well-established in the realm of linear regression before the Frisch and Waugh paper. George Udny Yule's comprehensive analysis of partial regressions, published in 1907, included the theorem in section 9 on page 184.[8] Yule emphasized the theorem's importance for understanding multiple and partial regression and correlation coefficients, as mentioned in section 10 of the same paper.[8]
By 1933, Yule's findings were generally recognized[weasel¬†words], thanks in part to the detailed discussion of partial correlation and the introduction of his innovative notation in 1907.[citation needed] The theorem, later associated with Frisch, Waugh, and Lovell, was also included in chapter 10 of Yule's successful statistics textbook, first published in 1911. The book reached its tenth edition by 1932.[9]
In a 1931 paper co-authored with Mudgett, Frisch cited Yule's results.[10] Yule's formulas for partial regressions were quoted and explicitly attributed to him in order to rectify a misquotation by another author.[10] Although Yule was not explicitly mentioned in the 1933 paper by Frisch and Waugh, they utilized the notation for partial regression coefficients initially introduced by Yule in 1907, which was widely accepted by 1933[original research?].
In 1963, Lovell published a proof[11] considered more straightforward and intuitive. In recognition, people generally add his name to the theorem name.

References[edit]


^ Frisch, Ragnar; Waugh, Frederick V. (1933). "Partial Time Regressions as Compared with Individual Trends". Econometrica. 1 (4): 387‚Äì401. doi:10.2307/1907330. JSTOR¬†1907330.

^ Lovell, M. (1963). "Seasonal Adjustment of Economic Time Series and Multiple Regression Analysis". Journal of the American Statistical Association. 58 (304): 993‚Äì1010. doi:10.1080/01621459.1963.10480682.

^ Lovell, M. (2008). "A Simple Proof of the FWL Theorem". Journal of Economic Education. 39 (1): 88‚Äì91. doi:10.3200/JECE.39.1.88-91. S2CID¬†154907484.

^ Hayashi, Fumio (2000). Econometrics. Princeton: Princeton University Press. pp.¬†18‚Äì19. ISBN¬†0-691-01018-8.

^ Davidson, James (2000). Econometric Theory. Malden: Blackwell. p.¬†7. ISBN¬†0-631-21584-0.

^ Mosteller, F.; Tukey, J. W. (1977). Data Analysis and Regression a Second Course in Statistics. Addison-Wesley.

^ Peng, Ding (2021). "The Frisch--Waugh--Lovell theorem for standard errors". Statistics and Probability Letters. 168: 108945.

^ a b Yule, George Udny (1907). "On the Theory of Correlation for any Number of Variables, Treated by a New System of Notation". Proceedings of the Royal Society A. 79 (529): 182‚Äì193. doi:10.1098/rspa.1907.0028. hdl:2027/coo.31924081088423.

^ Yule, George Udny (1932). An Introduction to the Theory of Statistics 10th edition. London: Charles Griffin &Co.

^ a b Frisch, Ragnar; Mudgett, B. D. (1931). "Statistical Correlation and the Theory of Cluster Types" (PDF). Journal of the American Statistical Association. 21 (176): 375‚Äì392. doi:10.1080/01621459.1931.10502225.

^ Lovell, M. (1963). "Seasonal Adjustment of Economic Time Series and Multiple Regression Analysis". Journal of the American Statistical Association. 58 (304): 993‚Äì1010. doi:10.1080/01621459.1963.10480682.


Further reading[edit]
Davidson, Russell; MacKinnon, James G. (1993). Estimation and Inference in Econometrics. New York: Oxford University Press. pp.¬†19‚Äì24. ISBN¬†0-19-506011-3.
Davidson, Russell; MacKinnon, James G. (2004). Econometric Theory and Methods. New York: Oxford University Press. pp.¬†62‚Äì75. ISBN¬†0-19-512372-7.
Hastie, Trevor; Tibshirani, Robert; Friedman, Jerome (2017). "Multiple Regression from Simple Univariate Regression" (PDF). The Elements of Statistical Learning¬†: Data Mining, Inference, and Prediction (2nd¬†ed.). New York: Springer. pp.¬†52‚Äì55. ISBN¬†978-0-387-84857-0.
Ruud, P. A. (2000). An Introduction to Classical Econometric Theory. New York: Oxford University Press. pp.¬†54‚Äì60. ISBN¬†0-19-511164-8.
Stachurski, John (2016). A Primer in Econometric Theory. MIT Press. pp.¬†311‚Äì314.
vteLeast squares and regression analysisComputational statistics
Least squares
Linear least squares
Non-linear least squares
Iteratively reweighted least squares
Correlation and dependence
Pearson product-moment correlation
Rank correlation (Spearman's rho
Kendall's tau)
Partial correlation
Confounding variable
Regression analysis
Ordinary least squares
Partial least squares
Total least squares
Ridge regression
Regression as a statistical modelLinear regression
Simple linear regression
Ordinary least squares
Generalized least squares
Weighted least squares
General linear model
Predictor structure
Polynomial regression
Growth curve (statistics)
Segmented regression
Local regression
Non-standard
Nonlinear regression
Nonparametric
Semiparametric
Robust
Quantile
Isotonic
Non-normal errors
Generalized linear model
Binomial
Poisson
Logistic
Decomposition of variance
Analysis of variance
Analysis of covariance
Multivariate AOV
Model exploration
Stepwise regression
Model selection
Mallows's Cp
AIC
BIC
Model specification
Regression validation
Background
Mean and predicted response
Gauss‚ÄìMarkov theorem
Errors and residuals
Goodness of fit
Studentized residual
Minimum mean-square error
Frisch‚ÄìWaugh‚ÄìLovell theorem
Design of experiments
Response surface methodology
Optimal design
Bayesian design
Numerical approximation
Numerical analysis
Approximation theory
Numerical integration
Gaussian quadrature
Orthogonal polynomials
Chebyshev polynomials
Chebyshev nodes
Applications
Curve fitting
Calibration curve
Numerical smoothing and differentiation
System identification
Moving least squares

Regression analysis category
Statistics category
¬†Mathematics portal
Statistics outline
Statistics topics




