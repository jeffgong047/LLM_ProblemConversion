"Rank theorem" redirects here. For the rank theorem of multivariable calculus, see constant rank theorem.
In linear algebra, relation between 3 dimensions
Rankâ€“nullity theorem
The rankâ€“nullity theorem is a theorem in linear algebra, which asserts:

the number of columns of a matrix M is the sum of the rank of M and the nullity of M; and
the dimension of the domain of a linear transformation f is the sum of the rank of f (the dimension of the image of f) and the nullity of f (the dimension of the kernel of f).[1][2][3][4]
It follows that for linear transformations of vector spaces of finite dimension, either injectivity or surjectivity implies bijectivity[further explanation needed].


Stating the theorem[edit]
Linear transformations[edit]
Let T:Vâ†’W:V be a linear transformation between two vector spaces where T's domain V is finite dimensional. Then
rankâ¡(T)+nullityâ¡(T)=dimâ¡V,rank(T)Â +Â nullity(T)Â =Â ,
where rankâ¡(T)rank(T) is the rank of T (the dimension of its image) and nullityâ¡(T)nullity(T) is the nullity of T (the dimension of its kernel).  In other words,
dimâ¡(Imâ¡T)+dimâ¡(Kerâ¡T)=dimâ¡(Domainâ¡(T)).(ImT)+(KerT)=(Domain(T)).
This theorem can be refined via the splitting lemma to be a statement about an isomorphism of spaces, not just dimensions. Explicitly, since T induces an isomorphism from V/Kerâ¡(T)/Ker(T) to Imageâ¡(T),Image(T), the existence of a basis for V that extends any given basis of Kerâ¡(T)Ker(T) implies, via the splitting lemma, that Imageâ¡(T)âŠ•Kerâ¡(T)â‰…V.Image(T)âŠ•Ker(T). Taking dimensions, the rankâ€“nullity theorem follows.

Matrices[edit]
Linear maps can be represented with matrices. More precisely, an mÃ—n matrix M represents a linear map f:Fnâ†’Fm,:F^n^m, where F is the underlying field.[5] So, the dimension of the domain of f is n, the number of columns of M, and the rankâ€“nullity theorem for an mÃ—n matrix M is
rankâ¡(M)+nullityâ¡(M)=n.rank(M)+nullity(M)=n.

Proofs[edit]
Here we provide two proofs. The first[2] operates in the general case, using linear maps. The second proof[6] looks at the homogeneous system Ax=0,ğ€ğ±=0, where Ağ€ is a mÃ—n with rank r,, and shows explicitly that there exists a set of nâˆ’r-r linearly independent solutions that span the null space of Ağ€.
While the theorem requires that the domain of the linear map be finite-dimensional, there is no such assumption on the codomain. This means that there are linear maps not given by matrices for which the theorem applies. Despite this, the first proof is not actually more general than the second: since the image of the linear map is finite-dimensional, we can represent the map from its domain to its image by a matrix, prove the theorem for that matrix, then compose with the inclusion of the image into the full codomain.

First proof[edit]
Let V,W,W be vector spaces over some field F,, and T defined as in the statement of the theorem with dimâ¡V=n=n.
As Kerâ¡TâŠ‚VKerT is a subspace, there exists a basis for it. Suppose dimâ¡Kerâ¡T=kKerT=k and let
K:=v1,â€¦,vkâŠ‚Kerâ¡(T)ğ’¦:={v_1,â€¦,v_k}âŠ‚Ker(T)
be such a basis.
We may now, by the Steinitz exchange lemma, extend Kğ’¦ with nâˆ’k-k linearly independent vectors w1,â€¦,wnâˆ’k_1,â€¦,w_n-k to form a full basis of V.
Let
S:=w1,â€¦,wnâˆ’kâŠ‚Vâˆ–Kerâ¡(T)ğ’®:={w_1,â€¦,w_n-k}âˆ–Ker(T)
such that
B:=KâˆªS=v1,â€¦,vk,w1,â€¦,wnâˆ’kâŠ‚Vâ„¬:=ğ’¦âˆªğ’®={v_1,â€¦,v_k,w_1,â€¦,w_n-k}
is a basis for V.
From this, we know that
Imâ¡T=Spanâ¡T(B)=Spanâ¡T(v1),â€¦,T(vk),T(w1),â€¦,T(wnâˆ’k)ImT=SpanT(â„¬)=Span{T(v_1),â€¦,T(v_k),T(w_1),â€¦,T(w_n-k)}

=Spanâ¡T(w1),â€¦,T(wnâˆ’k)=Spanâ¡T(S).=Span{T(w_1),â€¦,T(w_n-k)}=SpanT(ğ’®).
We now claim that T(S)(ğ’®) is a basis for Imâ¡TImT.
The above equality already states that T(S)(ğ’®) is a generating set for Imâ¡TImT; it remains to be shown that it is also linearly independent to conclude that it is a basis.
Suppose T(S)(ğ’®) is not linearly independent, and let
âˆ‘j=1nâˆ’kÎ±jT(wj)=0Wâˆ‘_j=1^n-kÎ±_jT(w_j)=0_W
for some Î±jâˆˆFÎ±_j.
Thus, owing to the linearity of T, it follows that
T(âˆ‘j=1nâˆ’kÎ±jwj)=0WâŸ¹(âˆ‘j=1nâˆ’kÎ±jwj)âˆˆKerâ¡T=Spanâ¡KâŠ‚V.(âˆ‘_j=1^n-kÎ±_jw_j)=0_W(âˆ‘_j=1^n-kÎ±_jw_j)âˆˆKerT=Spanğ’¦.
This is a contradiction to Bâ„¬ being a basis, unless all Î±jÎ±_j are equal to zero. This shows that T(S)(ğ’®) is linearly independent, and more specifically that it is a basis for Imâ¡TImT.
To summarize, we have Kğ’¦, a basis for Kerâ¡TKerT, and T(S)(ğ’®), a basis for Imâ¡TImT.
Finally we may state that
Rankâ¡(T)+Nullityâ¡(T)=dimâ¡Imâ¡T+dimâ¡Kerâ¡TRank(T)+Nullity(T)=ImT+KerT

=|T(S)|+|K|=(nâˆ’k)+k=n=dimâ¡V.=|T(ğ’®)|+|ğ’¦|=(n-k)+k=n=.
This concludes our proof.

Second proof[edit]
Let Ağ€ be an mÃ—n matrix with r linearly independent columns (i.e. Rankâ¡(A)=rRank(ğ€)=r). We will show that:

There exists a set of nâˆ’r-r linearly independent solutions to the homogeneous system Ax=0ğ€ğ±=0.That every other solution is a linear combination of these nâˆ’r-r solutions.
To do this, we will produce an nÃ—(nâˆ’r)Ã—(n-r) matrix Xğ— whose columns form a basis of the null space of Ağ€.
Without loss of generality, assume that the first r columns of Ağ€ are linearly independent. So, we can write
A=(A1A2),ğ€=[ ğ€_1 ğ€_2 ],
where

A1ğ€_1 is an mÃ—r matrix with r linearly independent column vectors, and
A2ğ€_2 is an mÃ—(nâˆ’r)Ã—(n-r) matrix such that each of its nâˆ’r-r columns is linear combinations of the columns of A1ğ€_1.
This means that A2=A1Bğ€_2=ğ€_1ğ for some rÃ—(nâˆ’r)Ã—(n-r) matrix Bğ (see rank factorization) and, hence,
A=(A1A1B).ğ€=[  ğ€_1 ğ€_1ğ ].
Let
X=(âˆ’BInâˆ’r),ğ—=[    -ğ; ğˆ_n-r ],
where Inâˆ’rğˆ_n-r is the (nâˆ’r)Ã—(nâˆ’r)(n-r)Ã—(n-r) identity matrix. So, Xğ— is an nÃ—(nâˆ’r)Ã—(n-r) matrix such that
AX=(A1A1B)(âˆ’BInâˆ’r)=âˆ’A1B+A1B=0mÃ—(nâˆ’r).ğ€ğ—=[  ğ€_1 ğ€_1ğ ][    -ğ; ğˆ_n-r ]=-ğ€_1ğ+ğ€_1ğ=0_mÃ—(n-r).
Therefore, each of the nâˆ’r-r columns of Xğ— are particular solutions of Ax=0Fmğ€ğ±=0_F^m.
Furthermore, the nâˆ’r-r columns of Xğ— are linearly independent because Xu=0Fnğ—ğ®=0_F^n will imply u=0Fnâˆ’rğ®=0_F^n-r for uâˆˆFnâˆ’rğ®âˆˆF^n-r:
Xu=0FnâŸ¹(âˆ’BInâˆ’r)u=0FnâŸ¹(âˆ’Buu)=(0Fr0Fnâˆ’r)âŸ¹u=0Fnâˆ’r.ğ—ğ®=0_F^n[    -ğ; ğˆ_n-r ]ğ®=0_F^n[ -ğğ®;   ğ® ]=[   0_F^r; 0_F^n-r ]ğ®=0_F^n-r.
Therefore, the column vectors of Xğ— constitute a set of nâˆ’r-r linearly independent solutions for Ax=0Fmğ€ğ±=0_ğ”½^m.
We next prove that any solution of Ax=0Fmğ€ğ±=0_F^m must be a linear combination of the columns of Xğ—.
For this, let
u=(u1u2)âˆˆFnğ®=[ ğ®_1; ğ®_2 ]âˆˆF^n
be any vector such that Au=0Fmğ€ğ®=0_F^m. Since the columns of A1ğ€_1 are linearly independent, A1x=0Fmğ€_1ğ±=0_F^m implies x=0Frğ±=0_F^r.
Therefore,
Au=0FmâŸ¹(A1A1B)(u1u2)=A1u1+A1Bu2=A1(u1+Bu2)=0FmâŸ¹u1+Bu2=0FrâŸ¹u1=âˆ’Bu2[                        ğ€ğ®                         =                     0_F^m; [  ğ€_1 ğ€_1ğ ][ ğ®_1; ğ®_2 ]                         =            ğ€_1ğ®_1+ğ€_1ğğ®_2                         =             ğ€_1(ğ®_1+ğğ®_2)                         =                     0_ğ”½^m;                  ğ®_1+ğğ®_2                         =                     0_F^r;                       ğ®_1                         =                     -ğğ®_2 ]
âŸ¹u=(u1u2)=(âˆ’BInâˆ’r)u2=Xu2.ğ®=[ ğ®_1; ğ®_2 ]=[    -ğ; ğˆ_n-r ]ğ®_2=ğ—ğ®_2.
This proves that any vector uğ® that is a solution of Ax=0ğ€ğ±=0 must be a linear combination of the nâˆ’r-r special solutions given by the columns of Xğ—. And we have already seen that the columns of Xğ— are linearly independent. Hence, the columns of Xğ— constitute a basis for the null space of Ağ€. Therefore, the nullity of Ağ€ is nâˆ’r-r. Since r equals rank of Ağ€, it follows that Rankâ¡(A)+Nullityâ¡(A)=nRank(ğ€)+Nullity(ğ€)=n. This concludes our proof.

A third fundamental subspace[edit]
When T:Vâ†’W:V is a linear transformation between two finite-dimensional subspaces, with n=dimâ¡(V)=(V) and m=dimâ¡(W)=(W) (so can be represented by an mÃ—n matrix M), the rankâ€“nullity theorem asserts that if T has rank r, then nâˆ’r-r is the dimension of the null space of M, which represents the kernel of T.  In some texts, a third fundamental subspace associated to T is considered alongside its image and kernel: the cokernel of T is the  quotient space W/Imageâ¡(T)/Image(T), and its dimension is mâˆ’r-r.  This dimension formula (which might also be rendered dimâ¡Imageâ¡(T)+dimâ¡Cokerâ¡(T)=dimâ¡(W)Image(T)+Coker(T)=(W)) together with the rankâ€“nullity theorem is sometimes called the fundamental theorem of linear algebra.[7][8]

Reformulations and generalizations[edit]
This theorem is a statement of the first isomorphism theorem of algebra for the case of vector spaces; it generalizes to the splitting lemma.
In more modern language, the theorem can also be phrased as saying that each short exact sequence of vector spaces splits. Explicitly, given that
0â†’Uâ†’Vâ†’TRâ†’00Tâ†’Râ†’0
is a short exact sequence of vector spaces, then UâŠ•Râ‰…V, hence
dimâ¡(U)+dimâ¡(R)=dimâ¡(V).(U)+(R)=(V).
Here R plays the role of Imâ¡TImT and U is Kerâ¡TKerT, i.e.
0â†’kerâ¡Tâ†ªVâ†’Timâ¡Tâ†’00â†’â†ªVTâ†’imTâ†’0
In the finite-dimensional case, this formulation is susceptible to a generalization: if
0â†’V1â†’V2â†’â‹¯Vrâ†’00_1_2â†’_râ†’0
is an exact sequence of finite-dimensional vector spaces, then[9]
âˆ‘i=1r(âˆ’1)idimâ¡(Vi)=0.âˆ‘_i=1^r(-1)^i(V_i)=0.
The rankâ€“nullity theorem for finite-dimensional vector spaces may also be formulated in terms of the index of a linear map. The index of a linear map TâˆˆHomâ¡(V,W)âˆˆHom(V,W), where V and W are finite-dimensional, is defined by
indexâ¡T=dimâ¡Kerâ¡(T)âˆ’dimâ¡Cokerâ¡T.indexT=Ker(T)-CokerT.
Intuitively, dimâ¡Kerâ¡TKerT is the number of independent solutions v of the equation Tv=0=0, and dimâ¡Cokerâ¡TCokerT is the number of independent restrictions that have to be put on w to make Tv=w=w solvable. The rankâ€“nullity theorem for finite-dimensional vector spaces is equivalent to the statement
indexâ¡T=dimâ¡Vâˆ’dimâ¡W.indexT=-.
We see that we can easily read off the index of the linear map T from the involved spaces, without any need to analyze T in detail. This effect also occurs in a much deeper result: the Atiyahâ€“Singer index theorem states that the index of certain differential operators can be read off the geometry of the involved spaces.

Citations[edit]

^ Axler (2015) p. 63, Â§3.22

^ a b Friedberg, Insel & Spence (2014) p. 70, Â§2.1, Theorem 2.3

^ Katznelson & Katznelson (2008) p. 52, Â§2.5.1

^ Valenza (1993) p. 71, Â§4.3

^ Friedberg, Insel & Spence (2014) pp. 103-104, Â§2.4, Theorem 2.20

^ Banerjee, Sudipto; Roy, Anindya (2014), Linear Algebra and Matrix Analysis for Statistics, Texts in Statistical Science (1stÂ ed.), Chapman and Hall/CRC, ISBNÂ 978-1420095388

^ * Strang, Gilbert. Linear Algebra and Its Applications. 3rd ed. Orlando: Saunders, 1988.

^ Strang, Gilbert (1993), "The fundamental theorem of linear algebra" (PDF), American Mathematical Monthly, 100 (9): 848â€“855, CiteSeerXÂ 10.1.1.384.2309, doi:10.2307/2324660, JSTORÂ 2324660

^ Zaman, Ragib. "Dimensions of vector spaces in an exact sequence". Mathematics Stack Exchange. Retrieved 27 October 2015.


References[edit]
Axler, Sheldon (2015). Linear Algebra Done Right. Undergraduate Texts in Mathematics (3rdÂ ed.). Springer. ISBNÂ 978-3-319-11079-0.
Banerjee, Sudipto; Roy, Anindya (2014), Linear Algebra and Matrix Analysis for Statistics, Texts in Statistical Science (1stÂ ed.), Chapman and Hall/CRC, ISBNÂ 978-1420095388
Friedberg, Stephen H.; Insel, Arnold J.; Spence, Lawrence E. (2014). Linear Algebra (4thÂ ed.). Pearson Education. ISBNÂ 978-0130084514.
Meyer, Carl D. (2000), Matrix Analysis and Applied Linear Algebra, SIAM, ISBNÂ 978-0-89871-454-8.
Katznelson, Yitzhak; Katznelson, Yonatan R. (2008). A (Terse) Introduction to Linear Algebra. American Mathematical Society. ISBNÂ 978-0-8218-4419-9.
Valenza, Robert J. (1993) [1951]. Linear Algebra: An Introduction to Abstract Mathematics. Undergraduate Texts in Mathematics (3rdÂ ed.). Springer. ISBNÂ 3-540-94099-5.
External links[edit]
Gilbert Strang, MIT Linear Algebra Lecture on the Four Fundamental Subspaces, from MIT OpenCourseWare



