Theorem in statistics
This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: "Lehmann–Scheffé theorem" – news · newspapers · books · scholar · JSTOR (April 2011) (Learn how and when to remove this template message)
In statistics, the Lehmann–Scheffé theorem is a prominent statement, tying together the ideas of completeness, sufficiency, uniqueness, and best unbiased estimation.[1] The theorem states that any estimator which is unbiased for a given unknown quantity and that depends on the data only through a complete, sufficient statistic is the unique best unbiased estimator of that quantity. The Lehmann–Scheffé theorem is named after Erich Leo Lehmann and Henry Scheffé, given their two early papers.[2][3]
If T is a complete sufficient statistic for θ and E(g(T)) = τ(θ) then g(T) is the uniformly minimum-variance unbiased estimator (UMVUE) of τ(θ).


Statement[edit]
Let X→=X1,X2,…,XnX⃗=X_1,X_2,…,X_n be a random sample from a distribution that has p.d.f (or p.m.f in the discrete case) f(x:θ)(x:θ) where θ∈Ωθ∈Ω is a parameter in the parameter space. Suppose Y=u(X→)=u(X⃗) is a sufficient statistic for θ, and let fY(y:θ):θ∈Ω{f_Y(y:θ):θ∈Ω} be a complete family. If φ:E⁡[φ(Y)]=θφ:E[φ(Y)]=θ then φ(Y)φ(Y) is the unique MVUE of θ.

Proof[edit]
By the Rao–Blackwell theorem, if Z is an unbiased estimator of θ then φ(Y):=E⁡[Z∣Y]φ(Y):=E[Z] defines an unbiased estimator of θ with the property that its variance is not greater than that of Z.
Now we show that this function is unique. Suppose W is another candidate MVUE estimator of θ. Then again ψ(Y):=E⁡[W∣Y]ψ(Y):=E[W] defines an unbiased estimator of θ with the property that its variance is not greater than that of W. Then

E⁡[φ(Y)−ψ(Y)]=0,θ∈Ω.E[φ(Y)-ψ(Y)]=0,θ∈Ω.
Since fY(y:θ):θ∈Ω{f_Y(y:θ):θ∈Ω} is a complete family

E⁡[φ(Y)−ψ(Y)]=0⟹φ(y)−ψ(y)=0,θ∈ΩE[φ(Y)-ψ(Y)]=0φ(y)-ψ(y)=0,θ∈Ω
and therefore the function φφ is the unique function of Y with variance not greater than that of any other unbiased estimator. We conclude that φ(Y)φ(Y) is the MVUE.

Example for when using a non-complete minimal sufficient statistic[edit]
An example of an improvable Rao–Blackwell improvement, when using a minimal sufficient statistic that is not complete, was provided by Galili and Meilijson in 2016.[4] Let X1,…,Xn_1,…,X_n be a random sample from a scale-uniform distribution X∼U((1−k)θ,(1+k)θ),((1-k)θ,(1+k)θ), with unknown mean E⁡[X]=θE[X]=θ and known design parameter k∈(0,1)∈(0,1). In the search for "best" possible unbiased estimators for θθ, it is natural to consider X1_1 as an initial (crude) unbiased estimator for θθ and then try to improve it. Since X1_1 is not a function of T=(X(1),X(n))=(X_(1),X_(n)), the minimal sufficient statistic for θθ (where X(1)=miniXi_(1)=min_iX_i and X(n)=maxiXi_(n)=max_iX_i), it may be improved using the Rao–Blackwell theorem as follows: 

θ^RB=Eθ⁡[X1∣X(1),X(n)]=X(1)+X(n)2.θ̂_RB=E_θ[X_1_(1),X_(n)]=X_(1)+X_(n)/2.
However, the following unbiased estimator can be shown to have lower variance: 

θ^LV=1k2n−1n+1+1⋅(1−k)X(1)+(1+k)X(n)2.θ̂_LV=1/k^2n-1/n+1+1·(1-k)X_(1)+(1+k)X_(n)/2.
And in fact, it could be even further improved when using the following estimator:  

θ^BAYES=n+1n[1−X(1)(1+k)X(n)(1−k)−1(X(1)(1+k)X(n)(1−k))n+1−1]X(n)1+kθ̂_BAYES=n+1/n[1-X_(1)(1+k)/X_(n)(1-k)-1/(X_(1)(1+k)/X_(n)(1-k))^n+1-1]X_(n)/1+k
The model is a scale model. Optimal equivariant estimators can then be derived for loss functions that are invariant.[5]

See also[edit]
Basu's theorem
Complete class theorem
Rao–Blackwell theorem
References[edit]


^ Casella, George (2001). Statistical Inference. Duxbury Press. p. 369. ISBN 978-0-534-24312-8.

^ Lehmann, E. L.; Scheffé, H. (1950). "Completeness, similar regions, and unbiased estimation. I." Sankhyā. 10 (4): 305–340. doi:10.1007/978-1-4614-1412-4_23. JSTOR 25048038. MR 0039201.

^ Lehmann, E.L.; Scheffé, H. (1955). "Completeness, similar regions, and unbiased estimation. II". Sankhyā. 15 (3): 219–236. doi:10.1007/978-1-4614-1412-4_24. JSTOR 25048243. MR 0072410.

^ Tal Galili; Isaac Meilijson (31 Mar 2016). "An Example of an Improvable Rao–Blackwell Improvement, Inefficient Maximum Likelihood Estimator, and Unbiased Generalized Bayes Estimator". The American Statistician. 70 (1): 108–113. doi:10.1080/00031305.2015.1100683. PMC 4960505. PMID 27499547.

^ Taraldsen, Gunnar (2020). "Micha Mandel (2020), "The Scaled Uniform Model Revisited," The American Statistician, 74:1, 98–100: Comment". The American Statistician. 74 (3): 315. doi:10.1080/00031305.2020.1769727. S2CID 219493070.


vteStatistics
Outline
Index
Descriptive statisticsContinuous dataCenter
Mean
Arithmetic
Arithmetic-Geometric
Cubic
Generalized/power
Geometric
Harmonic
Heronian
Heinz
Lehmer
Median
Mode
Dispersion
Average absolute deviation
Coefficient of variation
Interquartile range
Percentile
Range
Standard deviation
Variance
Shape
Central limit theorem
Moments
Kurtosis
L-moments
Skewness
Count data
Index of dispersion
Summary tables
Contingency table
Frequency distribution
Grouped data
Dependence
Partial correlation
Pearson product-moment correlation
Rank correlation
Kendall's τ
Spearman's ρ
Scatter plot
Graphics
Bar chart
Biplot
Box plot
Control chart
Correlogram
Fan chart
Forest plot
Histogram
Pie chart
Q–Q plot
Radar chart
Run chart
Scatter plot
Stem-and-leaf display
Violin plot
Data collectionStudy design
Effect size
Missing data
Optimal design
Population
Replication
Sample size determination
Statistic
Statistical power
Survey methodology
Sampling
Cluster
Stratified
Opinion poll
Questionnaire
Standard error
Controlled experiments
Blocking
Factorial experiment
Interaction
Random assignment
Randomized controlled trial
Randomized experiment
Scientific control
Adaptive designs
Adaptive clinical trial
Stochastic approximation
Up-and-down designs
Observational studies
Cohort study
Cross-sectional study
Natural experiment
Quasi-experiment
Statistical inferenceStatistical theory
Population
Statistic
Probability distribution
Sampling distribution
Order statistic
Empirical distribution
Density estimation
Statistical model
Model specification
Lp space
Parameter
location
scale
shape
Parametric family
Likelihood (monotone)
Location–scale family
Exponential family
Completeness
Sufficiency
Statistical functional
Bootstrap
U
V
Optimal decision
loss function
Efficiency
Statistical distance
divergence
Asymptotics
Robustness
Frequentist inferencePoint estimation
Estimating equations
Maximum likelihood
Method of moments
M-estimator
Minimum distance
Unbiased estimators
Mean-unbiased minimum-variance
Rao–Blackwellization
Lehmann–Scheffé theorem
Median unbiased
Plug-in
Interval estimation
Confidence interval
Pivot
Likelihood interval
Prediction interval
Tolerance interval
Resampling
Bootstrap
Jackknife
Testing hypotheses
1- & 2-tails
Power
Uniformly most powerful test
Permutation test
Randomization test
Multiple comparisons
Parametric tests
Likelihood-ratio
Score/Lagrange multiplier
Wald
Specific tests
Z-test (normal)
Student's t-test
F-test
Goodness of fit
Chi-squared
G-test
Kolmogorov–Smirnov
Anderson–Darling
Lilliefors
Jarque–Bera
Normality (Shapiro–Wilk)
Likelihood-ratio test
Model selection
Cross validation
AIC
BIC
Rank statistics
Sign
Sample median
Signed rank (Wilcoxon)
Hodges–Lehmann estimator
Rank sum (Mann–Whitney)
Nonparametric anova
1-way (Kruskal–Wallis)
2-way (Friedman)
Ordered alternative (Jonckheere–Terpstra)
Van der Waerden test
Bayesian inference
Bayesian probability
prior
posterior
Credible interval
Bayes factor
Bayesian estimator
Maximum posterior estimator
CorrelationRegression analysisCorrelation
Pearson product-moment
Partial correlation
Confounding variable
Coefficient of determination
Regression analysis
Errors and residuals
Regression validation
Mixed effects models
Simultaneous equations models
Multivariate adaptive regression splines (MARS)
Linear regression
Simple linear regression
Ordinary least squares
General linear model
Bayesian regression
Non-standard predictors
Nonlinear regression
Nonparametric
Semiparametric
Isotonic
Robust
Heteroscedasticity
Homoscedasticity
Generalized linear model
Exponential families
Logistic (Bernoulli) / Binomial / Poisson regressions
Partition of variance
Analysis of variance (ANOVA, anova)
Analysis of covariance
Multivariate ANOVA
Degrees of freedom
Categorical / Multivariate / Time-series / Survival analysisCategorical
Cohen's kappa
Contingency table
Graphical model
Log-linear model
McNemar's test
Cochran–Mantel–Haenszel statistics
Multivariate
Regression
Manova
Principal components
Canonical correlation
Discriminant analysis
Cluster analysis
Classification
Structural equation model
Factor analysis
Multivariate distributions
Elliptical distributions
Normal
Time-seriesGeneral
Decomposition
Trend
Stationarity
Seasonal adjustment
Exponential smoothing
Cointegration
Structural break
Granger causality
Specific tests
Dickey–Fuller
Johansen
Q-statistic (Ljung–Box)
Durbin–Watson
Breusch–Godfrey
Time domain
Autocorrelation (ACF)
partial (PACF)
Cross-correlation (XCF)
ARMA model
ARIMA model (Box–Jenkins)
Autoregressive conditional heteroskedasticity (ARCH)
Vector autoregression (VAR)
Frequency domain
Spectral density estimation
Fourier analysis
Least-squares spectral analysis
Wavelet
Whittle likelihood
SurvivalSurvival function
Kaplan–Meier estimator (product limit)
Proportional hazards models
Accelerated failure time (AFT) model
First hitting time
Hazard function
Nelson–Aalen estimator
Test
Log-rank test
ApplicationsBiostatistics
Bioinformatics
Clinical trials / studies
Epidemiology
Medical statistics
Engineering statistics
Chemometrics
Methods engineering
Probabilistic design
Process / quality control
Reliability
System identification
Social statistics
Actuarial science
Census
Crime statistics
Demography
Econometrics
Jurimetrics
National accounts
Official statistics
Population statistics
Psychometrics
Spatial statistics
Cartography
Environmental statistics
Geographic information system
Geostatistics
Kriging

Category
 Mathematics portal
Commons
 WikiProject




