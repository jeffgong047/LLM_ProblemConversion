In functional analysis, a Hilbert space
Figure illustrates related but varying approaches to viewing RKHS
In functional analysis (a branch of mathematics), a reproducing kernel Hilbert space (RKHS) is a Hilbert space of functions in which point evaluation is a continuous linear functional. Roughly speaking, this means that if two functions f and g in the RKHS are close in norm, i.e., â€–fâˆ’gâ€–f-g is small, then f and g are also pointwise close, i.e., |f(x)âˆ’g(x)||f(x)-g(x)| is small for all x. The converse does not need to be true. Informally, this can be shown by looking at the supremum norm: the sequence of functions sinnâ¡(x)sin^n(x) converges pointwise, but does not converge uniformly i.e. does not converge with respect to the supremum norm. (This is not a counterexample because the supremum norm does not arise from any inner product due to not satisfying the parallelogram law.)
It is not entirely straightforward to construct a Hilbert space of functions which is not an RKHS.[1] Some examples, however, have been found.[2][3]
L2 spaces are not Hilbert spaces of functions (and hence not RKHSs), but rather Hilbert spaces of equivalence classes of functions (for example, the functions f and g defined by f(x)=0(x)=0 and g(x)=1Q(x)=1_â„š are equivalent in L2). However, there are RKHSs in which the norm is an L2-norm, such as the space of band-limited functions (see the example below).
An RKHS is associated with a kernel that reproduces every function in the space in the sense that for every x in the set on which the functions are defined, "evaluation at x" can be performed by taking an inner product with a function determined by the kernel. Such a reproducing kernel exists if and only if every evaluation functional is continuous.
The reproducing kernel was first introduced in the 1907 work of StanisÅ‚aw Zaremba concerning boundary value problems for harmonic and biharmonic functions.  James Mercer simultaneously examined functions which satisfy the reproducing property in the theory of integral equations. The idea of the reproducing kernel remained untouched for nearly twenty years until it appeared in the dissertations of GÃ¡bor SzegÅ‘, Stefan Bergman, and Salomon Bochner.  The subject was eventually systematically developed in the early 1950s by Nachman Aronszajn and Stefan Bergman.[4]
These spaces have wide applications, including complex analysis, harmonic analysis, and quantum mechanics.  Reproducing kernel Hilbert spaces are particularly important in the field of statistical learning theory because of the celebrated representer theorem which states that every function in an RKHS that minimises an empirical risk functional can be written as a linear combination of the kernel function evaluated at the training points.  This is a practically useful result as it effectively simplifies the empirical risk minimization problem from an infinite dimensional to a finite dimensional optimization problem.
For ease of understanding, we provide the framework for real-valued Hilbert spaces.  The theory can be easily extended to spaces of complex-valued functions and hence include the many important examples of reproducing kernel Hilbert spaces that are spaces of analytic functions.[5]


Definition[edit]
Let X be an arbitrary set and H a Hilbert space of real-valued functions on X, equipped with pointwise addition and pointwise scalar multiplication.  The evaluation functional over the Hilbert space of functions H is a linear functional that evaluates each function at a point x,

Lx:fâ†¦f(x)âˆ€fâˆˆH._x:f(x).
We say that H is a reproducing kernel Hilbert space if, for all x in X, Lx_x is continuous at every f in H or, equivalently, if Lx_x is a bounded operator on H, i.e. there exists some Mx>0_x>0 such that




|Lx(f)|:=|f(x)|â‰¤Mxâ€–fâ€–Hâˆ€fâˆˆH.|L_x(f)|:=|f(x)|_x f_H    . 





Â 

Â 

Â 



Â 



(1)

Although Mx<âˆ_x<âˆ is assumed for all xâˆˆX, it might still be the case that supxMx=âˆsup_xM_x=âˆ.
While property (1) is the weakest condition that ensures both the existence of an inner product and the evaluation of every function in H at every point in the domain, it does not lend itself to easy application in practice.  A more intuitive definition of the RKHS can be obtained by observing that this property guarantees that the evaluation functional can be represented by taking the inner product of f with a function Kx_x in H.  This function is the so-called reproducing kernel[citation needed] for the Hilbert space H from which the RKHS takes its name.  More formally, the Riesz representation theorem implies that for all x in X there exists a unique element Kx_x of H with the reproducing property,




f(x)=Lx(f)=âŸ¨f,KxâŸ©Hâˆ€fâˆˆH.(x)=L_x(f)=,_xâŸ©_H  .





Â 

Â 

Â 



Â 



(2)

Since Kx_x is itself a function defined on X with values in the field Râ„ (or Câ„‚ in the case of complex Hilbert spaces) and as Kx_x is in H we have that

Kx(y)=Ly(Kx)=âŸ¨Kx,KyâŸ©H,_x(y)=L_y(K_x)=_x,_yâŸ©_H,
where  KyâˆˆH_y is the element in H associated to Ly_y.
This allows us to define the reproducing kernel of H as a function K:XÃ—Xâ†’R:Xâ†’â„ by

K(x,y)=âŸ¨Kx,KyâŸ©H.(x,y)=_x,_yâŸ©_H.
From this definition it is easy to see that K:XÃ—Xâ†’R:Xâ†’â„ (or Câ„‚ in the complex case) is both symmetric (resp. conjugate symmetric) and positive definite, i.e.

âˆ‘i,j=1ncicjK(xi,xj)=âˆ‘i=1nciâŸ¨Kxi,âˆ‘j=1ncjKxjâŸ©H=âŸ¨âˆ‘i=1nciKxi,âˆ‘j=1ncjKxjâŸ©H=â€–âˆ‘i=1nciKxiâ€–H2â‰¥0âˆ‘_i,j=1^nc_ic_jK(x_i,x_j)=âˆ‘_i=1^nc_i_x_i,âˆ‘_j=1^nc_jK_x_jâŸ©_H=âŸ¨âˆ‘_i=1^nc_iK_x_i,âˆ‘_j=1^nc_jK_x_jâŸ©_H=âˆ‘_i=1^nc_iK_x_i_H^2â‰¥0
for every nâˆˆN,x1,â€¦,xnâˆˆX,andc1,â€¦,cnâˆˆR.âˆˆâ„•,x_1,â€¦,x_n,andc_1,â€¦,c_nâˆˆâ„.[6] The Mooreâ€“Aronszajn theorem (see below) is a sort of converse to this: if a function K satisfies these conditions then there is a Hilbert space of functions on X for which it is a reproducing kernel.

Example[edit]
The space of bandlimited continuous functions H is a RKHS, as we now show.  Formally, fix some cutoff frequency 0<a<âˆ0<a<âˆ and define the Hilbert space

H=fâˆˆC(R)âˆ£suppâ¡(F)âŠ‚[âˆ’a,a]={f(â„)|supp(F)âŠ‚[-a,a]}
where C(R)(â„) is the set of continuous square integrable functions, and F(Ï‰)=âˆ«âˆ’âˆâˆf(t)eâˆ’iÏ‰tdt(Ï‰)=âˆ«_-âˆ^âˆf(t)e^-i dt is the Fourier transform of f. As the inner product of this Hilbert space, we use

âŸ¨f,gâŸ©L2=âˆ«âˆ’âˆâˆf(x)â‹…g(x)Â¯dx.,gâŸ©_L^2=âˆ«_-âˆ^âˆf(x)Â·g(x) dx.
From the Fourier inversion theorem, we have

f(x)=12Ï€âˆ«âˆ’aaF(Ï‰)eixÏ‰dÏ‰.(x)=1/2Ï€âˆ«_-a^aF(Ï‰)e^ixÏ‰ dÏ‰.
It then follows by the Cauchyâ€“Schwarz inequality and Plancherel's theorem that, for all x,

|f(x)|â‰¤12Ï€âˆ«âˆ’aa2a|F(Ï‰)|2dÏ‰=1Ï€a2âˆ«âˆ’âˆâˆ|F(Ï‰)|2dÏ‰=aÏ€â€–fâ€–L2.|f(x)|â‰¤1/2Ï€âˆš(âˆ«_-a^a2a|F(Ï‰)|^2 dÏ‰)=1/Ï€âˆš(a/2âˆ«_-âˆ^âˆ|F(Ï‰)|^2 dÏ‰)=âˆš(a/Ï€)f_L^2.
This inequality shows that the evaluation functional is bounded, proving that H is indeed a RKHS.
The kernel function Kx_x in this case is given by

Kx(y)=aÏ€sincâ¡(aÏ€(yâˆ’x))=sinâ¡(a(yâˆ’x))Ï€(yâˆ’x)._x(y)=a/Ï€sinc(a/Ï€(y-x))=sin(a(y-x))/Ï€(y-x).
The Fourier transform of Kx(y)_x(y) defined above is given by

âˆ«âˆ’âˆâˆKx(y)eâˆ’iÏ‰ydy=eâˆ’iÏ‰xifÏ‰âˆˆ[âˆ’a,a],0otherwise,âˆ«_-âˆ^âˆK_x(y)e^-i dy=e^-i   ifÏ‰âˆˆ[-a,a],
0   otherwise,
which is a consequence of the time-shifting property of the Fourier transform. Consequently, using Plancherel's theorem, we have

âŸ¨f,KxâŸ©L2=âˆ«âˆ’âˆâˆf(y)â‹…Kx(y)Â¯dy=12Ï€âˆ«âˆ’aaF(Ï‰)â‹…eiÏ‰xdÏ‰=f(x).,K_xâŸ©_L^2=âˆ«_-âˆ^âˆf(y)Â·K_x(y) dy=1/2Ï€âˆ«_-a^aF(Ï‰)^i dÏ‰=f(x).
Thus we obtain the reproducing property of the kernel.
Kx_x in this case is the "bandlimited version" of the Dirac delta function, and that Kx(y)_x(y) converges to Î´(yâˆ’x)Î´(y-x) in the weak sense as the cutoff frequency a tends to infinity.

Mooreâ€“Aronszajn theorem[edit]
We have seen how a reproducing kernel Hilbert space defines a reproducing kernel function that is both symmetric and positive definite. The Mooreâ€“Aronszajn theorem goes in the other direction; it states that every symmetric, positive definite kernel defines a unique reproducing kernel Hilbert space. The theorem first appeared in Aronszajn's Theory of Reproducing Kernels, although he attributes it to E. H. Moore.

Theorem. Suppose K is a symmetric, positive definite kernel on a set X. Then there is a unique Hilbert space of functions on X for which K is a reproducing kernel.
Proof. For all x in X, define Kx = K(x, â‹… ). Let H0 be the linear span of {KxÂ : x âˆˆ X}. Define an inner product on H0 by

âŸ¨âˆ‘j=1nbjKyj,âˆ‘i=1maiKxiâŸ©H0=âˆ‘i=1mâˆ‘j=1naibjK(yj,xi),âŸ¨âˆ‘_j=1^nb_jK_y_j,âˆ‘_i=1^ma_iK_x_iâŸ©_H_0=âˆ‘_i=1^mâˆ‘_j=1^na_ib_jK(y_j,x_i),
which implies K(x,y)=âŸ¨Kx,KyâŸ©H0(x,y)=_x,K_yâŸ©_H_0.
The symmetry of this inner product follows from the symmetry of K and the non-degeneracy follows from the fact that K is positive definite.
Let H be the completion of H0 with respect to this inner product. Then H consists of functions of the form

f(x)=âˆ‘i=1âˆaiKxi(x)wherelimnâ†’âˆsuppâ‰¥0â€–âˆ‘i=nn+paiKxiâ€–H0=0.(x)=âˆ‘_i=1^âˆa_iK_x_i(x)  where  lim_nâ†’âˆsup_pâ‰¥0âˆ‘_i=n^n+pa_iK_x_i_H_0=0.
Now we can check the reproducing property (2):

âŸ¨f,KxâŸ©H=âˆ‘i=1âˆaiâŸ¨Kxi,KxâŸ©H0=âˆ‘i=1âˆaiK(xi,x)=f(x).,K_xâŸ©_H=âˆ‘_i=1^âˆa_i_x_i,K_xâŸ©_H_0=âˆ‘_i=1^âˆa_iK(x_i,x)=f(x).
To prove uniqueness, let G be another Hilbert space of functions for which K is a reproducing kernel. For every x and y in X, (2) implies that

âŸ¨Kx,KyâŸ©H=K(x,y)=âŸ¨Kx,KyâŸ©G._x,K_yâŸ©_H=K(x,y)=_x,K_yâŸ©_G.
By linearity, âŸ¨â‹…,â‹…âŸ©H=âŸ¨â‹…,â‹…âŸ©GâŸ¨Â·,Â·âŸ©_H=âŸ¨Â·,Â·âŸ©_G on the span of Kx:xâˆˆX{K_x:x}. Then HâŠ‚G because G is complete and contains H0 and hence contains its completion.
Now we need to prove that every element of G is in H. Let f be an element of G. Since H is a closed subspace of G, we can write f=fH+fHâŠ¥=f_H+f_H^ where fHâˆˆH_H and fHâŠ¥âˆˆHâŠ¥_H^^. Now if xâˆˆX then, since K is a reproducing kernel of G and H:

f(x)=âŸ¨Kx,fâŸ©G=âŸ¨Kx,fHâŸ©G+âŸ¨Kx,fHâŠ¥âŸ©G=âŸ¨Kx,fHâŸ©G=âŸ¨Kx,fHâŸ©H=fH(x),(x)=_x,fâŸ©_G=_x,f_HâŸ©_G+_x,f_H^âŸ©_G=_x,f_HâŸ©_G=_x,f_HâŸ©_H=f_H(x),
where we have used the fact that Kx_x belongs to H so that its inner product with fHâŠ¥_H^ in G is zero.
This shows that f=fH=f_H in G and concludes the proof.

Integral operators and Mercer's theorem[edit]
We may characterize a symmetric positive definite kernel K via the integral operator using Mercer's theorem and obtain an additional view of the RKHS. Let X be a compact space equipped with a strictly positive finite Borel measure Î¼Î¼ and K:XÃ—Xâ†’R:Xâ†’â„ a continuous, symmetric, and positive definite function. Define the integral operator TK:L2(X)â†’L2(X)_K:L_2(X)_2(X) as

[TKf](â‹…)=âˆ«XK(â‹…,t)f(t)dÎ¼(t)[T_Kf](Â·)=âˆ«_XK(Â·,t)f(t) dÎ¼(t)
where L2(X)_2(X) is the space of square integrable functions with respect to Î¼Î¼.
Mercer's theorem states that the spectral decomposition of the integral operator TK_K of K yields a series representation of K in terms of the eigenvalues and eigenfunctions of TK_K. This then implies that K is a reproducing kernel so that the corresponding RKHS can be defined in terms of these eigenvalues and eigenfunctions. We provide the details below.
Under these assumptions TK_K is a compact, continuous, self-adjoint, and positive operator.  The spectral theorem for self-adjoint operators implies that there is an at most countable decreasing sequence (Ïƒi)iâ‰¥0(Ïƒ_i)_iâ‰¥0 such that limiâ†’âˆÏƒi=0lim_iâ†’âˆÏƒ_i=0 and
TKÏ†i(x)=ÏƒiÏ†i(x)_KÏ†_i(x)=Ïƒ_iÏ†_i(x), where the Ï†i{Ï†_i} form an orthonormal basis of L2(X)_2(X). By the positivity of TK,Ïƒi>0_K,Ïƒ_i>0 for all i.. One can also show that TK_K maps continuously into the space of continuous functions C(X)(X) and therefore we may choose continuous functions as the eigenvectors, that is, Ï†iâˆˆC(X)Ï†_i(X) for all i.. Then by Mercer's theorem  K may be written in terms of the eigenvalues and continuous eigenfunctions as

K(x,y)=âˆ‘j=1âˆÏƒjÏ†j(x)Ï†j(y)(x,y)=âˆ‘_j=1^âˆÏƒ_j Ï†_j(x) Ï†_j(y)
for all x,yâˆˆX,y such that

limnâ†’âˆsupu,v|K(u,v)âˆ’âˆ‘j=1nÏƒjÏ†j(u)Ï†j(v)|=0.lim_nâ†’âˆsup_u,v|K(u,v)-âˆ‘_j=1^nÏƒ_j Ï†_j(u) Ï†_j(v)|=0.
This above series representation is referred to as a Mercer kernel or Mercer representation of K.
Furthermore, it can be shown that the RKHS H of K is given by

H=fâˆˆL2(X)|âˆ‘i=1âˆâŸ¨f,Ï†iâŸ©L22Ïƒi<âˆ={f_2(X) | âˆ‘_i=1^âˆ,Ï†_iâŸ©_L_2^2/Ïƒ_i<âˆ}
where the inner product of H given by

âŸ¨f,gâŸ©H=âˆ‘i=1âˆâŸ¨f,Ï†iâŸ©L2âŸ¨g,Ï†iâŸ©L2Ïƒi.,gâŸ©_H=âˆ‘_i=1^âˆ,Ï†_iâŸ©_L_2,Ï†_iâŸ©_L_2/Ïƒ_i.
This representation of the RKHS has application in probability and statistics, for example  to the Karhunen-LoÃ¨ve representation for stochastic processes and kernel PCA.

Feature maps[edit]
A feature map is a map Ï†:Xâ†’FÏ†, where F is a Hilbert space which we will call the feature space.  The first sections presented the connection between bounded/continuous evaluation functions, positive definite functions, and integral operators and in this section we provide another representation of the RKHS in terms of feature maps.
Every feature map defines a kernel via




K(x,y)=âŸ¨Ï†(x),Ï†(y)âŸ©F.(x,y)=âŸ¨Ï†(x),Ï†(y)âŸ©_F. 





Â 

Â 

Â 



Â 



(3)

Clearly K is symmetric and positive definiteness follows from the properties of inner product in F.  Conversely, every positive definite function and corresponding reproducing kernel Hilbert space has infinitely many associated feature maps such that (3) holds.
For example, we can trivially take F=H=H and Ï†(x)=KxÏ†(x)=K_x for all xâˆˆX.  Then (3) is satisfied by the reproducing property. Another classical example of a feature map relates to the previous section regarding integral operators by taking F=â„“2=â„“^2 and Ï†(x)=(ÏƒiÏ†i(x))iÏ†(x)=(âˆš(Ïƒ_i)Ï†_i(x))_i.
This connection between kernels and feature maps provides us with a new way to understand positive definite functions and hence reproducing kernels as inner products in H. Moreover, every feature map can naturally define a RKHS by means of the definition of a positive definite function.
Lastly, feature maps allow us to construct function spaces that reveal another perspective on the RKHS.  Consider the linear space

HÏ†=f:Xâ†’Râˆ£âˆƒwâˆˆF,f(x)=âŸ¨w,Ï†(x)âŸ©F,âˆ€xâˆˆX._Ï†={f:Xâ†’â„|,f(x)=,Ï†(x)âŸ©_F,âˆ€x}.
We can define a norm on HÏ†_Ï†  by

â€–fâ€–Ï†=infâ€–wâ€–F:wâˆˆF,f(x)=âŸ¨w,Ï†(x)âŸ©F,âˆ€xâˆˆX.f_Ï†=inf{w_F:w,f(x)=,Ï†(x)âŸ©_F,âˆ€x}.
It can be shown that HÏ†_Ï† is a RKHS with kernel defined by K(x,y)=âŸ¨Ï†(x),Ï†(y)âŸ©F(x,y)=âŸ¨Ï†(x),Ï†(y)âŸ©_F.  This representation implies that the elements of the RKHS are inner products of elements in the feature space and can accordingly be seen as hyperplanes.  This view of the RKHS is related to the kernel trick in machine learning.[7]

Properties[edit]
The following properties of RKHSs may be useful to readers.

Let (Xi)i=1p(X_i)_i=1^p be a sequence of sets and (Ki)i=1p(K_i)_i=1^p be a collection of corresponding positive definite functions on (Xi)i=1p.(X_i)_i=1^p. It then follows that
K((x1,â€¦,xp),(y1,â€¦,yp))=K1(x1,y1)â‹¯Kp(xp,yp)((x_1,â€¦,x_p),(y_1,â€¦,y_p))=K_1(x_1,y_1)_p(x_p,y_p)
is a kernel on X=X1Ã—â‹¯Ã—Xp.=X_1Ã—â€¦_p.
Let X0âŠ‚X,_0, then the restriction of K to X0Ã—X0_0_0 is also a reproducing kernel.
Consider a normalized kernel K such that K(x,x)=1(x,x)=1 for all xâˆˆX. Define a pseudo-metric on X as
dK(x,y)=â€–Kxâˆ’Kyâ€–H2=2(1âˆ’K(x,y))âˆ€xâˆˆX._K(x,y)=K_x-K_y_H^2=2(1-K(x,y))    .
By the Cauchyâ€“Schwarz inequality,
K(x,y)2â‰¤K(x,x)K(y,y)=1âˆ€x,yâˆˆX.(x,y)^2(x,x)K(y,y)=1    ,y.
This inequality allows us to view K as a measure of similarity between inputs. If x,yâˆˆX,y are similar then K(x,y)(x,y) will be closer to 1 while if x,yâˆˆX,y are dissimilar then K(x,y)(x,y) will be closer to 0.
The closure of the span of Kxâˆ£xâˆˆX{K_x} coincides with H.[8]
Common examples[edit]
Bilinear kernels[edit]
K(x,y)=âŸ¨x,yâŸ©(x,y)=,yâŸ©
The RKHS H corresponding to this kernel is the dual space, consisting of functions f(x)=âŸ¨x,Î²âŸ©(x)=,Î²âŸ© satisfying â€–fâ€–H2=â€–Î²â€–2f_H^2=Î²^2.

Polynomial kernels[edit]
K(x,y)=(Î±âŸ¨x,yâŸ©+1)d,Î±âˆˆR,dâˆˆN(x,y)=(Î±,yâŸ©+1)^d,    Î±âˆˆâ„,dâˆˆâ„•
Radial basis function kernels[edit]
These are another common class of kernels which satisfy K(x,y)=K(â€–xâˆ’yâ€–)(x,y)=K(x-y). Some examples include:

Gaussian or squared exponential kernel:
K(x,y)=eâˆ’â€–xâˆ’yâ€–22Ïƒ2,Ïƒ>0(x,y)=e^-x-y^2/2Ïƒ^2,    Ïƒ>0
Laplacian kernel:
K(x,y)=eâˆ’â€–xâˆ’yâ€–Ïƒ,Ïƒ>0(x,y)=e^-x-y/Ïƒ,    Ïƒ>0
The squared norm of a function f in the RKHS H with this kernel is:[9]
â€–fâ€–H2=âˆ«R(1Ïƒf(x)2+Ïƒfâ€²(x)2)dx.f_H^2=âˆ«_â„(1/Ïƒf(x)^2+'(x)^2)dx.
Bergman kernels[edit]
We also provide examples of Bergman kernels. Let X be finite and let H consist of all complex-valued functions on X.  Then an element of H can be represented as an array of complex numbers. If the usual inner product is used, then Kx is the function whose value is 1 at x and 0 everywhere else, and K(x,y)(x,y) can be thought of as an identity matrix since

K(x,y)=1x=y0xâ‰ y(x,y)=1   x=y
0   x
In this case, H is isomorphic to Cnâ„‚^n.
The case of X=D=ğ”» (where Dğ”» denotes the unit disc) is more sophisticated. Here the Bergman space H2(D)^2(ğ”») is the space of square-integrable holomorphic functions on Dğ”». It can be shown that the reproducing kernel for H2(D)^2(ğ”») is

K(x,y)=1Ï€1(1âˆ’xyÂ¯)2.(x,y)=1/Ï€1/(1-xy)^2.
Lastly, the space of band limited functions in L2(R)^2(â„) with bandwidth 2a2a is a RKHS with reproducing kernel

K(x,y)=sinâ¡a(xâˆ’y)Ï€(xâˆ’y).(x,y)=(x-y)/Ï€(x-y).
Extension to vector-valued functions[edit]
In this section we extend the definition of the RKHS to spaces of vector-valued functions as this extension is particularly important in multi-task learning and manifold regularization.  The main difference is that the reproducing kernel Î“Î“ is a symmetric function that is now a positive semi-definite matrix for every x,y,y in X.  More formally, we define a vector-valued RKHS (vvRKHS) as a Hilbert space of functions f:Xâ†’RT:Xâ†’â„^T such that for all câˆˆRTâˆˆâ„^T and xâˆˆX

Î“xc(y)=Î“(x,y)câˆˆHforyâˆˆXÎ“_xc(y)=Î“(x,y)cfory
and

âŸ¨f,Î“xcâŸ©H=f(x)âŠºc.,Î“_xcâŸ©_H=f(x)^âŠºc.
This second property parallels the reproducing property for the scalar-valued case.   This definition can also be connected to integral operators, bounded evaluation functions, and feature maps as we saw for the scalar-valued RKHS.  We can equivalently define the vvRKHS as a vector-valued Hilbert space with a bounded evaluation functional and show that this implies the existence of a unique reproducing kernel by the Riesz Representation theorem.  Mercer's theorem can also be extended to address the vector-valued setting and we can therefore obtain a feature map view of the vvRKHS. Lastly, it can also be shown that the closure of the span of Î“xc:xâˆˆX,câˆˆRT{Î“_xc:x,câˆˆâ„^T} coincides with H, another property similar to the scalar-valued case.
We can gain intuition for the vvRKHS by taking a component-wise perspective on these spaces.  In particular, we find that every vvRKHS is isometrically isomorphic to a scalar-valued RKHS on a particular input space. Let Î›=1,â€¦,TÎ›={1,â€¦,T}.  Consider the space XÃ—Î›Ã—Î› and the corresponding reproducing kernel




Î³:XÃ—Î›Ã—XÃ—Î›â†’R.Î³:XÃ—Î›Ã—Î›â†’â„.





Â 

Â 

Â 



Â 



(4)

As noted above, the RKHS associated to this reproducing kernel is given by the closure of the span of Î³(x,t):xâˆˆX,tâˆˆÎ›{Î³_(x,t):x,tâˆˆÎ›} where 
Î³(x,t)(y,s)=Î³((x,t),(y,s))Î³_(x,t)(y,s)=Î³((x,t),(y,s)) for every set of pairs (x,t),(y,s)âˆˆXÃ—Î›(x,t),(y,s)Ã—Î›.
The connection to the scalar-valued RKHS can then be made by the fact that every matrix-valued kernel can be identified with a kernel of the form of (4) via

Î“(x,y)(t,s)=Î³((x,t),(y,s)).Î“(x,y)_(t,s)=Î³((x,t),(y,s)).
Moreover, every kernel with the form of (4) defines a matrix-valued kernel with the above expression.  Now letting the map D:HÎ“â†’HÎ³:H_Î“_Î³ be defined as

(Df)(x,t)=âŸ¨f(x),etâŸ©RT(Df)(x,t)=(x),e_tâŸ©_â„^T
where et_t is the tth^th component of the canonical basis for RTâ„^T, one can show that D is bijective and an isometry between HÎ“_Î“ and HÎ³_Î³.
While this view of the vvRKHS can be useful in multi-task learning, this isometry does not reduce the study of the vector-valued case to that of the scalar-valued case.  In fact, this isometry procedure can make both the scalar-valued kernel and the input space too difficult to work with in practice as properties of the original kernels are often lost.[10][11][12]
An important class of matrix-valued reproducing kernels are separable kernels which can factorized as the product of a scalar valued kernel and a T-dimensional symmetric positive semi-definite matrix.  In light of our previous discussion these kernels are of the form

Î³((x,t),(y,s))=K(x,y)KT(t,s)Î³((x,t),(y,s))=K(x,y)K_T(t,s)
for all x,y,y in X and t,s,s in T.  As the scalar-valued kernel encodes dependencies between the inputs, we can observe that the matrix-valued kernel encodes dependencies among both the inputs and the outputs.
We lastly remark that the above theory can be further extended to spaces of functions with values in function spaces but obtaining kernels for these spaces is a more difficult task.[13]

Connection between RKHSs and the ReLU function[edit]
The ReLU function is commonly defined as f(x)=max0,x(x)=max{0,x} and is a mainstay in the architecture of neural networks where it is used as an activation function. One can construct a ReLU-like nonlinear function using the theory of reproducing kernel Hilbert spaces. Below, we derive this construction and show how it implies the representation power of neural networks with ReLU activations. 
We will work with the Hilbert space H=L21(0)[0,âˆ)â„‹=L_2^1(0)[0,âˆ) of absolutely continuous functions with f(0)=0(0)=0 and square integrable (i.e. L2_2) derivative. It has the inner product 

âŸ¨f,gâŸ©H=âˆ«0âˆfâ€²(x)gâ€²(x)dx.,gâŸ©_â„‹=âˆ«_0^âˆf'(x)g'(x) dx.
To construct the reproducing kernel it suffices to consider a dense subspace, so let fâˆˆC1[0,âˆ)^1[0,âˆ) and f(0)=0(0)=0. 
The Fundamental Theorem of Calculus then gives 

f(y)=âˆ«0yfâ€²(x)dx=âˆ«0âˆG(x,y)fâ€²(x)dx=âŸ¨Ky,fâŸ©(y)=âˆ«_0^yf'(x) dx=âˆ«_0^âˆG(x,y)f'(x) dx=_y,fâŸ©
where

G(x,y)=1,x<y0,otherwise(x,y)=1,   x<y
0,   otherwise
and Kyâ€²(x)=G(x,y),Ky(0)=0_y'(x)=G(x,y),_y(0)=0 i.e.

K(x,y)=Ky(x)=âˆ«0xG(z,y)dz=x,0â‰¤x<yy,otherwise.=min(x,y)(x,y)=K_y(x)=âˆ«_0^xG(z,y) dz=x,   0<y
y,   otherwise.=min(x,y)
This implies Ky=K(â‹…,y)_y=K(Â·,y) reproduces f.
Moreover the minimum function on XÃ—X=[0,âˆ)Ã—[0,âˆ)=[0,âˆ)Ã—[0,âˆ) has the following representations with the ReLu function: 

min(x,y)=xâˆ’ReLUâ¡(xâˆ’y)=yâˆ’ReLUâ¡(yâˆ’x).min(x,y)=x-ReLU(x-y)=y-ReLU(y-x).
Using this formulation, we can apply the representer theorem to the RKHS, letting one prove the optimality of using ReLU activations in neural network settings.[citation needed]

See also[edit]
Positive definite kernel
Mercer's theorem
Kernel trick
Kernel embedding of distributions
Representer theorem
Notes[edit]


^ Alpay, D., and T. M. Mills. "A family of Hilbert spaces which are not reproducing kernel Hilbert spaces." J. Anal. Appl. 1.2 (2003): 107â€“111.

^  Z. Pasternak-Winiarski, "On weights which admit reproducing kernel of Bergman type", International Journal of Mathematics and Mathematical Sciences, vol. 15, Issue 1, 1992. 

^  T. Å. Å»ynda, "On weights which admit reproducing kernel of SzegÂ¨o type", Journal of Contemporary Mathematical Analysis (Armenian Academy of Sciences), 55, 2020. 

^ Okutmustur

^ Paulson

^ Durrett

^ Rosasco

^ Rosasco

^ Berlinet, Alain and Thomas, Christine. Reproducing kernel Hilbert spaces in Probability and Statistics, Kluwer Academic Publishers, 2004

^ De Vito

^ Zhang

^ Alvarez

^ Rosasco


References[edit]
Alvarez, Mauricio, Rosasco, Lorenzo and Lawrence, Neil, â€œKernels for Vector-Valued Functions: a Review,â€ https://arxiv.org/abs/1106.6251, June 2011.
Aronszajn, Nachman (1950). "Theory of Reproducing Kernels". Transactions of the American Mathematical Society. 68 (3): 337â€“404. doi:10.1090/S0002-9947-1950-0051437-7. JSTORÂ 1990404. MRÂ 0051437.
Berlinet, Alain and Thomas, Christine. Reproducing kernel Hilbert spaces in Probability and Statistics, Kluwer Academic Publishers, 2004.
Cucker, Felipe; Smale, Steve (2002). "On the Mathematical Foundations of Learning". Bulletin of the American Mathematical Society. 39 (1): 1â€“49. doi:10.1090/S0273-0979-01-00923-5. MRÂ 1864085.
De Vito, Ernest, Umanita, Veronica, and Villa, Silvia. "An extension of Mercer theorem to vector-valued measurable kernels," arXiv:1110.4017, June 2013.
Durrett, Greg.  9.520 Course Notes, Massachusetts Institute of Technology, https://www.mit.edu/~9.520/scribe-notes/class03_gdurett.pdf, February 2010.
Kimeldorf, George; Wahba, Grace (1971). "Some results on Tchebycheffian Spline Functions" (PDF). Journal of Mathematical Analysis and Applications. 33 (1): 82â€“95. doi:10.1016/0022-247X(71)90184-3. MRÂ 0290013.
Okutmustur, Baver.   â€œReproducing Kernel Hilbert Spaces,â€ M.S. dissertation, Bilkent University, http://www.thesis.bilkent.edu.tr/0002953.pdf, August 2005.
Paulsen, Vern. â€œAn introduction to the theory of reproducing kernel Hilbert spaces,â€ http://www.math.uh.edu/~vern/rkhs.pdf.
Steinwart, Ingo; Scovel, Clint (2012). "Mercer's theorem on general domains: On the interaction between measures, kernels, and RKHSs". Constr. Approx. 35 (3): 363â€“417. doi:10.1007/s00365-012-9153-3. MRÂ 2914365.
Rosasco, Lorenzo and Poggio, Thomas.  "A Regularization Tour of Machine Learning â€“ MIT 9.520 Lecture Notes" Manuscript, Dec. 2014.
Wahba, Grace,  Spline Models for Observational Data, SIAM, 1990.
Zhang, Haizhang; Xu, Yuesheng; Zhang, Qinghui (2012). "Refinement of Operator-valued Reproducing Kernels" (PDF). Journal of Machine Learning Research. 13: 91â€“136.



