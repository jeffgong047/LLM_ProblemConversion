Mathematical theorem in stochastic processes
In the theory of stochastic processes in discrete time, a part of the mathematical theory of probability, the Doob decomposition theorem gives a unique decomposition of every adapted and integrable stochastic process as the sum of a martingale and a predictable process (or "drift") starting at zero. The theorem was proved by and is named for Joseph L. Doob.[1]
The analogous theorem in the continuous-time case is the Doob–Meyer decomposition theorem.


Statement[edit]
Let (Ω,F,P)(Ω,ℱ,ℙ) be a probability space, I = {0, 1, 2, ..., N} with N∈N∈ℕ or I=N0=ℕ_0 a finite or an infinite index set, (Fn)n∈I(ℱ_n)_n a filtration of Fℱ, and X = (Xn)n∈I an adapted stochastic process with E[|Xn|] < ∞ for all n ∈ I. Then there exist a martingale M = (Mn)n∈I and an integrable  predictable process A = (An)n∈I starting with  A0 = 0 such that Xn = Mn + An for every n ∈ I.
Here predictable means that An is Fn−1ℱ_n-1-measurable for every n ∈ I \ {0}.
This decomposition is almost surely unique.[2][3][4]

Remark[edit]
The theorem is valid word by word also for stochastic processes X taking values in the d-dimensional Euclidean space Rdℝ^d or the complex vector space Cdℂ^d. This follows from the one-dimensional version by considering the components individually.

Proof[edit]
Existence[edit]
Using conditional expectations, define the processes A and M, for every n ∈ I, explicitly by




An=∑k=1n(E[Xk|Fk−1]−Xk−1)_n=∑_k=1^n(𝔼[X_k | ℱ_k-1]-X_k-1)





 

 

 



 



(1)

and




Mn=X0+∑k=1n(Xk−E[Xk|Fk−1]),_n=X_0+∑_k=1^n(X_k-𝔼[X_k | ℱ_k-1]),





 

 

 



 



(2)

where the sums for n = 0 are empty and defined as zero. Here A adds up the expected increments of X, and M adds up the surprises, i.e., the part of every Xk that is not known one time step before.
Due to these definitions, An+1 (if n + 1 ∈ I) and Mn are Fn-measurable because the process X is adapted, E[|An|] < ∞ and E[|Mn|] < ∞ because the process X is integrable, and the decomposition Xn = Mn + An is valid for every n ∈ I. The martingale property

E[Mn−Mn−1|Fn−1]=0𝔼[M_n-M_n-1 | ℱ_n-1]=0    a.s.
also follows from the above definition (2), for every n ∈ I \ {0}.

Uniqueness[edit]
To prove uniqueness, let X = M' + A' be an additional decomposition. Then the process Y :=  M − M' = A' − A is a martingale, implying that

E[Yn|Fn−1]=Yn−1𝔼[Y_n | ℱ_n-1]=Y_n-1    a.s.,
and also predictable, implying that

E[Yn|Fn−1]=Yn𝔼[Y_n | ℱ_n-1]=Y_n    a.s.
for any n ∈ I \ {0}. Since Y0 = A'0 − A0 = 0 by the convention about the starting point of the predictable processes, this implies iteratively that Yn = 0 almost surely for all n ∈ I, hence the decomposition is almost surely unique.

Corollary[edit]
A real-valued stochastic process X is a submartingale if and only if it has a Doob decomposition into a martingale M and an integrable predictable process A that is almost surely increasing.[5] It is a supermartingale, if and only if A is almost surely decreasing.

Proof[edit]
If X is a submartingale, then

E[Xk|Fk−1]≥Xk−1𝔼[X_k | ℱ_k-1]_k-1    a.s.
for all k ∈ I \ {0}, which is equivalent to saying that every term in definition (1) of A is almost surely positive, hence A is almost surely increasing. The equivalence for supermartingales is proved similarly.

Example[edit]
Let X = (Xn)n∈N0ℕ_0 be a sequence in independent, integrable, real-valued random variables. They are adapted to the filtration generated by the sequence, i.e. Fn = σ(X0, . . . , Xn)  for all n ∈ N0ℕ_0. By (1) and (2), the Doob decomposition is given by

An=∑k=1n(E[Xk]−Xk−1),n∈N0,_n=∑_k=1^n(𝔼[X_k]-X_k-1),∈ℕ_0,
and

Mn=X0+∑k=1n(Xk−E[Xk]),n∈N0._n=X_0+∑_k=1^n(X_k-𝔼[X_k]),∈ℕ_0.
If the random variables of the original sequence X have mean zero, this simplifies to

An=−∑k=0n−1Xk_n=-∑_k=0^n-1X_k    and    Mn=∑k=0nXk,n∈N0,_n=∑_k=0^nX_k,∈ℕ_0,
hence both processes are (possibly time-inhomogeneous) random walks. If the sequence X = (Xn)n∈N0ℕ_0 consists of symmetric random variables taking the values +1 and −1, then X is bounded, but the martingale M and the predictable process A are unbounded simple random walks (and not uniformly integrable), and Doob's optional stopping theorem might not be applicable to the martingale M unless the stopping time has a finite expectation.

Application[edit]
In mathematical finance, the Doob decomposition theorem can be used to determine the largest optimal exercise time of an American option.[6][7] Let X = (X0, X1, . . . , XN) denote the non-negative, discounted payoffs of an American option in a N-period financial market model, adapted to a filtration 
(F0, F1, . . . , FN), and let Qℚ denote an equivalent martingale measure. Let U = (U0, U1, . . . , UN) denote the Snell envelope of X with respect to Qℚ. The Snell envelope is the smallest Qℚ-supermartingale dominating X[8] and in a complete financial market it represents the minimal amount of capital necessary to hedge the American option up to maturity.[9] Let U = M + A denote the Doob decomposition with respect to Qℚ of the Snell envelope U into a martingale M = (M0, M1, . . . , MN) and a decreasing predictable process A = (A0, A1, . . . , AN) with A0 = 0. Then the largest stopping time to exercise the American option in an optimal way[10][11] is

τmax:=NifAN=0,minn∈0,…,N−1∣An+1<0ifAN<0.τ_max:=N   ifA_N=0,
min{n∈{0,…,N-1}_n+1<0}   ifA_N<0.
Since A is predictable, the event {τmax = n} = {An = 0, An+1 < 0} is in Fn for every n ∈ {0, 1, . . . , N − 1}, hence τmax is indeed a stopping time. It gives the last moment before the discounted value of the American option will drop in expectation; up to time τmax the discounted value process U is a martingale with respect to Qℚ.

Generalization[edit]
The Doob decomposition theorem can be generalized from probability spaces to σ-finite measure spaces.[12]

Citations[edit]


^ Doob (1953), see (Doob 1990, pp. 296−298)

^ Durrett (2010)

^ (Föllmer & Schied 2011, Proposition 6.1)

^ (Williams 1991, Section 12.11, part (a) of the Theorem)

^ (Williams 1991, Section 12.11, part (b) of the Theorem)

^ (Lamberton & Lapeyre 2008, Chapter 2: Optimal stopping problem and American options)

^ (Föllmer & Schied 2011, Chapter 6: American contingent claims)

^ (Föllmer & Schied 2011, Proposition 6.10)

^ (Föllmer & Schied 2011, Theorem 6.11)

^ (Lamberton & Lapeyre 2008, Proposition 2.3.2)

^ (Föllmer & Schied 2011, Theorem 6.21)

^ (Schilling 2005, Problem 23.11)


References[edit]
Doob, Joseph L. (1953), Stochastic Processes, New York: Wiley, ISBN 978-0-471-21813-5, MR 0058896, Zbl 0053.26802
Doob, Joseph L. (1990), Stochastic Processes (Wiley Classics Library ed.), New York: John Wiley & Sons, Inc., ISBN 0-471-52369-0, MR 1038526, Zbl 0696.60003
Durrett, Rick (2010), Probability: Theory and Examples, Cambridge Series in Statistical and Probabilistic Mathematics (4. ed.), Cambridge University Press, ISBN 978-0-521-76539-8, MR 2722836, Zbl 1202.60001
Föllmer, Hans; Schied, Alexander (2011), Stochastic Finance: An Introduction in Discrete Time, De Gruyter graduate (3. rev. and extend ed.), Berlin, New York: De Gruyter, ISBN 978-3-11-021804-6, MR 2779313, Zbl 1213.91006
Lamberton, Damien; Lapeyre, Bernard (2008), Introduction to Stochastic Calculus Applied to Finance, Chapman & Hall/CRC financial mathematics series (2. ed.), Boca Raton, FL: Chapman & Hall/CRC, ISBN 978-1-58488-626-6, MR 2362458, Zbl 1167.60001
Schilling, René L. (2005), Measures, Integrals and Martingales, Cambridge: Cambridge University Press, ISBN 978-0-52185-015-5, MR 2200059, Zbl 1084.28001
Williams, David (1991), Probability with Martingales, Cambridge University Press, ISBN 0-521-40605-6, MR 1155402, Zbl 0722.60001
vteStochastic processesDiscrete time
Bernoulli process
Branching process
Chinese restaurant process
Galton–Watson process
Independent and identically distributed random variables
Markov chain
Moran process
Random walk
Loop-erased
Self-avoiding
 Biased
Maximal entropy
Continuous time
Additive process
Bessel process
Birth–death process
pure birth
Brownian motion
Bridge
Excursion
Fractional
Geometric
Meander
Cauchy process
Contact process
Continuous-time random walk
Cox process
Diffusion process
Empirical process
Feller process
Fleming–Viot process
Gamma process
Geometric process
Hawkes process
Hunt process
Interacting particle systems
Itô diffusion
Itô process
Jump diffusion
Jump process
Lévy process
Local time
Markov additive process
McKean–Vlasov process
Ornstein–Uhlenbeck process
Poisson process
Compound
Non-homogeneous
Schramm–Loewner evolution
Semimartingale
Sigma-martingale
Stable process
Superprocess
Telegraph process
Variance gamma process
Wiener process
Wiener sausage
Both
Branching process
Galves–Löcherbach model
Gaussian process
Hidden Markov model (HMM)
Markov process
Martingale
Differences
Local
Sub-
Super-
Random dynamical system
Regenerative process
Renewal process
Stochastic chains with memory of variable length
White noise
Fields and other
Dirichlet process
Gaussian random field
Gibbs measure
Hopfield model
Ising model
Potts model
Boolean network
Markov random field
Percolation
Pitman–Yor process
Point process
Cox
Poisson
Random field
Random graph
Time series models
Autoregressive conditional heteroskedasticity (ARCH) model
Autoregressive integrated moving average (ARIMA) model
Autoregressive (AR) model
Autoregressive–moving-average (ARMA) model
Generalized autoregressive conditional heteroskedasticity (GARCH) model
Moving-average (MA) model
Financial models
Binomial options pricing model
Black–Derman–Toy
Black–Karasinski
Black–Scholes
Chan–Karolyi–Longstaff–Sanders (CKLS)
Chen
Constant elasticity of variance (CEV)
Cox–Ingersoll–Ross (CIR)
Garman–Kohlhagen
Heath–Jarrow–Morton (HJM)
Heston
Ho–Lee
Hull–White
LIBOR market
Rendleman–Bartter
SABR volatility
Vašíček
Wilkie
Actuarial models
Bühlmann
Cramér–Lundberg
Risk process
Sparre–Anderson
Queueing models
Bulk
Fluid
Generalized queueing network
M/G/1
M/M/1
M/M/c
Properties
Càdlàg paths
Continuous
Continuous paths
Ergodic
Exchangeable
Feller-continuous
Gauss–Markov
Markov
Mixing
Piecewise-deterministic
Predictable
Progressively measurable
Self-similar
Stationary
Time-reversible
Limit theorems
Central limit theorem
Donsker's theorem
Doob's martingale convergence theorems
Ergodic theorem
Fisher–Tippett–Gnedenko theorem
Large deviation principle
Law of large numbers (weak/strong)
Law of the iterated logarithm
Maximal ergodic theorem
Sanov's theorem
Zero–one laws (Blumenthal, Borel–Cantelli, Engelbert–Schmidt, Hewitt–Savage,  Kolmogorov, Lévy)
Inequalities
Burkholder–Davis–Gundy
Doob's martingale
Doob's upcrossing
Kunita–Watanabe
Marcinkiewicz–Zygmund
Tools
Cameron–Martin formula
Convergence of random variables
Doléans-Dade exponential
Doob decomposition theorem
Doob–Meyer decomposition theorem
Doob's optional stopping theorem
Dynkin's formula
Feynman–Kac formula
Filtration
Girsanov theorem
Infinitesimal generator
Itô integral
Itô's lemma
Karhunen–Loève theorem
Kolmogorov continuity theorem
Kolmogorov extension theorem
Lévy–Prokhorov metric
Malliavin calculus
Martingale representation theorem
Optional stopping theorem
Prokhorov's theorem
Quadratic variation
Reflection principle
Skorokhod integral
Skorokhod's representation theorem
Skorokhod space
Snell envelope
Stochastic differential equation
Tanaka
Stopping time
Stratonovich integral
Uniform integrability
Usual hypotheses
Wiener space
Classical
Abstract
Disciplines
Actuarial mathematics
Control theory
Econometrics
Ergodic theory
Extreme value theory (EVT)
Large deviations theory
Mathematical finance
Mathematical statistics
Probability theory
Queueing theory
Renewal theory
Ruin theory
Signal processing
Statistics
Stochastic analysis
Time series analysis
Machine learning

List of topics
Category




