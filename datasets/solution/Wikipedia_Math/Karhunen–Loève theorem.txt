Theory of stochastic processes
In the theory of stochastic processes, the Karhunenâ€“LoÃ¨ve theorem  (named after Kari Karhunen and Michel LoÃ¨ve), also known as the Kosambiâ€“Karhunenâ€“LoÃ¨ve theorem[1][2] states that a stochastic process can be represented as an infinite linear combination of orthogonal functions, analogous to a Fourier series representation of a function on a bounded interval. The transformation is also known as Hotelling transform and eigenvector transform, and is closely related to principal component analysis (PCA) technique widely used in image processing and in data analysis in many fields.[3]
Stochastic processes given by infinite series of this form were first considered by Damodar Dharmananda Kosambi.[4][5]  There exist many such expansions of a stochastic process: if the process is indexed over [a, b], any orthonormal basis of L2([a, b]) yields an expansion thereof in that form. The importance of the Karhunenâ€“LoÃ¨ve theorem is that it yields the best such basis in the sense that it minimizes the total mean squared error.
In contrast to a Fourier series where the coefficients are fixed numbers and the expansion basis consists of sinusoidal functions (that is, sine and cosine functions), the coefficients in the Karhunenâ€“LoÃ¨ve theorem are random variables and the expansion basis depends on the process. In fact, the orthogonal basis functions used in this representation are determined by the covariance function of the process. One can think that the Karhunenâ€“LoÃ¨ve transform adapts to the process in order to produce the best possible basis for its expansion.
In the case of a centered stochastic process {Xt}t âˆˆ [a, b] (centered means E[Xt] = 0 for all t âˆˆ [a, b]) satisfying a technical continuity condition, X admits a decomposition

Xt=âˆ‘k=1âˆZkek(t)_t=âˆ‘_k=1^âˆZ_ke_k(t)
where Zk are pairwise uncorrelated random variables and the functions ek are continuous real-valued functions on [a, b] that are pairwise orthogonal in L2([a, b]). It is therefore sometimes said that the expansion is bi-orthogonal since the random coefficients Zk are orthogonal in the probability space while the deterministic functions ek are orthogonal in the time domain. The general case of a process Xt that is not centered can be brought back to the case of a centered process by considering Xt âˆ’ E[Xt] which is a centered process.
Moreover, if the process is Gaussian, then the random variables Zk are Gaussian and stochastically independent. This result generalizes the Karhunenâ€“LoÃ¨ve transform. An important example of a centered real stochastic process on [0, 1] is the Wiener process; the Karhunenâ€“LoÃ¨ve theorem can be used to provide a canonical orthogonal representation for it. In this case the expansion consists of sinusoidal functions.
The above expansion into uncorrelated random variables is also known as the Karhunenâ€“LoÃ¨ve expansion or Karhunenâ€“LoÃ¨ve decomposition. The empirical version (i.e., with the coefficients computed from a sample) is known as the Karhunenâ€“LoÃ¨ve transform (KLT), principal component analysis, proper orthogonal decomposition (POD), empirical orthogonal functions (a term used in meteorology and geophysics), or the Hotelling transform.


Formulation[edit]
Throughout this article, we will consider a random process Xt defined over a probability space (Î©, F, P) and indexed over a closed interval [a, b], which is square-integrable, is zero-mean, and with covariance function KX(s, t). In other words, we have:
âˆ€tâˆˆ[a,b]XtâˆˆL2(Î©,F,P),i.e.E[Xt2]<âˆ,âˆˆ[a,b]_t^2(Î©,F,ğ),  i.e.ğ„[X_t^2]<âˆ,
âˆ€tâˆˆ[a,b]E[Xt]=0,âˆˆ[a,b]    ğ„[X_t]=0,
âˆ€t,sâˆˆ[a,b]KX(s,t)=E[XsXt].,sâˆˆ[a,b]_X(s,t)=ğ„[X_sX_t].
The square-integrable condition E[Xt2]<âˆğ„[X_t^2]<âˆ is logically equivalent to KX(s,t)_X(s,t) being finite for all s,tâˆˆ[a,b],tâˆˆ[a,b].[6]

We associate to KX a linear operator (more specifically a Hilbertâ€“Schmidt integral operator) TKX defined in the following way:
TKX:L2([a,b])â†’L2([a,b]):fâ†¦TKXf=âˆ«abKX(s,â‹…)f(s)ds   T_K_X   :L^2([a,b])   ^2([a,b])
      :f_K_Xf   =âˆ«_a^bK_X(s,Â·)f(s) ds
Since TKX is a linear operator, it makes sense to talk about its eigenvalues Î»k and eigenfunctions ek, which are found solving the homogeneous Fredholm integral equation of the second kind
âˆ«abKX(s,t)ek(s)ds=Î»kek(t)âˆ«_a^bK_X(s,t)e_k(s) ds=Î»_ke_k(t)
Statement of the theorem[edit]
Theorem. Let Xt be a zero-mean square-integrable stochastic process defined over a probability space (Î©, F, P) and indexed over a closed and bounded interval [a,Â b], with continuous covariance function KX(s, t).
Then KX(s,t) is a Mercer kernel and letting ek be an orthonormal basis on L2([a, b]) formed by the eigenfunctions of TKX with respective eigenvalues Î»k, Xt admits the following representation

Xt=âˆ‘k=1âˆZkek(t)_t=âˆ‘_k=1^âˆZ_ke_k(t)
where the convergence is in L2, uniform in t and

Zk=âˆ«abXtek(t)dt_k=âˆ«_a^bX_te_k(t) dt
Furthermore, the random variables Zk have zero-mean, are uncorrelated and have variance Î»k

E[Zk]=0,âˆ€kâˆˆNandE[ZiZj]=Î´ijÎ»j,âˆ€i,jâˆˆNğ„[Z_k]=0,Â âˆˆâ„•        ğ„[Z_iZ_j]=Î´_ijÎ»_j,Â ,jâˆˆâ„•
Note that by generalizations of Mercer's theorem we can replace the interval [a, b] with other compact spaces C and the Lebesgue measure on [a, b] with a Borel measure whose support is C.

Proof[edit]
The covariance function KX satisfies the definition of a Mercer kernel. By Mercer's theorem, there consequently exists a set Î»k, ek(t) of eigenvalues and eigenfunctions of TKX forming an orthonormal basis of L2([a,b]), and KX can be expressed as
KX(s,t)=âˆ‘k=1âˆÎ»kek(s)ek(t)_X(s,t)=âˆ‘_k=1^âˆÎ»_ke_k(s)e_k(t)
The process Xt can be expanded in terms of the eigenfunctions ek as:
Xt=âˆ‘k=1âˆZkek(t)_t=âˆ‘_k=1^âˆZ_ke_k(t)
where the coefficients (random variables) Zk are given by the projection of Xt on the respective eigenfunctions
Zk=âˆ«abXtek(t)dt_k=âˆ«_a^bX_te_k(t) dt
We may then derive
E[Zk]=E[âˆ«abXtek(t)dt]=âˆ«abE[Xt]ek(t)dt=0E[ZiZj]=E[âˆ«abâˆ«abXtXsej(t)ei(s)dtds]=âˆ«abâˆ«abE[XtXs]ej(t)ei(s)dtds=âˆ«abâˆ«abKX(s,t)ej(t)ei(s)dtds=âˆ«abei(s)(âˆ«abKX(s,t)ej(t)dt)ds=Î»jâˆ«abei(s)ej(s)ds=Î´ijÎ»jğ„[Z_k]   =ğ„[âˆ«_a^bX_te_k(t) dt]=âˆ«_a^bğ„[X_t]e_k(t)dt=0
ğ„[Z_iZ_j]   =ğ„[âˆ«_a^bâˆ«_a^bX_tX_se_j(t)e_i(s) dt ds]
   =âˆ«_a^bâˆ«_a^bğ„[X_tX_s]e_j(t)e_i(s) dt ds
   =âˆ«_a^bâˆ«_a^bK_X(s,t)e_j(t)e_i(s) dt ds
   =âˆ«_a^be_i(s)(âˆ«_a^bK_X(s,t)e_j(t) dt) ds
   =Î»_jâˆ«_a^be_i(s)e_j(s) ds
   =Î´_ijÎ»_j
where we have used the fact that the ek are eigenfunctions of TKX and are orthonormal.
Let us now show that the convergence is in L2. Let
SN=âˆ‘k=1NZkek(t)._N=âˆ‘_k=1^NZ_ke_k(t).
Then:
E[|Xtâˆ’SN|2]=E[Xt2]+E[SN2]âˆ’2E[XtSN]=KX(t,t)+E[âˆ‘k=1Nâˆ‘l=1NZkZâ„“ek(t)eâ„“(t)]âˆ’2E[Xtâˆ‘k=1NZkek(t)]=KX(t,t)+âˆ‘k=1NÎ»kek(t)2âˆ’2E[âˆ‘k=1Nâˆ«abXtXsek(s)ek(t)ds]=KX(t,t)âˆ’âˆ‘k=1NÎ»kek(t)2ğ„[|X_t-S_N|^2]   =ğ„[X_t^2]+ğ„[S_N^2]-2ğ„[X_tS_N]
   =K_X(t,t)+ğ„[âˆ‘_k=1^Nâˆ‘_l=1^NZ_kZ_â„“e_k(t)e_â„“(t)]-2ğ„[X_tâˆ‘_k=1^NZ_ke_k(t)]
   =K_X(t,t)+âˆ‘_k=1^NÎ»_ke_k(t)^2-2ğ„[âˆ‘_k=1^Nâˆ«_a^bX_tX_se_k(s)e_k(t) ds]
   =K_X(t,t)-âˆ‘_k=1^NÎ»_ke_k(t)^2
which goes to 0 by Mercer's theorem.
Properties of the Karhunenâ€“LoÃ¨ve transform[edit]
Special case: Gaussian distribution[edit]
Since the limit in the mean of jointly Gaussian random variables is jointly Gaussian, and jointly Gaussian random (centered) variables are independent if and only if they are orthogonal, we can also conclude:
Theorem.  The variables Zi have a joint Gaussian distribution and are stochastically independent if the original process {Xt}t is Gaussian.
In the Gaussian case, since the variables Zi are independent, we can say more:

limNâ†’âˆâˆ‘i=1Nei(t)Zi(Ï‰)=Xt(Ï‰)lim_Nâ†’âˆâˆ‘_i=1^Ne_i(t)Z_i(Ï‰)=X_t(Ï‰)
almost surely.

The Karhunenâ€“LoÃ¨ve transform decorrelates the process[edit]
This is a consequence of the independence of the Zk.

The Karhunenâ€“LoÃ¨ve expansion minimizes the total mean square error[edit]
In the introduction, we mentioned that the truncated Karhunenâ€“Loeve expansion was the best approximation of the original process in the sense that it reduces the total mean-square error resulting of its truncation. Because of this property, it is often said that the KL transform optimally compacts the energy.
More specifically, given any orthonormal basis {fk} of L2([a, b]), we may decompose the process Xt as:

Xt(Ï‰)=âˆ‘k=1âˆAk(Ï‰)fk(t)_t(Ï‰)=âˆ‘_k=1^âˆA_k(Ï‰)f_k(t)
where

Ak(Ï‰)=âˆ«abXt(Ï‰)fk(t)dt_k(Ï‰)=âˆ«_a^bX_t(Ï‰)f_k(t) dt
and we may approximate Xt by the finite sum

X^t(Ï‰)=âˆ‘k=1NAk(Ï‰)fk(t)XÌ‚_t(Ï‰)=âˆ‘_k=1^NA_k(Ï‰)f_k(t)
for some integer N.
Claim. Of all such approximations, the KL approximation is the one that minimizes the total mean square error (provided we have arranged the eigenvalues in decreasing order).

Proof
Consider the error resulting from the truncation at the N-th term in the following orthonormal expansion:

ÎµN(t)=âˆ‘k=N+1âˆAk(Ï‰)fk(t)Îµ_N(t)=âˆ‘_k=N+1^âˆA_k(Ï‰)f_k(t)
The mean-square error ÎµN2(t) can be written as:

ÎµN2(t)=E[âˆ‘i=N+1âˆâˆ‘j=N+1âˆAi(Ï‰)Aj(Ï‰)fi(t)fj(t)]=âˆ‘i=N+1âˆâˆ‘j=N+1âˆE[âˆ«abâˆ«abXtXsfi(t)fj(s)dsdt]fi(t)fj(t)=âˆ‘i=N+1âˆâˆ‘j=N+1âˆfi(t)fj(t)âˆ«abâˆ«abKX(s,t)fi(t)fj(s)dsdtÎµ_N^2(t)   =ğ„[âˆ‘_i=N+1^âˆâˆ‘_j=N+1^âˆA_i(Ï‰)A_j(Ï‰)f_i(t)f_j(t)]
   =âˆ‘_i=N+1^âˆâˆ‘_j=N+1^âˆğ„[âˆ«_a^bâˆ«_a^bX_tX_sf_i(t)f_j(s) ds dt]f_i(t)f_j(t)
   =âˆ‘_i=N+1^âˆâˆ‘_j=N+1^âˆf_i(t)f_j(t)âˆ«_a^bâˆ«_a^bK_X(s,t)f_i(t)f_j(s) ds dt
We then integrate this last equality over [a, b]. The orthonormality of the fk yields:

âˆ«abÎµN2(t)dt=âˆ‘k=N+1âˆâˆ«abâˆ«abKX(s,t)fk(t)fk(s)dsdtâˆ«_a^bÎµ_N^2(t) dt=âˆ‘_k=N+1^âˆâˆ«_a^bâˆ«_a^bK_X(s,t)f_k(t)f_k(s) ds dt
The problem of minimizing the total mean-square error thus comes down to minimizing the right hand side of this equality subject to the constraint that the fk be normalized. We hence introduce Î²k, the Lagrangian multipliers associated with these constraints, and aim at minimizing the following function:

Er[fk(t),kâˆˆN+1,â€¦]=âˆ‘k=N+1âˆâˆ«abâˆ«abKX(s,t)fk(t)fk(s)dsdtâˆ’Î²k(âˆ«abfk(t)fk(t)dtâˆ’1)[f_k(t),kâˆˆ{N+1,â€¦}]=âˆ‘_k=N+1^âˆâˆ«_a^bâˆ«_a^bK_X(s,t)f_k(t)f_k(s) ds dt-Î²_k(âˆ«_a^bf_k(t)f_k(t) dt-1)
Differentiating with respect to fi(t) (this is a functional derivative) and setting the derivative to 0 yields:

âˆ‚Erâˆ‚fi(t)=âˆ«ab(âˆ«abKX(s,t)fi(s)dsâˆ’Î²ifi(t))dt=0/_i(t)=âˆ«_a^b(âˆ«_a^bK_X(s,t)f_i(s) ds-Î²_if_i(t)) dt=0
which is satisfied in particular when

âˆ«abKX(s,t)fi(s)ds=Î²ifi(t).âˆ«_a^bK_X(s,t)f_i(s) ds=Î²_if_i(t).
In other words, when the fk are chosen to be the eigenfunctions of TKX, hence resulting in the KL expansion.


Explained variance[edit]
An important observation is that since the random coefficients Zk of the KL expansion are uncorrelated, the BienaymÃ© formula asserts that the variance of Xt is simply the sum of the variances of the individual components of the sum:

varâ¡[Xt]=âˆ‘k=0âˆek(t)2varâ¡[Zk]=âˆ‘k=1âˆÎ»kek(t)2var[X_t]=âˆ‘_k=0^âˆe_k(t)^2var[Z_k]=âˆ‘_k=1^âˆÎ»_ke_k(t)^2
Integrating over [a, b] and using the orthonormality of the ek, we obtain that the total variance of the process is:

âˆ«abvarâ¡[Xt]dt=âˆ‘k=1âˆÎ»kâˆ«_a^bvar[X_t] dt=âˆ‘_k=1^âˆÎ»_k
In particular, the total variance of the N-truncated approximation is

âˆ‘k=1NÎ»k.âˆ‘_k=1^NÎ»_k.
As a result, the N-truncated expansion explains

âˆ‘k=1NÎ»kâˆ‘k=1âˆÎ»kâˆ‘_k=1^NÎ»_k/âˆ‘_k=1^âˆÎ»_k
of the variance; and if we are content with an approximation that explains, say, 95% of the variance, then we just have to determine an NâˆˆNâˆˆâ„• such that

âˆ‘k=1NÎ»kâˆ‘k=1âˆÎ»kâ‰¥0.95.âˆ‘_k=1^NÎ»_k/âˆ‘_k=1^âˆÎ»_kâ‰¥0.95.
The Karhunenâ€“LoÃ¨ve expansion has the minimum representation entropy property[edit]
Given a representation of Xt=âˆ‘k=1âˆWkÏ†k(t)_t=âˆ‘_k=1^âˆW_kÏ†_k(t), for some orthonormal basis Ï†k(t)Ï†_k(t) and random Wk_k, we let pk=E[|Wk|2]/E[|Xt|L22]_k=ğ”¼[|W_k|^2]/ğ”¼[|X_t|_L^2^2], so that âˆ‘k=1âˆpk=1âˆ‘_k=1^âˆp_k=1. We may then define the representation entropy to be H(Ï†k)=âˆ’âˆ‘ipklogâ¡(pk)({Ï†_k})=-âˆ‘_ip_klog(p_k). Then we have H(Ï†k)â‰¥H(ek)({Ï†_k})({e_k}), for all choices of Ï†kÏ†_k. That is, the KL-expansion has minimal representation entropy.
Proof:
Denote the coefficients obtained for the basis ek(t)_k(t) as pk_k, and for Ï†k(t)Ï†_k(t) as qk_k.
Choose Nâ‰¥1â‰¥1. Note that since ek_k minimizes the mean squared error, we have that

E|âˆ‘k=1NZkek(t)âˆ’Xt|L22â‰¤E|âˆ‘k=1NWkÏ†k(t)âˆ’Xt|L22ğ”¼|âˆ‘_k=1^NZ_ke_k(t)-X_t|_L^2^2â‰¤ğ”¼|âˆ‘_k=1^NW_kÏ†_k(t)-X_t|_L^2^2
Expanding the right hand size, we get:

E|âˆ‘k=1NWkÏ†k(t)âˆ’Xt|L22=E|Xt2|L2+âˆ‘k=1Nâˆ‘â„“=1NE[Wâ„“Ï†â„“(t)Wkâˆ—Ï†kâˆ—(t)]L2âˆ’âˆ‘k=1NE[WkÏ†kXtâˆ—]L2âˆ’âˆ‘k=1NE[XtWkâˆ—Ï†kâˆ—(t)]L2ğ”¼|âˆ‘_k=1^NW_kÏ†_k(t)-X_t|_L^2^2=ğ”¼|X_t^2|_L^2+âˆ‘_k=1^Nâˆ‘_â„“=1^Nğ”¼[W_â„“Ï†_â„“(t)W_k^*Ï†_k^*(t)]_L^2-âˆ‘_k=1^Nğ”¼[W_kÏ†_kX_t^*]_L^2-âˆ‘_k=1^Nğ”¼[X_tW_k^*Ï†_k^*(t)]_L^2
Using the orthonormality of Ï†k(t)Ï†_k(t), and expanding Xt_t in the Ï†k(t)Ï†_k(t) basis, we get that the right hand size is equal to:

E[Xt]L22âˆ’âˆ‘k=1NE[|Wk|2]ğ”¼[X_t]_L^2^2-âˆ‘_k=1^Nğ”¼[|W_k|^2]
We may perform identical analysis for the ek(t)_k(t), and so rewrite the above inequality as:

E[Xt]L22âˆ’âˆ‘k=1NE[|Zk|2]â‰¤E[Xt]L22âˆ’âˆ‘k=1NE[|Wk|2]ğ”¼[X_t]_L^2^2-âˆ‘_k=1^Nğ”¼[|Z_k|^2]â‰¤ğ”¼[X_t]_L^2^2-âˆ‘_k=1^Nğ”¼[|W_k|^2]
Subtracting the common first term, and dividing by E[|Xt|L22]ğ”¼[|X_t|_L^2^2], we obtain that:

âˆ‘k=1Npkâ‰¥âˆ‘k=1Nqkâˆ‘_k=1^Np_kâ‰¥âˆ‘_k=1^Nq_k
This implies that:

âˆ’âˆ‘k=1âˆpklogâ¡(pk)â‰¤âˆ’âˆ‘k=1âˆqklogâ¡(qk)-âˆ‘_k=1^âˆp_klog(p_k)â‰¤-âˆ‘_k=1^âˆq_klog(q_k)
Linear Karhunenâ€“LoÃ¨ve approximations[edit]
Consider a whole class of signals we want to approximate over the first M vectors of a basis. These signals are modeled as realizations of a random vector Y[n] of size N. To optimize the approximation we design a basis that minimizes the average approximation error. This section proves that optimal bases are Karhunenâ€“Loeve bases that diagonalize the covariance matrix of Y. The random vector Y can be decomposed in an orthogonal basis

gm0â‰¤mâ‰¤N{g_m}_0
as follows:

Y=âˆ‘m=0Nâˆ’1âŸ¨Y,gmâŸ©gm,=âˆ‘_m=0^N-1,g_m_m,
where each

âŸ¨Y,gmâŸ©=âˆ‘n=0Nâˆ’1Y[n]gmâˆ—[n],g_mâŸ©=âˆ‘_n=0^N-1Y[n]g_m^*[n]
is a random variable. The approximation from the first M â‰¤ N vectors of the basis is

YM=âˆ‘m=0Mâˆ’1âŸ¨Y,gmâŸ©gm_M=âˆ‘_m=0^M-1,g_m_m
The energy conservation in an orthogonal basis implies

Îµ[M]=Eâ€–Yâˆ’YMâ€–2=âˆ‘m=MNâˆ’1E|âŸ¨Y,gmâŸ©|2Îµ[M]=ğ„{Y-Y_M^2}=âˆ‘_m=M^N-1ğ„{|,g_mâŸ©|^2}
This error is related to the covariance of Y defined by

R[n,m]=EY[n]Yâˆ—[m][n,m]=ğ„{Y[n]Y^*[m]}
For any vector x[n] we denote by K the covariance operator represented by this matrix,

E|âŸ¨Y,xâŸ©|2=âŸ¨Kx,xâŸ©=âˆ‘n=0Nâˆ’1âˆ‘m=0Nâˆ’1R[n,m]x[n]xâˆ—[m]ğ„{|,xâŸ©|^2}=,xâŸ©=âˆ‘_n=0^N-1âˆ‘_m=0^N-1R[n,m]x[n]x^*[m]
The error Îµ[M] is therefore a sum of the last N âˆ’ M coefficients of the covariance operator

Îµ[M]=âˆ‘m=MNâˆ’1âŸ¨Kgm,gmâŸ©Îµ[M]=âˆ‘_m=M^N-1_m,g_mâŸ©
The covariance operator K is Hermitian and Positive and is thus diagonalized in an orthogonal basis called a Karhunenâ€“LoÃ¨ve basis. The following theorem states that a Karhunenâ€“LoÃ¨ve basis is optimal for linear approximations.
Theorem (Optimality of Karhunenâ€“LoÃ¨ve basis). Let K be a covariance operator. For all M â‰¥ 1, the approximation error

Îµ[M]=âˆ‘m=MNâˆ’1âŸ¨Kgm,gmâŸ©Îµ[M]=âˆ‘_m=M^N-1_m,g_mâŸ©
is minimum if and only if

gm0â‰¤m<N{g_m}_0<N
is a Karhunenâ€“Loeve basis ordered by decreasing eigenvalues.

âŸ¨Kgm,gmâŸ©â‰¥âŸ¨Kgm+1,gm+1âŸ©,0â‰¤m<Nâˆ’1._m,g_mâŸ©â‰¥_m+1,g_m+1âŸ©,    0<N-1.
Non-Linear approximation in bases[edit]
Linear approximations project the signal on M vectors a priori. The approximation can be made more precise by choosing the M orthogonal vectors depending on the signal properties. This section analyzes the general performance of these non-linear approximations. A signal fâˆˆHâˆˆH is approximated with M vectors selected adaptively in an orthonormal basis for HH[definition needed]

B=gmmâˆˆNB={g_m}_mâˆˆâ„•
Let fM_M be the projection of f over M vectors whose indices are in IM:

fM=âˆ‘mâˆˆIMâŸ¨f,gmâŸ©gm_M=âˆ‘_m_M,g_m_m
The approximation error is the sum of the remaining coefficients

Îµ[M]=â€–fâˆ’fMâ€–2=âˆ‘mâˆ‰IMNâˆ’1|âŸ¨f,gmâŸ©|2Îµ[M]={f-f_M^2}=âˆ‘_m_M^N-1{|,g_mâŸ©|^2}
To minimize this error, the indices in IM must correspond to the M vectors having the largest inner product amplitude

|âŸ¨f,gmâŸ©|.|,g_mâŸ©|.
These are the vectors that best correlate f. They can thus be interpreted as the main features of f. The resulting error is necessarily smaller than the error of a linear approximation which selects the M approximation vectors independently of f. Let us sort

|âŸ¨f,gmâŸ©|mâˆˆN{|,g_mâŸ©|}_mâˆˆâ„•
in decreasing order

|âŸ¨f,gmkâŸ©|â‰¥|âŸ¨f,gmk+1âŸ©|.|,g_m_kâŸ©|â‰¥|,g_m_k+1âŸ©|.
The best non-linear approximation is

fM=âˆ‘k=1MâŸ¨f,gmkâŸ©gmk_M=âˆ‘_k=1^M,g_m_k_m_k
It can also be written as inner product thresholding:

fM=âˆ‘m=0âˆÎ¸T(âŸ¨f,gmâŸ©)gm_M=âˆ‘_m=0^âˆÎ¸_T(,g_mâŸ©)g_m
with

T=|âŸ¨f,gmMâŸ©|,Î¸T(x)=x|x|â‰¥T0|x|<T=|,g_m_MâŸ©|,    Î¸_T(x)=x   |x|
0   |x|<T
The non-linear error is

Îµ[M]=â€–fâˆ’fMâ€–2=âˆ‘k=M+1âˆ|âŸ¨f,gmkâŸ©|2Îµ[M]={f-f_M^2}=âˆ‘_k=M+1^âˆ{|,g_m_kâŸ©|^2}
this error goes quickly to zero as M increases, if the sorted values of |âŸ¨f,gmkâŸ©||,g_m_kâŸ©| have a fast decay as k increases. This decay is quantified by computing the IPI^P norm of the signal inner products in B:

â€–fâ€–B,p=(âˆ‘m=0âˆ|âŸ¨f,gmâŸ©|p)1pf_B,p=(âˆ‘_m=0^âˆ|,g_mâŸ©|^p)^1/p
The following theorem relates the decay of Îµ[M] to â€–fâ€–B,pf_B,p
Theorem (decay of error).  If â€–fâ€–B,p<âˆf_B,p<âˆ with p < 2 then

Îµ[M]â‰¤â€–fâ€–B,p22pâˆ’1M1âˆ’2pÎµ[M]â‰¤f_B,p^2/2/p-1M^1-2/p
and

Îµ[M]=o(M1âˆ’2p).Îµ[M]=o(M^1-2/p).
Conversely, if Îµ[M]=o(M1âˆ’2p)Îµ[M]=o(M^1-2/p) then
â€–fâ€–B,q<âˆf_B,q<âˆ for any q > p.

Non-optimality of Karhunenâ€“LoÃ¨ve bases[edit]
To further illustrate the differences between linear and non-linear approximations, we study the decomposition of a simple non-Gaussian random vector in a Karhunenâ€“LoÃ¨ve basis. Processes whose realizations have a random translation are stationary. The Karhunenâ€“LoÃ¨ve basis is then a Fourier basis and we study its performance. To simplify the analysis, consider a random vector Y[n] of size N that is random shift modulo N of a deterministic signal f[n] of zero mean

âˆ‘n=0Nâˆ’1f[n]=0âˆ‘_n=0^N-1f[n]=0
Y[n]=f[(nâˆ’p)modN][n]=f[(n-p)N]
The random shift P is uniformly distributed on [0,Â NÂ âˆ’Â 1]:

Pr(P=p)=1N,0â‰¤p<N(P=p)=1/N,    0<N
Clearly

EY[n]=1Nâˆ‘p=0Nâˆ’1f[(nâˆ’p)modN]=0ğ„{Y[n]}=1/Nâˆ‘_p=0^N-1f[(n-p)N]=0
and

R[n,k]=EY[n]Y[k]=1Nâˆ‘p=0Nâˆ’1f[(nâˆ’p)modN]f[(kâˆ’p)modN]=1NfÎ˜fÂ¯[nâˆ’k],fÂ¯[n]=f[âˆ’n][n,k]=ğ„{Y[n]Y[k]}=1/Nâˆ‘_p=0^N-1f[(n-p)N]f[(k-p)N]=1/NfÎ˜fÌ…[n-k],  fÌ…[n]=f[-n]
Hence

R[n,k]=RY[nâˆ’k],RY[k]=1NfÎ˜fÂ¯[k][n,k]=R_Y[n-k],_Y[k]=1/NfÎ˜fÌ…[k]
Since RY is N periodic, Y is a circular stationary random vector. The covariance operator is a circular convolution with RY and is therefore diagonalized in the discrete Fourier Karhunenâ€“LoÃ¨ve basis

1Nei2Ï€mn/N0â‰¤m<N.{1/âˆš(N)e^i2/N}_0<N.
The power spectrum is Fourier transform of RY:

PY[m]=R^Y[m]=1N|f^[m]|2_Y[m]=RÌ‚_Y[m]=1/N|fÌ‚[m]|^2
Example:  Consider an extreme case where f[n]=Î´[n]âˆ’Î´[nâˆ’1][n]=Î´[n]-Î´[n-1]. A theorem stated above guarantees that the Fourier Karhunenâ€“LoÃ¨ve basis produces a smaller expected approximation error than a canonical basis of Diracs gm[n]=Î´[nâˆ’m]0â‰¤m<N{g_m[n]=Î´[n-m]}_0<N. Indeed, we do not know a priori the abscissa of the non-zero coefficients of Y, so there is no particular Dirac that is better adapted to perform the approximation. But the Fourier vectors cover the whole support of Y and thus absorb a part of the signal energy.

E|âŸ¨Y[n],1Nei2Ï€mn/NâŸ©|2=PY[m]=4Nsin2â¡(Ï€kN)ğ„{|[n],1/âˆš(N)e^i2/NâŸ©|^2}=P_Y[m]=4/Nsin^2(/N)
Selecting higher frequency Fourier coefficients yields a better mean-square approximation than choosing a priori a few Dirac vectors to perform the approximation. The situation is totally different for non-linear approximations. If f[n]=Î´[n]âˆ’Î´[nâˆ’1][n]=Î´[n]-Î´[n-1] then the discrete Fourier basis is extremely inefficient because f and hence Y have an energy that is almost uniformly spread among all Fourier vectors. In contrast, since f has only two non-zero coefficients in the Dirac basis, a non-linear approximation of Y with M â‰¥ 2 gives zero error.[7]

Principal component analysis[edit]
Main article: Principal component analysis
We have established the Karhunenâ€“LoÃ¨ve theorem and derived a few properties thereof. We also noted that one hurdle in its application was the numerical cost of determining the eigenvalues and eigenfunctions of its covariance operator through the Fredholm integral equation of the second kind

âˆ«abKX(s,t)ek(s)ds=Î»kek(t).âˆ«_a^bK_X(s,t)e_k(s) ds=Î»_ke_k(t).
However, when applied to a discrete and finite process (Xn)nâˆˆ1,â€¦,N(X_n)_nâˆˆ{1,â€¦,N}, the problem takes a much simpler form and standard algebra can be used to carry out the calculations.
Note that a continuous process can also be sampled at N points in time in order to reduce the problem to a finite version.
We henceforth consider a random N-dimensional vector X=(X1X2â€¦XN)T=(X_1Â X_2Â â€¦Â X_N)^T. As mentioned above, X could contain N samples of a signal but it can hold many more representations depending on the field of application. For instance it could be the answers to a survey or economic data in an econometrics analysis.
As in the continuous version, we assume that X is centered, otherwise we can let X:=Xâˆ’Î¼X:=X-Î¼_X (where Î¼XÎ¼_X is the mean vector of X) which is centered.
Let us adapt the procedure to the discrete case.

Covariance matrix[edit]
Recall that the main implication and difficulty of the KL transformation is computing the eigenvectors of the linear operator associated to the covariance function, which are given by the solutions to the integral equation written above.
Define Î£, the covariance matrix of X, as an N Ã— N matrix whose elements are given by:

Î£ij=E[XiXj],âˆ€i,jâˆˆ1,â€¦,NÎ£_ij=ğ„[X_iX_j],    ,jâˆˆ{1,â€¦,N}
Rewriting the above integral equation to suit the discrete case, we observe that it turns into:

âˆ‘j=1NÎ£ijej=Î»eiâ‡”Î£e=Î»eâˆ‘_j=1^NÎ£_ije_j=_i  â‡”  =
where e=(e1e2â€¦eN)T=(e_1Â e_2Â â€¦Â e_N)^T is an N-dimensional vector.
The integral equation thus reduces to a simple matrix eigenvalue problem, which explains why the PCA has such a broad domain of applications.
Since Î£ is a positive definite symmetric matrix, it possesses a set of orthonormal eigenvectors forming a basis of RNâ„^N, and we write Î»i,Ï†iiâˆˆ1,â€¦,N{Î»_i,Ï†_i}_iâˆˆ{1,â€¦,N} this set of eigenvalues and corresponding eigenvectors, listed in decreasing values of Î»i. Let also Î¦ be the orthonormal matrix consisting of these eigenvectors:

Î¦:=(Ï†1Ï†2â€¦Ï†N)TÎ¦TÎ¦=IÎ¦   :=(Ï†_1Â Ï†_2Â â€¦Â Ï†_N)^T
Î¦^TÎ¦   =I
Principal component transform[edit]
It remains to perform the actual KL transformation, called the principal component transform in this case. Recall that the transform was found by expanding the process with respect to  the basis spanned by the eigenvectors of the covariance function. In this case, we hence have:

X=âˆ‘i=1NâŸ¨Ï†i,XâŸ©Ï†i=âˆ‘i=1NÏ†iTXÏ†i=âˆ‘_i=1^NâŸ¨Ï†_i,XâŸ©Ï†_i=âˆ‘_i=1^NÏ†_i^TXÏ†_i
In a more compact form, the principal component transform of X is defined by:

Y=Î¦TXX=Î¦YY=Î¦^TX
X=
The i-th component of Y is Yi=Ï†iTX_i=Ï†_i^TX, the projection of X on Ï†iÏ†_i and the inverse transform X = Î¦Y yields the expansion of X on the space spanned by the Ï†iÏ†_i:

X=âˆ‘i=1NYiÏ†i=âˆ‘i=1NâŸ¨Ï†i,XâŸ©Ï†i=âˆ‘_i=1^NY_iÏ†_i=âˆ‘_i=1^NâŸ¨Ï†_i,XâŸ©Ï†_i
As in the continuous case, we may reduce the dimensionality of the problem by truncating the sum at some Kâˆˆ1,â€¦,Nâˆˆ{1,â€¦,N} such that

âˆ‘i=1KÎ»iâˆ‘i=1NÎ»iâ‰¥Î±âˆ‘_i=1^KÎ»_i/âˆ‘_i=1^NÎ»_iâ‰¥Î±
where Î± is the explained variance threshold we wish to set.
We can also reduce the dimensionality through the use of multilevel dominant eigenvector estimation (MDEE).[8]

Examples[edit]
The Wiener process[edit]
There are numerous equivalent characterizations of the Wiener process which is a mathematical formalization of Brownian motion.  Here we regard it as the centered standard Gaussian process Wt with covariance function

KW(t,s)=covâ¡(Wt,Ws)=min(s,t)._W(t,s)=cov(W_t,W_s)=min(s,t).
We restrict the time domain to [a, b]=[0,1] without loss of generality.
The eigenvectors of the covariance kernel are easily determined.  These are

ek(t)=2sinâ¡((kâˆ’12)Ï€t)_k(t)=âˆš(2)sin((k-12))
and the corresponding eigenvalues are

Î»k=1(kâˆ’12)2Ï€2.Î»_k=1/(k-1/2)^2Ï€^2.
Proof
In order to find the eigenvalues and eigenvectors, we need to solve the integral equation:

âˆ«abKW(s,t)e(s)ds=Î»e(t)âˆ€t,0â‰¤tâ‰¤1âˆ«01min(s,t)e(s)ds=Î»e(t)âˆ€t,0â‰¤tâ‰¤1âˆ«0tse(s)ds+tâˆ«t1e(s)ds=Î»e(t)âˆ€t,0â‰¤tâ‰¤1âˆ«_a^bK_W(s,t)e(s) ds   =(t)    ,0â‰¤1
âˆ«_0^1min(s,t)e(s) ds   =(t)    ,0â‰¤1
âˆ«_0^tse(s) ds+tâˆ«_t^1e(s) ds   =(t)    ,0â‰¤1
differentiating once with respect to t yields:

âˆ«t1e(s)ds=Î»eâ€²(t)âˆ«_t^1e(s) ds='(t)
a second differentiation produces the following differential equation:

âˆ’e(t)=Î»eâ€³(t)-e(t)=â€(t)
The general solution of which has the form:

e(t)=Asinâ¡(tÎ»)+Bcosâ¡(tÎ»)(t)=Asin(t/âˆš(Î»))+Bcos(t/âˆš(Î»))
where A and B are two constants to be determined with the boundary conditions. Setting tÂ =Â 0 in the initial integral equation gives e(0)Â =Â 0 which implies that BÂ =Â 0 and similarly, setting tÂ =Â 1 in the first differentiation yields e' (1)Â =Â 0, whence:

cosâ¡(1Î»)=0cos(1/âˆš(Î»))=0
which in turn implies that eigenvalues of TKX are:

Î»k=(1(kâˆ’12)Ï€)2,kâ‰¥1Î»_k=(1/(k-1/2)Ï€)^2,â‰¥1
The corresponding eigenfunctions are thus of the form:

ek(t)=Asinâ¡((kâˆ’12)Ï€t),kâ‰¥1_k(t)=Asin((k-1/2)),â‰¥1
A is then chosen so as to normalize ek:

âˆ«01ek2(t)dt=1âŸ¹A=2âˆ«_0^1e_k^2(t) dt=1  =âˆš(2)

This gives the following representation of the Wiener process:
Theorem.  There is a sequence {Zi}i of independent Gaussian random variables with mean zero and variance 1 such that

Wt=2âˆ‘k=1âˆZksinâ¡((kâˆ’12)Ï€t)(kâˆ’12)Ï€._t=âˆš(2)âˆ‘_k=1^âˆZ_ksin((k-1/2))/(k-1/2)Ï€.
Note that this representation is only valid for tâˆˆ[0,1].âˆˆ[0,1].  On larger intervals, the increments are not independent.  As stated in the theorem, convergence is in the L2 norm and uniform inÂ t.

The Brownian bridge[edit]
Similarly the Brownian bridge Bt=Wtâˆ’tW1_t=W_t-tW_1 which is a stochastic process with covariance function

KB(t,s)=min(t,s)âˆ’ts_B(t,s)=min(t,s)-ts
can be represented as the series

Bt=âˆ‘k=1âˆZk2sinâ¡(kÏ€t)kÏ€_t=âˆ‘_k=1^âˆZ_kâˆš(2)sin(k)/kÏ€
Applications[edit]
This section needs expansion. You can help by adding to it.  (July 2010)
Adaptive optics systems sometimes use Kâ€“L functions to reconstruct wave-front phase information (Dai 1996, JOSA A).
Karhunenâ€“LoÃ¨ve expansion is closely related to the Singular Value Decomposition. The latter has myriad applications in image processing, radar, seismology, and the like. If one has independent vector observations from a vector valued stochastic process then the left singular vectors are maximum likelihood estimates of the ensemble KL expansion.

Applications in signal estimation and detection[edit]
Detection of a known continuous signal S(t)[edit]
In communication, we usually have to decide whether a signal from a noisy channel contains valuable information. The following hypothesis testing is used for detecting continuous signal s(t) from channel output X(t), N(t) is the channel noise, which is usually assumed zero mean Gaussian process with correlation function RN(t,s)=E[N(t)N(s)]_N(t,s)=E[N(t)N(s)]

H:X(t)=N(t),:X(t)=N(t),
K:X(t)=N(t)+s(t),tâˆˆ(0,T):X(t)=N(t)+s(t),âˆˆ(0,T)
Signal detection  in white noise[edit]
When the channel noise is white, its correlation function is

RN(t)=12N0Î´(t),_N(t)=12N_0Î´(t),
and it has constant power spectrum density. In physically practical channel, the noise power is finite, so:

SN(f)=N02|f|<w0|f|>w_N(f)=N_0/2   |f|<w
0   |f|>w
Then the noise correlation function is sinc function with zeros at n2Ï‰,nâˆˆZ.n/2Ï‰,nâˆˆğ™. Since are uncorrelated and gaussian, they are independent. Thus we can take samples from X(t) with time spacing

Î”t=n2Ï‰within(0,â€³Tâ€³).=n/2Ï‰within(0,â€Tâ€).
Let Xi=X(iÎ”t)_i=X(i ). We have a total of n=TÎ”t=T(2Ï‰)=2Ï‰T=T/=T(2Ï‰)=2 i.i.d observations X1,X2,â€¦,Xn{X_1,X_2,â€¦,X_n} to develop the likelihood-ratio test.  Define signal Si=S(iÎ”t)_i=S(i ), the problem becomes,

H:Xi=Ni,:X_i=N_i,
K:Xi=Ni+Si,i=1,2,â€¦,n.:X_i=N_i+S_i,i=1,2,â€¦,n.
The log-likelihood ratio

L(x_)=logâ¡âˆ‘i=1n(2Sixiâˆ’Si2)2Ïƒ2â‡”Î”tâˆ‘i=1nSixi=âˆ‘i=1nS(iÎ”t)x(iÎ”t)Î”tâ‰·Î»â‹…2â„’(x)=logâˆ‘_i=1^n(2S_ix_i-S_i^2)/2Ïƒ^2â‡”âˆ‘_i=1^nS_ix_i=âˆ‘_i=1^nS(i )x(i ) â‰·Î»_Â·2
As t â†’ 0, let:

G=âˆ«0TS(t)x(t)dt.=âˆ«_0^TS(t)x(t) dt.
Then G is the test statistics and the Neymanâ€“Pearson optimum detector is

G(x_)>G0â‡’K<G0â‡’H.(x)>G_0<G_0.
As G is Gaussian, we can characterize it by finding its mean and variances. Then we get

H:Gâˆ¼N(0,12N0E):G(0,12N_0E)
K:Gâˆ¼N(E,12N0E):G(E,12N_0E)
where

E=âˆ«0TS2(t)dtğ„=âˆ«_0^TS^2(t) dt
is the signal energy.
The false alarm error

Î±=âˆ«G0âˆN(0,12N0E)dGâ‡’G0=12N0EÎ¦âˆ’1(1âˆ’Î±)Î±=âˆ«_G_0^âˆN(0,12N_0E) dG_0=âˆš(12N_0E)Î¦^-1(1-Î±)
And the probability of detection:

Î²=âˆ«G0âˆN(E,12N0E)dG=1âˆ’Î¦(G0âˆ’E12N0E)=Î¦(2EN0âˆ’Î¦âˆ’1(1âˆ’Î±)),Î²=âˆ«_G_0^âˆN(E,12N_0E) dG=1-Î¦(G_0-E/âˆš(12N_0E))=Î¦(âˆš(2E/N_0)-Î¦^-1(1-Î±)),
where Î¦ is the cdf of standard normal, or Gaussian, variable.

Signal detection in colored noise[edit]
When N(t) is colored (correlated in time) Gaussian noise  with zero mean and covariance function RN(t,s)=E[N(t)N(s)],_N(t,s)=E[N(t)N(s)],  we cannot sample independent discrete observations by evenly spacing the time. Instead, we can use Kâ€“L expansion to decorrelate the noise process and get independent Gaussian observation 'samples'.  The Kâ€“L expansion of N(t):

N(t)=âˆ‘i=1âˆNiÎ¦i(t),0<t<T,(t)=âˆ‘_i=1^âˆN_iÎ¦_i(t),  0<t<T,
where Ni=âˆ«N(t)Î¦i(t)dt_i=(t)Î¦_i(t) dt and the orthonormal bases Î¦it{Î¦_it} are generated by kernel RN(t,s)_N(t,s), i.e., solution to

âˆ«0TRN(t,s)Î¦i(s)ds=Î»iÎ¦i(t),varâ¡[Ni]=Î»i.âˆ«_0^TR_N(t,s)Î¦_i(s) ds=Î»_iÎ¦_i(t),  var[N_i]=Î»_i.
Do the expansion:

S(t)=âˆ‘i=1âˆSiÎ¦i(t),(t)=âˆ‘_i=1^âˆS_iÎ¦_i(t),
where Si=âˆ«0TS(t)Î¦i(t)dt_i=âˆ«_0^TS(t)Î¦_i(t) dt, then

Xi=âˆ«0TX(t)Î¦i(t)dt=Ni_i=âˆ«_0^TX(t)Î¦_i(t) dt=N_i
under H and Ni+Si_i+S_i under K. Let XÂ¯=X1,X2,â€¦X={X_1,X_2,â€¦}, we have

Ni_i are independent Gaussian r.v's with variance Î»iÎ»_i
under H: Xi{X_i} are independent Gaussian r.v's.
fH[x(t)|0<t<T]=fH(x_)=âˆi=1âˆ12Ï€Î»iexpâ¡(âˆ’xi22Î»i)_H[x(t)|0<t<T]=f_H(x)=âˆ_i=1^âˆ1/âˆš(2Ï€Î»_i)exp(-x_i^2/2Î»_i)
under K: Xiâˆ’Si{X_i-S_i} are independent Gaussian r.v's.
fK[x(t)âˆ£0<t<T]=fK(x_)=âˆi=1âˆ12Ï€Î»iexpâ¡(âˆ’(xiâˆ’Si)22Î»i)_K[x(t)|0<t<T]=f_K(x)=âˆ_i=1^âˆ1/âˆš(2Ï€Î»_i)exp(-(x_i-S_i)^2/2Î»_i)
Hence, the log-LR is given by

L(x_)=âˆ‘i=1âˆ2Sixiâˆ’Si22Î»iâ„’(x)=âˆ‘_i=1^âˆ2S_ix_i-S_i^2/2Î»_i
and the optimum detector is

G=âˆ‘i=1âˆSixiÎ»i>G0â‡’K,<G0â‡’H.=âˆ‘_i=1^âˆS_ix_iÎ»_i>G_0,<G_0.
Define

k(t)=âˆ‘i=1âˆÎ»iSiÎ¦i(t),0<t<T,(t)=âˆ‘_i=1^âˆÎ»_iS_iÎ¦_i(t),0<t<T,
then G=âˆ«0Tk(t)x(t)dt.=âˆ«_0^Tk(t)x(t) dt.

How to find k(t)[edit]
Since

âˆ«0TRN(t,s)k(s)ds=âˆ‘i=1âˆÎ»iSiâˆ«0TRN(t,s)Î¦i(s)ds=âˆ‘i=1âˆSiÎ¦i(t)=S(t),âˆ«_0^TR_N(t,s)k(s) ds=âˆ‘_i=1^âˆÎ»_iS_iâˆ«_0^TR_N(t,s)Î¦_i(s) ds=âˆ‘_i=1^âˆS_iÎ¦_i(t)=S(t),
k(t) is the solution to

âˆ«0TRN(t,s)k(s)ds=S(t).âˆ«_0^TR_N(t,s)k(s) ds=S(t).
If N(t)is wide-sense stationary,

âˆ«0TRN(tâˆ’s)k(s)ds=S(t),âˆ«_0^TR_N(t-s)k(s) ds=S(t),
which is known as the Wienerâ€“Hopf equation. The equation can be solved by taking fourier transform, but not practically realizable since infinite spectrum needs spatial factorization. A special case which is easy to calculate k(t) is white Gaussian noise.

âˆ«0TN02Î´(tâˆ’s)k(s)ds=S(t)â‡’k(t)=CS(t),0<t<T.âˆ«_0^TN_0/2Î´(t-s)k(s) ds=S(t)(t)=CS(t),  0<t<T.
The corresponding impulse response is h(t) = k(TÂ âˆ’Â t) = CS(TÂ âˆ’Â t). Let CÂ =Â 1, this is just the result we arrived at in previous section for detecting of signal in white noise.

Test threshold for Neymanâ€“Pearson detector[edit]
Since X(t) is a Gaussian process,

G=âˆ«0Tk(t)x(t)dt,=âˆ«_0^Tk(t)x(t) dt,
is a Gaussian random variable that can be characterized by its mean and variance.

E[Gâˆ£H]=âˆ«0Tk(t)E[x(t)âˆ£H]dt=0E[Gâˆ£K]=âˆ«0Tk(t)E[x(t)âˆ£K]dt=âˆ«0Tk(t)S(t)dtâ‰¡ÏE[G2âˆ£H]=âˆ«0Tâˆ«0Tk(t)k(s)RN(t,s)dtds=âˆ«0Tk(t)(âˆ«0Tk(s)RN(t,s)ds)=âˆ«0Tk(t)S(t)dt=Ïvarâ¡[Gâˆ£H]=E[G2âˆ£H]âˆ’(E[Gâˆ£H])2=ÏE[G2âˆ£K]=âˆ«0Tâˆ«0Tk(t)k(s)E[x(t)x(s)]dtds=âˆ«0Tâˆ«0Tk(t)k(s)(RN(t,s)+S(t)S(s))dtds=Ï+Ï2varâ¡[Gâˆ£K]=E[G2|K]âˆ’(E[G|K])2=Ï+Ï2âˆ’Ï2=Ïğ„[G]   =âˆ«_0^Tk(t)ğ„[x(t)] dt=0
ğ„[G]   =âˆ«_0^Tk(t)ğ„[x(t)] dt=âˆ«_0^Tk(t)S(t) dtâ‰¡Ï
ğ„[G^2]   =âˆ«_0^Tâˆ«_0^Tk(t)k(s)R_N(t,s) dt ds=âˆ«_0^Tk(t)(âˆ«_0^Tk(s)R_N(t,s) ds)=âˆ«_0^Tk(t)S(t) dt=Ï
var[G]   =ğ„[G^2]-(ğ„[G])^2=Ï
ğ„[G^2]   =âˆ«_0^Tâˆ«_0^Tk(t)k(s)ğ„[x(t)x(s)] dt ds=âˆ«_0^Tâˆ«_0^Tk(t)k(s)(R_N(t,s)+S(t)S(s)) dt ds=Ï+Ï^2
var[G]   =ğ„[G^2|K]-(ğ„[G|K])^2=Ï+Ï^2-Ï^2=Ï
Hence, we obtain the distributions of H and K:

H:Gâˆ¼N(0,Ï):G(0,Ï)
K:Gâˆ¼N(Ï,Ï):G(Ï,Ï)
The false alarm error is

Î±=âˆ«G0âˆN(0,Ï)dG=1âˆ’Î¦(G0Ï).Î±=âˆ«_G_0^âˆN(0,Ï) dG=1-Î¦(G_0/âˆš(Ï)).
So the test threshold for the Neymanâ€“Pearson optimum detector is

G0=ÏÎ¦âˆ’1(1âˆ’Î±)._0=âˆš(Ï)Î¦^-1(1-Î±).
Its power of detection is

Î²=âˆ«G0âˆN(Ï,Ï)dG=Î¦(Ïâˆ’Î¦âˆ’1(1âˆ’Î±))Î²=âˆ«_G_0^âˆN(Ï,Ï) dG=Î¦(âˆš(Ï)-Î¦^-1(1-Î±))
When the noise is white Gaussian process, the signal power is

Ï=âˆ«0Tk(t)S(t)dt=âˆ«0TS(t)2dt=E.Ï=âˆ«_0^Tk(t)S(t) dt=âˆ«_0^TS(t)^2 dt=E.
Prewhitening[edit]
For some type of colored noise, a typical practise is to add a prewhitening filter before the matched filter to transform the colored noise into white noise. For example, N(t) is a wide-sense stationary colored noise with correlation function

RN(Ï„)=BN04eâˆ’B|Ï„|_N(Ï„)=BN_0/4e^-B|Ï„|
SN(f)=N02(1+(wB)2)_N(f)=N_0/2(1+(w/B)^2)
The transfer function of prewhitening filter is

H(f)=1+jwB.(f)=1+jw/B.
Detection of a Gaussian random signal in Additive white Gaussian noise (AWGN)[edit]
When the signal we want to detect from the noisy channel is also random, for example, a white Gaussian process X(t), we can still implement Kâ€“L expansion to get independent sequence of observation. In this case, the detection problem is described as follows:

H0:Y(t)=N(t)_0:Y(t)=N(t)
H1:Y(t)=N(t)+X(t),0<t<T._1:Y(t)=N(t)+X(t),  0<t<T.
X(t) is a random process with correlation function RX(t,s)=EX(t)X(s)_X(t,s)=E{X(t)X(s)}
The Kâ€“L expansion of X(t) is

X(t)=âˆ‘i=1âˆXiÎ¦i(t),(t)=âˆ‘_i=1^âˆX_iÎ¦_i(t),
where

Xi=âˆ«0TX(t)Î¦i(t)dt_i=âˆ«_0^TX(t)Î¦_i(t) dt
and Î¦i(t)Î¦_i(t) are solutions to

âˆ«0TRX(t,s)Î¦i(s)ds=Î»iÎ¦i(t).âˆ«_0^TR_X(t,s)Î¦_i(s)ds=Î»_iÎ¦_i(t).
So Xi_i's are independent sequence of r.v's with zero mean and variance Î»iÎ»_i. Expanding Y(t) and N(t) by Î¦i(t)Î¦_i(t), we get

Yi=âˆ«0TY(t)Î¦i(t)dt=âˆ«0T[N(t)+X(t)]Î¦i(t)=Ni+Xi,_i=âˆ«_0^TY(t)Î¦_i(t) dt=âˆ«_0^T[N(t)+X(t)]Î¦_i(t)=N_i+X_i,
where

Ni=âˆ«0TN(t)Î¦i(t)dt._i=âˆ«_0^TN(t)Î¦_i(t) dt.
As N(t) is Gaussian white noise, Ni_i's are i.i.d sequence of r.v with zero mean and variance 12N012N_0, then the problem is simplified as follows,

H0:Yi=Ni_0:Y_i=N_i
H1:Yi=Ni+Xi_1:Y_i=N_i+X_i
The Neymanâ€“Pearson optimal test:

Î›=fYâˆ£H1fYâˆ£H0=Ceâˆ’âˆ‘i=1âˆyi22Î»i12N0(12N0+Î»i),Î›=f_Y_1/f_Y_0=Ce^-âˆ‘_i=1^âˆy_i^2/2Î»_i/12N_0(12N_0+Î»_i),
so the log-likelihood ratio is

L=lnâ¡(Î›)=Kâˆ’âˆ‘i=1âˆ12yi2Î»iN02(N02+Î»i).â„’=ln(Î›)=K-âˆ‘_i=1^âˆ12y_i^2Î»_i/N_0/2(N_0/2+Î»_i).
Since

X^i=Î»iN02(N02+Î»i)X_i=Î»_i/N_0/2(N_0/2+Î»_i)
is just the minimum-mean-square estimate of Xi_i given Yi_i's,

L=K+1N0âˆ‘i=1âˆYiX^i.â„’=K+1/N_0âˆ‘_i=1^âˆY_iX_i.
Kâ€“L expansion has the following property:  If

f(t)=âˆ‘fiÎ¦i(t),g(t)=âˆ‘giÎ¦i(t),(t)=_iÎ¦_i(t),g(t)=_iÎ¦_i(t),
where

fi=âˆ«0Tf(t)Î¦i(t)dt,gi=âˆ«0Tg(t)Î¦i(t)dt._i=âˆ«_0^Tf(t)Î¦_i(t) dt,_i=âˆ«_0^Tg(t)Î¦_i(t) dt.
then

âˆ‘i=1âˆfigi=âˆ«0Tg(t)f(t)dt.âˆ‘_i=1^âˆf_ig_i=âˆ«_0^Tg(t)f(t) dt.
So let

X^(tâˆ£T)=âˆ‘i=1âˆX^iÎ¦i(t),L=K+1N0âˆ«0TY(t)X^(tâˆ£T)dt.X(t)=âˆ‘_i=1^âˆX_iÎ¦_i(t),  â„’=K+1/N_0âˆ«_0^TY(t)X(t) dt.
Noncausal filter Q(t,s) can be used to get the estimate through

X^(tâˆ£T)=âˆ«0TQ(t,s)Y(s)ds.X(t)=âˆ«_0^TQ(t,s)Y(s) ds.
By orthogonality principle, Q(t,s) satisfies

âˆ«0TQ(t,s)RX(s,t)ds+N02Q(t,Î»)=RX(t,Î»),0<Î»<T,0<t<T.âˆ«_0^TQ(t,s)R_X(s,t) ds+N_02Q(t,Î»)=R_X(t,Î»),0<Î»<T,0<t<T.
However, for practical reasons, it's necessary to further derive the causal filter h(t,s), where h(t,s) = 0 for s > t, to get estimate X^(tâˆ£t)X(t). Specifically,

Q(t,s)=h(t,s)+h(s,t)âˆ’âˆ«0Th(Î»,t)h(s,Î»)dÎ»(t,s)=h(t,s)+h(s,t)-âˆ«_0^Th(Î»,t)h(s,Î») dÎ»
See also[edit]
Principal component analysis
Polynomial chaos
Reproducing kernel Hilbert space
Mercer's theorem
Notes[edit]


^ Sapatnekar, Sachin (2011), "Overcoming variations in nanometer-scale technologies", IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 1 (1): 5â€“18, Bibcode:2011IJEST...1....5S, CiteSeerXÂ 10.1.1.300.5659, doi:10.1109/jetcas.2011.2138250, S2CIDÂ 15566585

^ Ghoman, Satyajit; Wang, Zhicun; Chen, PC; Kapania, Rakesh (2012). "A POD-based Reduced Order Design Scheme for Shape Optimization of Air Vehicles". Proc of 53rd AIAA/ASME/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference, AIAA-2012-1808, Honolulu, Hawaii.

^ Karhunenâ€“Loeve transform (KLT) Archived 2016-11-28 at the Wayback Machine, Computer Image Processing and Analysis (E161) lectures, Harvey Mudd College

^ Raju, C.K. (2009), "Kosambi the Mathematician", Economic and Political Weekly, 44 (20): 33â€“45

^ Kosambi, D. D. (1943), "Statistics in Function Space", Journal of the Indian Mathematical Society, 7: 76â€“88, MRÂ 0009816.

^ Giambartolomei, Giordano (2016). "4 The Karhunen-LoÃ¨ve Theorem". The Karhunen-LoÃ¨ve theorem (Bachelors). University of Bologna.

^ A wavelet tour of signal processing-StÃ©phane Mallat

^ X. Tang, â€œTexture information in run-length matrices,â€ IEEE Transactions on Image Processing, vol. 7, No. 11, pp. 1602â€“1609, Nov. 1998


References[edit]
Stark, Henry; Woods, John W. (1986). Probability, Random Processes, and Estimation Theory for Engineers. Prentice-Hall, Inc. ISBNÂ 978-0-13-711706-2. OLÂ 21138080M.
Ghanem, Roger; Spanos, Pol (1991). Stochastic finite elements: a spectral approach. Springer-Verlag. ISBNÂ 978-0-387-97456-9. OLÂ 1865197M.
Guikhman, I.; Skorokhod, A. (1977). Introduction a la ThÃ©orie des Processus AlÃ©atoires. Ã‰ditions MIR.
Simon, B. (1979). Functional Integration and Quantum Physics. Academic Press.
Karhunen, Kari (1947). "Ãœber lineare Methoden in der Wahrscheinlichkeitsrechnung". Ann. Acad. Sci. Fennicae. Ser. A I. Math.-Phys. 37: 1â€“79.
LoÃ¨ve, M. (1978). Probability theory. Vol. II, 4th ed. Graduate Texts in Mathematics. Vol.Â 46. Springer-Verlag. ISBNÂ 978-0-387-90262-3.
Dai, G. (1996). "Modal wave-front reconstruction with Zernike polynomials and Karhunenâ€“Loeve functions". JOSA A. 13 (6): 1218. Bibcode:1996JOSAA..13.1218D. doi:10.1364/JOSAA.13.001218.
Wu B., Zhu J., Najm F.(2005) "A Non-parametric Approach for Dynamic Range Estimation of Nonlinear Systems". In Proceedings of Design Automation Conference(841-844) 2005
Wu B., Zhu J., Najm F.(2006) "Dynamic Range Estimation". IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, Vol. 25 Issue:9 (1618â€“1636) 2006
Jorgensen, Palle E. T.; Song, Myung-Sin (2007). "Entropy Encoding, Hilbert Space and Karhunenâ€“Loeve Transforms". Journal of Mathematical Physics. 48 (10): 103503. arXiv:math-ph/0701056. Bibcode:2007JMP....48j3503J. doi:10.1063/1.2793569. S2CIDÂ 17039075.
External links[edit]
Mathematica KarhunenLoeveDecomposition function.
E161: Computer Image Processing and Analysis notes by Pr. Ruye Wang at Harvey Mudd College [1]



